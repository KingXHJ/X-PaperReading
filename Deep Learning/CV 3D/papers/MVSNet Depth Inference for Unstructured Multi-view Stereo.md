# 论文信息
- 时间：2018
- 期刊：ECCV
- 网络名称：MVSNet
- 意义：第一个基于深度学习的MVS模型，3D 从上图volume
- 作者：Yao Yao
- 实验环境：
- 数据集：室内数据集DTU；室外数据集Tanks and Temples
- [返回上一层 README](../README.md)

# 专有名词
1. Homography：
    - 如果两台相机拍摄的是同一个场景，但两台相机之间只有旋转角度的不同，没有任何位移，则这两台相机之间的关系称为Homography
    - 单应性变换是将一个平面内的点映射到另一个平面内的二维投射变换，在这里，平面是指图像或者三维中的平面表面
2. 点云（Point clouds）
    - 点云是三维空间(xyz坐标)点的集合
3. 体素网格(Voxel grids)
    - 体素是3D空间的像素。量化的，大小固定的点云。每个单元都是固定大小和离散坐标
4. 多边形网格(Polygon meshes)
    - mesh是面片的集合
5. 多视图表示(Multi-view representations)
    - 多视图表示是从不同模拟视点渲染的2D图像集合

# 一、解决的问题

1. 传统方法的限制：
    - 弱纹理、镜面和反光非常难处理，容易导致重构的不完整性
2. 3D深度神经网络现状：
    - 现在的算法在精确程度上表现得非常好，但是重构场景的完整性还有很大提升空间
3. 基于学习的方法的好处：
    - 能够学习全局的语义信息，从而解决传统方法的限制，而且鲁棒性更好
4. CNN的优势：
    - 立体匹配任务非常适合应用基于CNN的方法，由于输入图像对是提前校正好的，所以只需要找水平方向的视差，无需关心相机参数
5. 两个视角到多视角的问题：
    - 无法充分利用多视角信息，导致准确率很低
6. 之前的CNN方法的限制：
    - 受限于内存消耗，不能存储太大的3D体，之前的网络难以被扩大
    - LSM(Learned Stereo Machine)：选择处理低分辨率的合成物体任务
    - SurfaceNet：要么通过时间换取空间，重建大场景非常耗时
7. MVS重建技术的概况：
    - 主要分为3类：
        1. 直接生成点云：这种方法难以并行，速度缓慢
        2. 体素重建：是空间离散的，误差较大，内存消耗非常大
        3. 深度图重建：本文选择的方法，是最灵活的方法，能够把复杂的问题解耦成由一个参考视图和几个源视图的组合，每次只关注一帧图像对应的场景重建，而且也很容易就可以转化成体素或者点云

# 二、做出的创新
1. 首先，出于深度图推断的目的，我们的3D成本体积建立在相机截头体上，而不是规则的欧几里德空间上
2. 其次，我们的方法将MVS重建分割成逐视图深度图估计的小问题，这使得大规模重建成为可能
3. 摄像机参数在网络中被编码为投影操作，以形成代价体
4. 3D CNN用于分类体素是否属于曲面
5. 相比之下，我们的网络专注于 ***每次生成一个*** 参考图像的深度图，这允许我们直接自适应地重建大型场景


# 三、设计的模型
1. 提出了一个从多视角图片中，推断深度图的端到端（end-to-end）深度神经网络
2. 模型思路：每次前向推理重建深度图，而不是一个3D场景

    ![MVSNet structure](../pictures/MVSNet/MVSNet%20struct.png)

    1. 每次选择一个参考图像和几个原图像作为输入
    2. 首先提取深度视觉图像特征
    3. 这里的关键是 ***可微分单应性扭曲(differentiable homography warping)*** 操作，该操作隐式编码网络中的相机几何结构，以根据2D图像特征构建3D代价体，并实现端到端训练
    4. 为了适应输入中任意数量的源图像，我们提出了一种基于方差的度量，将多个特征映射到体积中的一个成本特征
    5. 然后，该成本体积经历多尺度3D卷积，并回归初始深度图
    6. 最后，利用参考图像对深度图进行细化，以提高边界区域的精度
3. 模型输入：
    - 灵活采用任意的N个视角的输入，通过基于方差的代价度量标准（相似度函数），将多特征映射到一个代价特征上
4. 模型关键：
    ![MVSNet camera frustum](../pictures/MVSNet/MVSNet%20camera%20frustum.jpg)
    - 将相机参数编码成可微的单应性变换，基于 ***视图平截锥体(camera frustum)*** 建代价体（构建2D特征提取到3D的代价归一网络的桥梁）
    
5. 模型结构细节：
    1. 图像特征：
        - 结构：8层2D CNN；3，6层的卷积步长为2，这样就能生成3中大小的特征；除了最后一层，每层都有一个BN和一个ReLU；共享参数
        - 特点：值得注意的是，尽管在特征提取之后一帧图像被缩小，但是每个剩余像素的原始相邻信息已经被编码在32通道像素描述符中，这防止了密集匹配丢失有用的上下文信息。
        - 意义：与简单地对原始图像执行密集匹配相比，提取的特征图显著提高了重建质量
    2. 代价体
        1. 可微的单应性变换：
            - 目的：是取src图像中，对应ref视角像素的信息 $$\begin{pmatrix} R & \mathbf{t} \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} R_ {i} & \mathbf{t}_ {i} \\ 0 & 1 \end{pmatrix} \begin{pmatrix} R_ {1} & \mathbf{t}_ {1} \\ 0 & 1 \end{pmatrix}^{-1}$$ $$\begin{pmatrix} R_ {1} & \mathbf{t}_ {1} \\ 0 & 1 \end{pmatrix}^{-1} = \frac{1}{R_ {1}} \begin{pmatrix} 1 & - \mathbf{t}_ {1} \\ 0 & R_ {1} \end{pmatrix} = \begin{pmatrix} R_ {1}^{-1} & - R_ {1}^{-1} \mathbf{t}_ {1} \\ 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix} R & \mathbf{t} \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} R_ {i} & \mathbf{t}_ {i} \\ 0 & 1 \end{pmatrix} \begin{pmatrix} R_ {1}^{-1} & - R_ {1}^{-1} \mathbf{t}_ {1} \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} R_ {i} R^{-1}_ {1} & \mathbf{t}_ {i} - R_ {i} R^{-1}_ {1} \mathbf{t}_ {1} \\ 0 & 1 \end{pmatrix}$$

            $$H=K_i (R_i R_1^{-1} - \frac{(\mathbf{t}_i - R_i R_1^{-1} \mathbf{t}_1)\mathbf{n}_1^T}{d})K_1^{-1}$$

            $$H=K_i R_i (I - \frac{(R_i^{-1} \mathbf{t}_i - R_1^{-1} \mathbf{t}_1)\mathbf{n}_1^T R_1}{d})R_1^{-1} K_1^{-1}$$

            - 扭曲过程类似于经典的平面扫描立体，除了可微分双线性插值用于从特征图 $\lbrace F_i \rbrace ^N_{i=1}$ 而不是从图像 $\lbrace I_i \rbrace ^N_{i=1}$ 中采样像素

        2. 代价矩阵
            - 聚合多个特征体成为一个代价体 $C$
            - $V=\frac{W}{4} \cdot \frac{H}{4} \cdot D \cdot F$
                - W：图像宽度
                - H：图像高度
                - D：采样深度序号
                - F：特征图的通道号
            - $C= \mathcal{m} (V_1, \dots ,V_N)=\frac{\sum{N}{i=1}(V_i-\bar{V_i})^2}{N}$
                - $\bar{V_i}$ 是所有特征体积中的平均体积，并且以上所有操作都是逐元素的。
            - 设计理念：大多数传统的MVS方法以启发式（不断迭代最后找到局部最优解的方法）方式聚集参考图像和所有源图像之间的成对代价。相反，我们的度量设计遵循这样一种理念，即所有视图都应该对匹配成本做出同等贡献，并且不偏爱参考图像
            - 选择方差的原因：因为“均值”操作本身不提供关于特征差异的信息，并且它们的网络需要CNN前和后层来帮助推断相似性。
        3. 代价矩阵的正则化
            - 最终目的：代价矩阵C -> 概率矩阵P
            - 原因：根据图像特征计算的原始成本体积可能受到噪声污染（例如，由于非朗伯曲面或对象遮挡的存在），并且应与平滑度约束结合以推断深度图
            - 方法：3D CNN；这里的四尺度网络类似于3D版本UNet，它使用编码器-解码器以相对低的存储和计算成本从大的接收场聚集相邻信息
            - 结构细节：为了进一步减少计算需求，我们在第一个3D卷积层之后将32通道代价体减少到8通道，并将每个尺度内的卷积从3层更改为2层。最后一个卷积层输出1通道体。最后，我们沿着深度方向应用softmax操作进行概率归一化
            - 结果评估：所得概率体积在深度图推断中是非常理想的，因为它不仅可以用于每像素深度估计，而且可以用于测量估计置信度
    3. 深度图

        ![MVSNet pd](../pictures/MVSNet/MVSNet%20pd.png)

        1. 初步估算
            - 问题：从概率体积P中检索深度图D的最简单方法是逐像素比较概率大小（赢家通吃）。但是，argmax运算无法产生亚像素估计，并且由于其不可区分性，无法使用反向传播进行训练
            - 方案：相反，我们计算沿深度方向的期望值，即所有假设的概率加权和.它是完全可微的，并且能够近似argmax结果
            $$D = \sum{d_{max}}{d=d_{min}} d \times P(d)$$
            - 输出深度图，如图b，与2D图像特征图的大小相同，与输入图像相比，2D图像在每个维度上缩小了四倍
        2. 概率图
            - 影响因素：沿深度方向的概率分布也反映了深度估计质量
            - 问题：尽管多尺度3D CNN具有很强的能力将概率正则化为单峰分布，但我们注意到，对于那些错误匹配的像素，它们的概率分布是分散的，不能集中到一个峰值，如图c
            - 方案：基于这一观察，我们将深度估计 $\hat{d}$ 的质量定义为真实深度在估计附近的小范围内的概率。由于深度假设是沿着 ***视图平截锥体(camera frustum)*** 离散采样的，我们只需在四个最近的深度假设上取概率和来测量估计质量
            - 效果：请注意，这里也可以使用其他统计测量，如标准差或熵，但在我们的实验中，我们观察到深度图过滤的这些测量没有显著改进。更重要的是，我们的概率和公式可以更好地控制异常值过滤的阈值参数
        3. 深度图优化
            - 问题：虽然从概率体中检索到的深度图是合格的输出，但由于正则化涉及的大感受野，类似于语义分割和图像抠图中的问题，重建边界可能会过度平滑
            - 方案：请注意，参考图像本身包含边界信息，因此我们使用参考图像作为指导来细化深度图。受最近的图像抠图算法的启发，我们在MVSNet的末尾应用了深度残差学习网络
            - 实现细节：初始深度图和调整大小的参考图像被连接为4通道输入，然后通过三个32通道2D卷积层，接着是一个1通道卷积层以学习深度残差。然后将初始深度图添加回，以生成细化的深度图。最后一层不包含BN层和ReLU单元，以学习负残差。此外，为了防止在某个深度尺度上出现偏差，我们将初始深度幅度预缩放到范围[0，1]，并在细化后将其转换回去
    4. 误差计算

        ![MVSNet loss](../pictures/MVSNet/MVSNet%20loss.png)

        - 内容：初始深度图和细化深度图的损失都被考虑在内
        - 问题及解决：由于真实深度图在整个图像中并不总是完整的，我们只考虑具有有效地面真实标签的像素
6. 训练
    1. 数据准备 ***提供了重要思路***
        - DTU为真实点云提供了正常信息，我们使用筛选的泊松曲面重建（SPSR）来生成网格曲面，然后将网格渲染到每个视点，以生成用于训练的深度图
    2. 视图选择
        - 在我们的训练中使用了一幅参考图像和两幅源图像（N=3）
        - 邻域帧的选择是根据稀疏点在两图像的夹角决定的。夹角越接近5°，分数越高
        - 请注意，在特征提取中图像将被缩小，再加上3D正则化部分中的四尺度编码器-解码器结构，***输入图像大小必须可被32倍整除***

7. 后期处理（Post-processing）
    1. 深度图过滤器
        - 目的：上述网络估计每个像素的深度值。在将结果转换为密集点云之前，有必要过滤掉那些背景和遮挡区域的异常值
        - 方案：我们提出了两个标准，即光度一致性和几何一致性，用于鲁棒深度图滤波
            - 光度一致性测量匹配质量，使用概率图。当概率低于0.8被认为异常值（outliers）
            - 几何约束测量多个视图之间的深度一致性。将参考图的一个像素 $p_1$ ，根据参考图上的深度 $d_1$ 投影到另一个视图上对应的像素 $p_i$ ，再根据 $p_i$ 在视图上的深度 $d_i$ 投影回参考视图  $p_{reproj}$ 。若 $|p_{reproj} - p_1| < 1$ 且 $|d_{reproj} - d_1|/d_1 < 0.01$ 我们认为此时两个视图一致
        - 优点：在我们的实验中，所有深度至少应保持三视图一致。这种简单的两步过滤策略对过滤不同类型的异常值具有很强的鲁棒性
    2. 深度图融合
        - 目的：与其他多视图立体方法类似，我们应用深度图融合步骤，将不同视图的深度图集成到统一的点云表示
        - 方法：在我们的重建中使用了基于可见性的融合算法，其中不同视点之间的深度遮挡和违规被最小化。为了进一步抑制重建噪声，我们在滤波步骤中确定每个像素的可见视图，并将所有重投影深度 $\hat{d_{reproj}}$ 的平均值作为像素的最终深度估计。然后将融合的深度图直接重新投影到空间以生成3D点云

# 四、实验结果
- 真值是结构光扫描得到的点云模型
    1. comp（完整性计算）：计算结构光扫描的模型的每个点到距离MVS重建的模型最近点的距离
    2. Acc（精度计算）：在可视mask内的MVS重建的点到结构光扫描模型最近点的距离（mask是根据结构光模型计算的有效测量区域得到的）
    3. f-score综合评价完整性和精度两个参数，只有两者都比较好时，f-score的分值才会高，具体公式见论文[Knapitsch, A., Park, J., Zhou, Q.Y., Koltun, V.: Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG) (2017)]
## 1、比之前模型的优势
1. 完整性出众：MVSNet精度不一定是最高的，但是也还不错。MVSNet的完整性是最好的
2. 泛化性不错：直接用在DTU上的结果，没经过下游微调，直接用于检测Tanks and Temples数据集，仍然表现得很好。
## 2、有优势的原因
- 消融实验
    1. 视图数量
        - 视图越多越好。但是三个视图的效果已经和更高阶视图的效果差不多，而且计算量更少
    2. 图像特征
        - 采用设计的8层金字塔式特征提取，就是比单独的一个32通道、7x7核、步长为4的卷积效果好
    3. 代价矩阵
        - 逐元素的方差效果就是比均值好
    4. 深度优化
        - 优化对实际表现影响不大（从结果看，不优化效果更好）
## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论
