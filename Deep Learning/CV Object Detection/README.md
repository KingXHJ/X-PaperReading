# 计算机视觉-Object Detection

## 目录
- [笔记](#笔记)
- [目标检测网络对比综述](./papers/Comparison%20of%20Object%20Detection%20Networks.md)
- [返回上一层 README](../README.md)


## 笔记

| 笔记 | 年份 | 收录 | 名字                                                         | 简介                 | 引用 |
| ------ | ---- | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
|        | 2008 | CVPR | [DPM](http://people.csail.mit.edu/torralba/courses/6.870/papers/FelzenszwalbMcAllesterRamanan.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F860a9d55d87663ca88e74b3ca357396cd51733d0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/860a9d55d87663ca88e74b3ca357396cd51733d0)  |
|        | 2013 | IJCV | [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38b6540ddd5beebffd05047c78183f7575559fb2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/38b6540ddd5beebffd05047c78183f7575559fb2)  |
|        | 2013 | ICLR | [SPPnet](https://arxiv.org/pdf/1312.6229.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1109b663453e78a59e4f66446d71720ac58cec25%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25)  |
|        | 2014 | CVPR | [MultiBox](https://www.semanticscholar.org/reader/67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6)  |
| [✅](./papers/Rich%20Feature%20Hierarchies%20for%20Accurate%20Object%20Detection%20and%20Semantic%20Segmentation.md) | 2014 | CVPR | [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)    | Two-stage             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f4df08d9072fc2ac181b7fced6a245315ce05c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8)  |
|        | 2015 | CVPR | [FCN](https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F317aee7fc081f2b137a85c4f20129007fd8e717e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/317aee7fc081f2b137a85c4f20129007fd8e717e)  |
| [✅](./papers/Fast%20R-CNN.md) | 2015 | ICCV | [Fast R-CNN](https://arxiv.org/pdf/1504.08083v2.pdf)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7ffdbc358b63378f07311e883dddacc9faeeaf4b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b)  |
| [✅](./papers/Faster%20R-CNN%20Towards%20Real-Time%20Object%20Detection%20with%20Region%20Proposal%20Networks.md) | 2015 | NeurIPS | [Faster R-CNN](https://arxiv.org/pdf/1506.01497v3.pdf) |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F424561d8585ff8ebce7d5d07de8dbf7aae5e7270%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270)  |
| [✅](./papers/You%20Only%20Look%20Once%20Unified,%20Real-Time%20Object%20Detection.md) | 2016 | CVPR | [YOLO](https://arxiv.org/pdf/1506.02640v5.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff8e79ac0ea341056ef20f2616628b3e964764cfd%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd)  |
| [✅](./papers/SSD%20Single%20Shot%20MultiBox%20Detector.md) | 2016 | ECCV | [SSD](https://arxiv.org/pdf/1512.02325v5.pdf)          | Single stage          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0)  |
|        | 2017 | CVPR | [FPN](https://arxiv.org/pdf/1612.03144.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb9b4e05faa194e5022edd9eb9dd07e3d675c2b36%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36)  |
| [✅](./papers/Mask%20R-CNN.md) | 2017 | ICCV | [Mask R-CNN](https://arxiv.org/pdf/1703.06870v3.pdf)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea99a5535388196d0d44be5b4d7dd02029a43bb2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2)  |
| [✅](./papers/YOLO9000%20Better,%20Faster,%20Stronger.md) | 2017 | CVPR | [YOLOv2](https://arxiv.org/pdf/1612.08242v1.pdf)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7d39d69b23424446f0400ef603b2e3e22d0309d6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6)  |
| [✅](./papers/Comparison%20of%20Object%20Detection%20Networks.md) | 2017 | ICCV | [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf)       | one-stage精度第一次超过two-stage         |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F72564a69bf339ff1d16a639c86a764db2321caab%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/72564a69bf339ff1d16a639c86a764db2321caab)  |
| [✅](./papers/YOLOv3%20An%20Incremental%20Improvement.md) | 2018 | arXiv | [YOLOv3](https://arxiv.org/pdf/1804.02767v1.pdf)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4845fb1e624965d4f036d7fd32e8dcdd2408148%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148)  |
|        | 2019 | arXiv | [CenterNet](https://arxiv.org/pdf/1904.07850.pdf) | Anchor free           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2)  |
| [✅](./papers/Comparison%20of%20Object%20Detection%20Networks.md) | 2019 | ICCV | [FCOS](https://arxiv.org/pdf/1904.01355.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe2751a898867ce6687e08a5cc7bdb562e999b841%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/e2751a898867ce6687e08a5cc7bdb562e999b841)  |
| [✅](./papers/End-to-End%20Object%20Detection%20with%20Transformers.md)       | 2020 | ECCV | [DETR](https://arxiv.org/pdf/2005.12872.pdf)      | Transformer           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F962dc29fdc3fbdc5930a10aba114050b82fe5a3e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e)  |
| [✅](./papers/YOLOv4%20Optimal%20Speed%20and%20Accuracy%20of%20Object%20Detection.md) | 2020 | arXiv | [YOLOv4](https://arxiv.org/pdf/2004.10934.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2a6f7f0d659c5f7dcd665064b71e7b751592c80e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/2a6f7f0d659c5f7dcd665064b71e7b751592c80e)  |
| [✅](./papers/Deformable%20DETR%20Deformable%20Transformers%20for%20End-to-End%20Object%20Detection.md) | 2021 | ICLR | [Deformable DERT](https://arxiv.org/pdf/2010.04159v4.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F39ca8f8ff28cc640e3b41a6bd7814ab85c586504%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/39ca8f8ff28cc640e3b41a6bd7814ab85c586504)  |
| [✅](./papers/Scaled-YOLOv4%20Scaling%20Cross%20Stage%20Partial%20Network.md) | 2021 | CVPR | [Scaled-YOLOv4](https://arxiv.org/pdf/2011.08036.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F126d36f17cae3e5210a6f62e5c6a23ddec0ef350%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/126d36f17cae3e5210a6f62e5c6a23ddec0ef350)  |
| [✅](./papers/Comparison%20of%20Object%20Detection%20Networks.md) | 2021 | GitHub Ultralytics | [YOLOv5](https://github.com/ultralytics/yolov5)      | GitHub源码 |[![Github stars](https://img.shields.io/github/stars/ultralytics/yolov5.svg)](https://github.com/ultralytics/yolov5)  |
| [✅](./papers/YOLOX%20Exceeding%20YOLO%20Series%20in%202021.md) | 2021 | arXiv | [YOLOX](https://arxiv.org/pdf/2107.08430.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc01b385205e488a731c8c8c11c0c494d426beb03%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/c01b385205e488a731c8c8c11c0c494d426beb03)  |
| [✅](./papers/You%20Only%20Learn%20One%20Representation%20Unified%20Network%20for%20Multiple%20Tasks.md) | 2021 | Journal of information science and engineering | [YOLOR](https://arxiv.org/pdf/2105.04206.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b14b26cf6c3dd161e272707b1b0236bc8152155%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/0b14b26cf6c3dd161e272707b1b0236bc8152155)  |
| [✅](./papers/YOLOv6%20A%20Single-Stage%20Object%20Detection%20Framework%20for%20Industrial%20Applications.md) | 2022 | arXiv | [YOLOv6](https://arxiv.org/pdf/2209.02976.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fee19e8936275c8efe30c91a3c64e5ad8f3b15dc3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/ee19e8936275c8efe30c91a3c64e5ad8f3b15dc3)  |
| [✅](./papers/YOLOv7%20Trainable%20bag-of-freebies%20sets%20new%20state-of-the-art%20for%20real-time%20object%20detectors.md) | 2023 | CVPR | [YOLOv7](https://arxiv.org/pdf/2207.02696.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3aed4648f7857c1d5e9b1da4c3afaf97463138c3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/3aed4648f7857c1d5e9b1da4c3afaf97463138c3)  |
| [✅](./papers/Comparison%20of%20Object%20Detection%20Networks.md) | 2023 | GitHub Ultralytics | [YOLOv8](https://github.com/ultralytics/ultralytics)      | GitHub源码 |[![Github stars](https://img.shields.io/github/stars/ultralytics/ultralytics.svg)](https://github.com/ultralytics/ultralytics)  |
| [✅](./papers/DETRs%20Beat%20YOLOs%20on%20Real-time%20Objext%20Detection.md) | 2023 | arXiv | [RT-DERT](https://arxiv.org/pdf/2304.08069)      | 实时检测Transformer(RT-DETR) |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1729703426d6d90bcc626cee9322480715793a6e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/1729703426d6d90bcc626cee9322480715793a6e)  |
| [✅](./papers/Balanced%20Classification%20A%20Unified%20Framework%20for%20Long-Tailed%20Object%20Detection.md) | 2023 | IEEE transactions on multimedia | [BACL](https://arxiv.org/pdf/2308.02213.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F22e21ea54609afb9db154e1b20b5bc5716791c39%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Balanced-Classification%3A-A-Unified-Framework-for/22e21ea54609afb9db154e1b20b5bc5716791c39)  |
|        | 2023 | arXiv | [SAM](https://arxiv.org/pdf/2304.02643)         | 出色的目标检测器                      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7470a1702c8c86e6f28d32cfa315381150102f5b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b)  |
|        | 2023 | arXiv | [SAM Fails to Segment Anything?](https://arxiv.org/pdf/2304.09148)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fecf514b5015acd8c35bea1d313bd1620cfa57eef%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ecf514b5015acd8c35bea1d313bd1620cfa57eef)  |
|        | 2023 | arXiv | [SAM Med2D](https://arxiv.org/pdf/2308.16184.pdf)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd7fb24b589f714cb237c05b2f5312c41f88cec68%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/SAM-Med2D-Cheng-Ye/d7fb24b589f714cb237c05b2f5312c41f88cec68)  |
|  | 2023 | GitHub Deci-AI super-gradients | [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md)      | GitHub源码 |[![Github stars](https://img.shields.io/github/stars/Deci-AI/super-gradients.svg)](https://github.com/Deci-AI/super-gradients)  |
| [✅](./papers/A%20Comprehensive%20Review%20of%20YOLO%20From%20YOLOv1%20to%20YOLOv8%20and%20Beyond.md) | 2023 | Machine Learning and Knowledge Extraction | [A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS](https://arxiv.org/pdf/2304.00501.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F913d86a84afae61b51281a1bce2edbd72b7c7acb%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Comprehensive-Review-of-YOLO-Architectures-in-to-Terven-Esparza/913d86a84afae61b51281a1bce2edbd72b7c7acb)  |
|  | 2024 | arXiv | [YOLO-World](https://arxiv.org/pdf/2401.17270.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F37c112454a236ab91c9c6b5cc165a6c3251e9206%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/YOLO-World%3A-Real-Time-Open-Vocabulary-Object-Cheng-Song/37c112454a236ab91c9c6b5cc165a6c3251e9206)  |
|  | 2024 | arXiv | [YOLOv9](https://arxiv.org/pdf/2402.13616.pdf)      |            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf70392a3b1ae92fdb1b70448aaddcbd03726d3d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/YOLOv9%3A-Learning-What-You-Want-to-Learn-Using-Wang-Yeh/cf70392a3b1ae92fdb1b70448aaddcbd03726d3d)  |


*[跳转至目录](#目录)*
