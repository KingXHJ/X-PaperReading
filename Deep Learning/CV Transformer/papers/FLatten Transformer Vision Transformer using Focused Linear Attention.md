# 论文信息
- 时间：2023
- 期刊：ICCV
- 网络/算法名称：FLatten Transformer
- 意义：能够将模型在CPU端加速约2.0倍，在GPU端加速约1.5倍
- 作者：Dongchen Han*, Xuran Pan∗, Yizeng Han, Shiji Song, Gao Huang†; Department of Automation, BNRist, Tsinghua University
- 实验环境：Test the inference latency on multiple hardware platforms, including a desktop CPU (Intel i5-8265U) and two server GPUs (RTX2080Ti and RTX3090)
- 数据集：ImageNet-1K classification, ADE20K semantic segmentation, and COCO object detection
- [返回上一层 README](../README.md)

# 一、解决的问题
- 近年来，Transformer在计算机视觉领域得到了巨大的发展和自我关注。随着视觉Transformer的出现，自注意技术在各种视觉任务中显示出巨大的潜力，包括图像分类、语义分割和对象检测。
- 全局感受野的生搬硬套
    - 然而，将Transformer应用于视觉模型是一项不平凡的任务。当使用具有全局感受野的自注意时，与序列长度 $n$ 相关的二次计算复杂度 $O(n^{2})$ 导致高计算成本。先前的工作试图通过将全局感受野限制在较小的区域来解决这个问题，例如设计稀疏的全局注意力模式或应用较小的注意力窗口。尽管这些方法是有效的，但它们要么容易受到注意力模式的影响，在这种模式下，其他区域的信息特征可能会被丢弃，要么不可避免地牺牲了对长期依赖性建模的能力。
- 解决方法
    - 另一方面，线性注意力被认为是一种简单而有效的替代方案，可以通过降低总体复杂性来解决计算难题。早期的研究利用了一种局部敏感的哈希方案，该方案将计算复杂度从 $O(n^{2})$ 压缩到 $O(nlog(n))$ 。尽管如此，它在复杂性项之前引入了一个大常数，这使得它在常见情况下仍然负担不起。最近的研究注意到，自注意操作中的Softmax函数实际上迫使所有查询和密钥之间进行成对计算，这导致了 $O(n^{2})$ 的复杂性，并成为主要开销。为了解决这一问题，几种方法采用简单的激活函数或特定的映射函数来近似原始Softmax函数。如图1所示，通过将计算顺序从 $(query \cdot key) \cdot value$ 更改为 $query \cdot (key \cdot value)$ ，可以将整体计算复杂度降低到 $O(n)$ 。然而，与Softmax注意力相比，当前的线性注意力方法仍然存在严重的性能下降，并且可能涉及映射函数的额外计算开销，限制了它们的实际应用。

![FLatten Transformer1](../pictures/Flatten%20Transformer/FLatten_Transformer1.png)

1. Vision Transformer and Self-Attention
    - Transformer和自注意机制最早出现在自然语言处理领域，在计算机视觉领域引起了广泛的研究兴趣。然而，自注意的高计算复杂度限制了直接应用于视觉任务。以前的工作试图从几个角度解决这一问题。先驱的视觉Transformer考虑通过将相邻像素合并为单个令牌来降低输入分辨率。在以下研究中也采用了类似的见解，并将其扩展到下游任务。另一条研究路线受到卷积神经网络中金字塔结构的启发，考虑逐步降低特征分辨率，同时采用精心设计的注意力模式来约束注意力标记的数量。例如，PVT使用稀疏注意力模式，并从全局角度选择注意力标记。DAT遵循这一路径，设计了一个可变形的注意力模块，以实现数据依赖的注意力模式。Swin-Transformer通过将输入划分为独立的窗口来本地选择关注令牌。NAT在卷积中遵循以查询为中心的模式，并为所有查询设计独立的注意令牌。一些研究还注意到，卷积运算对Transformer模型很有价值，可能有助于提高整体效率。CMT将Transformer块与深度卷积等高效卷积算子相结合，实现了更好的效率-性能折衷。ACmix分担卷积和自注意的计算开销，并以有限的成本集成了这两个模块。对于需要高效率的应用场景，MobileFormer分别为卷积和Transformer维护了两条路径，并从这两个模块中获益。MobileViT利用MobileNets的成功，使用mobilenet块和Transformer块的组合来实现轻量级和低延迟
    - 然而，这些方法仍然依赖于Softmax算子，其继承的高计算复杂度不可避免地导致了模型体系结构设计和实际应用的不便
    - 我们首先回顾一下 Vision Transformer 中自我关注的一般形式。给定输入 $N$ 个令牌 $x \in \mathbb{R}^{N \times C}$ ，在每个头部内，自关注可写为: $$Q=xW_ {Q}, \quad K=xW_ {K}, \quad V=xW_ {V}$$ $$O_ {i}=\sum^{N}_ {j=1} \frac{Sim(Q_ {i}, K_ {j})}{\sum^{N}_ {j=1}Sim(Q_ {i}, K_ {j})}V_ {j}$$ 式中，$W_ {Q}, \quad W_ {K}, \quad W_ {V} \in \mathbb{R}^{C \times C}$ 为投影矩阵，$Sim(\cdot,\cdot)$ 为相似度函数。现代视觉变形器主要采用Softmax注意力，其相似度度量为 $Sim(Q, K)=exp(QK^{T} / \sqrt{d})$，这种情况下，通过计算 ***所有*** 查询键对之间的相似度得到注意图，计算复杂度为 $O(N^{2})$。
    - 由于二次计算的复杂性，简单地使用全局接受域的自注意变得难以处理，通常会导致计算成本过高。以往的研究通过设计稀疏全局注意模式或应用较小的注意窗口来解决这一问题。虽然有效，但这些方法容易受到精心设计的注意力模式的影响，或者不可避免地牺牲了对长期依赖关系进行建模的能力。

1. Linear Attention
    - 除了上述方法外，另一种研究方法是通过线性关注来解决高计算复杂度问题。具体来说，线性注意用单独的核函数代替了自注意中的Softmax函数。在这种情况下，线性注意不必首先计算成对相似度 $QK^{T}$ 。如图1所示，基于矩阵乘法的关联性质，线性关注可以通过先计算 $K^{T}V$ 来改变计算顺序，从而将计算复杂度从 $O(N^{2}d)$ 降低到 $O(nd^{2})$ 。线性注意力模块虽然有效，但如何设计出与softmax注意力一样有效的线性注意力模块是一个不容忽视的问题。Performer用正交随机特征近似Softmax操作。 Efficient attention分别对 $Q$ 和 $K$ 应用Softmax函数，自然保证了 $QK^{T}$ 的每一行之和为1。Nystromformer和SOFT通过矩阵分解近似全自注意矩阵。Hydra attention用余弦相似度代替Softmax，提出Hydra技巧，将计算复杂度降低到 $O(Nd)$ 。EfficientVit使用深度卷积提高线性注意的局部特征提取能力。Castling-ViT提出了线性角核来衡量每个 $Q_ {i}$ 和 $K_ {j}$ 之间的光谱相似度。
    - 然而，当前的线性注意力设计要么没有足够的表达能力来赶上Softmax注意力，要么涉及来自复杂内核函数的额外计算开销。在这项工作中，我们从聚焦能力和特征多样性的角度分析了线性注意力表现下降的原因。基于这些分析，我们提出了一种新的线性注意力模块，称为聚焦线性注意力，它以较低的计算复杂度实现了比Softmax注意力更好的性能（图2）。
    ![FLatten Transformer2](../pictures/Flatten%20Transformer/FLatten_Transformer2.png)
    
    - 相比之下，线性注意力被认为是一种有效的替代方法，它将计算复杂度从 $O(N^{2})$ 限制到 $O(N)$ 。具体来说，引入精心设计的核函数作为原始相似函数的近似，即： $$Sim(Q,K)=\phi(Q)\phi(K)^{T}$$ 其中，自关注模块可以重写为: $$O_ {i}=\sum^{N}_ {j=1} \frac{\phi(Q_ {i})\phi(K_ {j})^{T}}{\sum^{N}_ {j=1}\phi(Q_ {i})\phi(K_ {j})^{T}}V_ {j}$$ 这样，我们就可以根据矩阵乘法的关联性质(如图1所示)，将计算顺序从 $(QK^{T})V$ 改为 $Q(K^{T}V)$ :  $$O_ {i}=\frac{\phi(Q_ {i})(\sum^{N}_ {j=1}\phi(K_ {j})^{T}V_ {j})}{\phi(Q_ {i})(\sum^{N}_ {j=1}\phi(K_ {j})^{T})}$$ 其中，相对于令牌数的计算复杂度降低到 $O(N)$ 。
    - 然而，目前的线性关注方法也面临着模型复杂性和表达性的两难困境。一方面，简单的近似，例如使用ReLU激活，过于松散，会导致显著的性能下降。另一方面，精心设计的核函数或矩阵分解方法可能会产生额外的计算开销。总的来说，线性注意力和Softmax注意力的实际表现之间仍然存在差距


# 二、做出的创新-Focused Linear Attention
- 在本文中，我们针对当前线性注意力方法的局限性，提出了一种新的 ***聚焦线性注意力*** 模块，该模块既高效又有表现力。具体来说，我们从两个角度分析了线性注意力表现下降的原因，并提出了相应的解决方案。首先，在以前的线性注意力模块中，注意力权重的分布相对平稳，缺乏解决信息量最大特征的聚焦能力。作为补救措施，我们提出了一个简单的映射函数来调整查询和关键字的特征方向，使注意力权重更容易区分。其次，我们注意到注意力矩阵的降秩限制了线性注意力的特征多样性。为了解决这个问题，我们提出了一个秩恢复模块，通过对原始注意力矩阵应用额外的深度卷积(DWC)，这有助于恢复矩阵秩，并保持不同的输出特征
- 我们使用五个高级视觉转换器模型，实证验证了我们的模块在图像分类、语义分割和对象检测任务上的有效性。研究结果表明，与所有基线和其他线性注意力方法相比，这些方法得到了一致的改进。


# 三、设计的模型
- 当将Transformer模型应用于视觉任务时，自注意的二次计算复杂性一直是一个挑战。另一方面，线性注意力通过精心设计的映射函数逼近Softmax运算，以其线性复杂性提供了一种更有效的替代方案。然而，当前的线性注意力方法要么遭受显著的性能退化，要么从映射函数引入额外的计算开销。在本文中，我们提出了一种新的聚焦线性注意力模块，以实现高效率和表现力。具体来说，我们首先从聚焦能力和特征多样性两个角度分析了导致线性注意力表现下降的因素。为了克服这些限制，引入了一个简单而有效的映射函数和一个有效的秩恢复模块，以增强自注意的表现力，同时保持低计算成本

- 虽然具有线性计算复杂度，但之前的各种研究也证明，简单地将Softmax注意力替换为线性注意力通常会导致严重的性能下降。在本节中，我们首先从聚焦能力和特征多样性两个角度详细分析了线性注意力的劣势。然后，我们介绍了聚焦线性注意力，充分解决了这些问题，实现了高效率和表达能力。
1. Focus ability
    - Softmax注意力实际上提供了一种非线性重加权机制，使其易于集中于重要特征。如图3所示，Softmax注意力的注意力图在某些区域上的分布尤其明显，例如前景对象。可比较的是，线性注意力的分布相对平稳，使其输出更接近所有特征的平均值，并且未能集中在信息量更大的区域。
    ![FLatten Transformer3](../pictures/Flatten%20Transformer/FLatten_Transformer3.png)

    - 作为补救措施，我们提出了一个简单而有效的解决方案，通过调整每个查询和键特性的方向，使相似的查询键对更接近，而将不相似的查询键对推开。具体来说，我们给出了一个简单的映射函数 $f_ {p}$ ，称为 ***聚焦函数*** : $$Sim(Q_ {i}, K_ {j})=\phi_ {p}(Q_ {i})\phi_ {p}(K_ {j})^{T}$$ $$where \quad \phi_ {p}(x)=f_ {p}(ReLU(x)), \quad f_ {p}(x)=\frac{||x||}{||x^{**p}||}x^{**p}$$ 并且 $x^{∗∗p}$ 表示 $x$ 的元素幂 $p$。我们遵循前面的线性注意力模块，首先使用ReLU函数来确保公式 $$O_ {i}=\frac{\phi(Q_ {i})(\sum^{N}_ {j=1}\phi(K_ {j})^{T}V_ {j})}{\phi(Q_ {i})(\sum^{N}_ {j=1}\phi(K_ {j})^{T})}$$ 中输入的非负性和分母的有效性。直接观察到映射后特征范数保持不变，即 $$||x||=||f_ {p}(x)||$$，说明只调整了特征方向。
    
    - 在此基础上，我们证明了在温和的假设下，所提出的映射函数 $f_ {p}$ 实际上影响了注意力的分布

    - **Proposition 1** (用 $f_ {p}$ 调整特征方向) *令 $x = (x_ {1} , \cdots , x_ {n}), y =(y_ {1} , · · · , y_ {n}) \in \mathbb{R}^{n}, x_ {i}, y_ {j} \ge 0$ 。假设 $x$ 和 $y$ 分别有单个最大值 $x_ {m}$ 和 $y_ {n}$。对于 $m=n$ 的特征 $\{ x, y \}$ 对* :  $$\exists p > 1, \quad s.t. \langle \varphi_ {p}(x), \varphi_ {p}(y) \rangle > \langle x, y \rangle$$ 对于 $m \neq n$ 的特征 $\{ x, y \}$ : $$\exists p > 1, \quad s.t. \langle \varphi_ {p}(x), \varphi_ {p}(y) \rangle < \langle x, y \rangle$$ *Proof* .  完整的证明请参阅原文附录

    - 因此，通过适当的 $p$ ，我们的聚焦函数 $f_ {p}(\cdot)$ 实际上实现了相似查询键对 $$\exists p > 1, \quad s.t. \langle \phi_ {p}(x), \phi_ {p}(y) \rangle > \langle x, y \rangle$$ 和不相似查询键对 $$\exists p > 1, \quad s.t. \langle \phi_ {p}(x), \phi_ {p}(y) \rangle < \langle x, y \rangle$$ 之间更明显的差异，恢复了原始Softmax函数那样尖锐的注意力分布。

    - 为了更好地理解，我们在图4中举了一个例子来说明 $f_ {p}$ 的作用。可以看出，$f_ {p}$ 实际上将每个向量“拉”到离它最近的轴上，而 $p$ 决定了这种“拉”的程度。通过这样做，$f_ {p}$ 有助于根据最近的轴将特征分成几组，提高每组内部的相似性，同时降低组之间的相似性。可视化与我们上面的分析一致。
    ![FLatten Transformer4](../pictures/Flatten%20Transformer/FLatten_Transformer4.png)

1. Feature diversity
    - 除了聚焦能力外，特征多样性也是制约线性注意表达能力的因素之一。其中一个可能的原因可能是注意力矩阵的等级，其中可以看到显著的差异。以DeiT-Tiny中 $N = 14 \times 14$ 的Transformer层为例，我们可以从图5(a)中看到，注意矩阵具有完整的秩(196个中的196个)，显示了从值中聚合特征时的多样性。
    ![FLatten Transformer5](../pictures/Flatten%20Transformer/FLatten_Transformer5.png)

    - 然而，这在线性注意力的情况下很难实现。实际上，线性注意中注意矩阵的秩是由每个头部的令牌数 $N$ 和通道维数 $d$ 来限定的: $$rank(\phi(Q)\phi(K)^{T}) \le min \{ rank(\phi(Q)), rank(\phi(K)) \}$$ $$rank(\phi(Q)\phi(K)^{T}) \le min \{ N, d \}$$ 其中，在普通视觉变换器设计中，$d$ 通常小于 $N$ ，例如，DeiT中的 $d=64，N=196$ 以及Swin Transformer中的 $d=32，N=49$ 。在这种情况下，注意力矩阵秩的上限被限制在较低的比率，这表明注意力图的许多行被严重同质化。由于自关注的输出是同一组 $V$ 的加权和，因此注意力权重的均匀化不可避免地导致聚合特征之间的相似性
    - 为了更好地说明，我们将DeiT-Tiny中原始的Softmax注意力替换为线性注意力，并在图5(b)中显示了注意力图的秩。可以看到，秩大大降低(196个中的54个)，并且注意矩阵的许多行相似。
    - 作为补救措施，我们提出了一个简单而有效的解决方案来解决这种线性注意力的限制。具体来说，在注意矩阵中加入深度卷积(DWC)模块，输出可表示为: $$O=\phi(Q)\phi(K)^{T}V + DWC(V)$$ 为了更好地理解这个DWC模块的效果，我们可以把它看作是一种关注，其中每个查询只关注空间中相邻的几个特征，而不是所有的特征 $V$ 。这种局部性保证了即使两个查询对应的线性关注值相同，我们仍然可以从不同的局部特征得到不同的输出，从而保持特征的多样性。DWC的影响也可以从矩阵秩的角度来解释。由上式可得:  $$O=(\phi(Q)\phi(K)^{T} + M_ {DWC})V=M_ {eq}V$$ 其中 $M_ {DWC}$ 表示为深度卷积函数对应的稀疏矩阵，$M_ {eq}$ 表示为等效的全注意图。由于 $M_ {DWC}$ 具有成为全秩矩阵的潜力，我们实际上增加了等效注意力矩阵的秩上界，这在极大地提高线性注意力性能的同时减少了计算开销。
    - 为了更好地说明，我们对DeiT-Tiny进行了类似的修改。通过增加DWC模块，可以将线性注意中注意图的秩恢复到满秩(196 out 196，如图5(c)所示)，保持了原有Softmax注意的特征多样性。

1. Focused linear attention module
    - 在此基础上，我们提出了一种新颖的线性注意模块，即聚焦线性注意模块，在保持表达能力的同时降低了计算复杂度。具体来说，我们首先设计了一个新的映射函数来模仿原始Softmax注意力的尖锐分布。在此基础上，我们针对以往线性关注模块的低秩困境，采用简单的深度卷积恢复特征多样性。通过这种方式，我们的新模块可以同时享受线性复杂性和高表达性的好处。具体来说，我们的模块可以表述为: $$O=Sim(Q,K)V=\phi_ {p}(Q)\phi_ {p}(K)^{T}V + DWC(V)$$
    - 总的来说，我们的模块有以下优点:
        1. Low computation complexity as linear attention：通过改变自注意的计算顺序，将复杂度从 $O(N^{2}d)$ 转换为 $O(nd^{2})$，其中 $N$ 和 $d$ 分别表示每个头部的令牌数和通道维数。在常见的视觉变压器设计中，$d$ 通常小于 $N$ ，如在DeiT中 $d= 64, N = 196$ ，在 Swin Transformer 中 $d= 32, N = 49$，实际上减少了整体计算量。此外，与以往设计复杂核函数的线性关注模块相比，我们提出的聚焦函数 $f_ {p}$ 仅采用简单算子，以最小的计算开销实现近似
        1. High expressive capability as Softmax attention：正如我们上面分析的那样，从聚焦能力和特征多样性的角度来看，以前基于核的线性注意力设计通常不如Softmax。利用所提出的聚焦函数 $f_ {p}$ 和深度卷积，我们的聚焦线性注意力可以达到比Softmax注意力更好的性能
    - 此外，我们的模块还具有适应更大的接收域和不同的模型架构的潜力。基于Softmax注意力的现代Transformer模型主要使用有限数量的键/值对，因为对令牌数的二次复杂度。然而，我们的模块的线性复杂性使我们能够在保持相同计算量的情况下将接受域扩展到更大的区域，并享有建模远程依赖关系的优势。此外，我们的模块可以作为一个插件模块，并很容易地采用在各种现代视觉变压器架构。我们在DeiT、PVT、PVT-v2、Swin Transformer和CSwin Transformer五种高级模型上进行了实证实现。考虑到接受野扩大的优势，我们在视觉变形的早期阶段采用聚焦的线性注意块，其余块保持不变。详细的模型架构见附录。


# 四、实验结果
- 在本节中，我们将在我们集中的线性关注中删除关键组件，以验证这些设计的有效性。我们报告了基于FLatten-DeiT-T和FLatten-Swin-T的ImageNet-1K分类结果。
1. Focused function $f_ {p}$ and DWC
    - 我们首先评估了我们提出的聚焦函数 $f_ {p}$ 和深度卷积的有效性。我们从线性注意开始，依次引入 $f_ {p}$ 和DWC。如表4所示，采用所提出的聚焦函数 $f_ {p}$ 可获得+1.3的改进。使用DWC来保持特征多样性进一步导致+2.3的精度增益，实现74.1的总体精度。这些结果证明，我们提出的 $f_ {p}$ 和DWC可以大大提高线性注意力的表达能力，从而帮助我们的聚焦线性注意力模块获得比Softmax注意力更好的性能。
    ![FLatten Transformer Table4](../pictures/Flatten%20Transformer/FLatten_Transformer_Table4.png)

1. Ablation on different $p$
    - 在表5中我们研究了聚焦因子 $p$ 对模型性能的影响。我们发现当 $p$ 在2和32之间变化时，Top-1 的分类精度变化不大，这意味着我们的模块对这个超参数具有鲁棒性。实际上，我们对文中所有模型都选择 $p = 3$ 而不进行额外的调整。
    ![FLatten Transformer Table5](../pictures/Flatten%20Transformer/FLatten_Transformer_Table5.png)

1. Receptive field
    - 我们还研究了基于FLatten-Swin-tiny的感受野的影响。如表6所示，随着窗口大小的增加，我们的模型的性能逐渐提高。这进一步证明，尽管窗口关注是有效的，但从全球角度来看，它不可避免地牺牲了自我关注的长期依赖性，仍然不如全球关注。对于线性复杂性，我们的模块有可能在保持相同计算量的同时实现大的甚至全局的感受野。
    ![FLatten Transformer Table6](../pictures/Flatten%20Transformer/FLatten_Transformer_Table6.png)

1. Focused linear attention at different stages
    - 我们在不同阶段用我们的模块替换Swin-T的shift-window注意力。如表7所示，我们可以看到替换前两个阶段导致性能增益0.8，而替换后两个阶段会略微降低整体精度。我们将这一结果归因于Swin的前两个阶段具有更大的分辨率，并且更适合我们具有大接受场的模块。
    ![FLatten Transformer Table7](../pictures/Flatten%20Transformer/FLatten_Transformer_Table7.png)

## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间


# 五、结论
- 在本文中，我们提出了一种新颖的聚焦线性注意模块。通过从焦点能力和特征多样性的角度解决以往线性关注方法的局限性，我们的模块实现了高效率和表达能力的令人印象深刻的结合。大量的图像分类、目标检测和语义分割实验表明，我们的模块可以广泛应用于各种视觉变形器，并在计算效率和模型性能之间实现更好的权衡。
## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题


# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
    - DWC提高了局部聚焦能力，但不能聚焦于任何位置，而fp实际上增强了模型的聚焦能力，帮助模型聚焦于更多信息区域。结合fp和DWC，我们的聚焦线性注意力模块恢复了原始Softmax注意力的尖锐分布
    1. 提出 $f_ {p}$ 的数学方法，并给出了一个命题（在附录里证明的），增强特征的聚焦程度，也就是 query 和 key 越相似的特征越突出。目的是将各个特征向量的 query 和 key 都拉向他们最靠近的轴。也就是说，让拉伸后的向量经过自注意力机制，靠近 query 的向量会越靠近与1，越远离 key 的向量会越靠近0。存在一个问题，对一个图像而言，特征向量有多长，这个方法最多能支持多少特征的表示
    1. 提出 DWC 的方法，增强线性方法能够表征的特征数量，解决特征同质化的问题。虽然看起来 $f_ {p}$ 是能使特征图突出注意点的重要工具，但是在附录中的图可以看出，核心的 Attention 点位是由 DWC 方法给出的，而 $f_ {p}$ 给出的是更多的、相关的特征
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论