# 计算机视觉-Transformer

## 目录
- [笔记](#笔记)
- [CLIP相关工作](#clip相关工作)
- [返回上一层 README](../README.md)


## 笔记

| 笔记 | 年份 | 名字                                                         | 简介                 | 引用 |
| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
| [✅](./papers/AN%20IMAGE%20IS%20WORTH%2016X16%20WORDS%20TRANSFORMERS%20FOR%20IMAGE%20RECOGNITION%20AT%20SCALE.md) | 2021 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) | Transformer杀入CV界                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7b15fa1b8d413fbe14ef7a97f651f47f5aff3903%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Image-is-Worth-16x16-Words%3A-Transformers-for-at-Dosovitskiy-Beyer/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903)  |
| [✅](./papers/Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision.md) | 2021 |  [CLIP](https://openai.com/blog/clip/) | 图片和文本之间的对比学习                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  |
| [✅](./papers/Swin%20Transformer%20Hierarchical%20Vision%20Transformer%20using%20Shifted%20Windows.md) | 2021 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) | 多层次的Vision Transformer                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc8b25fab5608c3e033d34b4483ec47e68ba109b7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7) |
| | 2021 | [MLP-Mixer](https://arxiv.org/pdf/2105.01601.pdf) | 使用MLP替换self-attention            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2def61f556f9a5576ace08911496b7c7e4f970a4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MLP-Mixer%3A-An-all-MLP-Architecture-for-Vision-Tolstikhin-Houlsby/2def61f556f9a5576ace08911496b7c7e4f970a4)  |
| [✅](./papers/Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners.md) | 2021 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) | BERT的CV版             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1962a8cf364595ed2838a097e9aa7cd159d3118%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/c1962a8cf364595ed2838a097e9aa7cd159d3118)  |
| [✅](./papers/FLatten%20Transformer%20Vision%20Transformer%20using%20Focused%20Linear%20Attention.md) | 2023 | [FLatten Transformer](https://arxiv.org/pdf/2308.00442.pdf) | 聚焦的线性注意力模块             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F24c5296ad98606192afe5003ef3f0d79aa19e321%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/FLatten-Transformer%3A-Vision-Transformer-using-Han-Pan/24c5296ad98606192afe5003ef3f0d79aa19e321)  |
|  | 2023 | [SG-Former](https://arxiv.org/pdf/2308.10916.pdf) | 具有自适应细粒度的有效全局Self-Attention             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F740512dfadd91cd52771f2a721509c371efb9b89%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/SG-Former%3A-Self-guided-Transformer-with-Evolving-Ren-Yang/740512dfadd91cd52771f2a721509c371efb9b89)  |


*[跳转至目录](#目录)*


## CLIP相关工作

| 笔记 | 年份 | 名字                                                         | 简介                 | 引用 |
| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
| [✅](./papers/Language-driven%20Semantic%20Segmentation.md) | 2022 | [Lseg](https://arxiv.org/pdf/2201.03546.pdf) | CLIP用在分割领域                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8)  |
| [✅](./papers/GroupViT%20Semantic%20Segmentation%20Emerges%20from%20Text%20Supervision.md) | 2022 |  [GroupViT](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.pdf) | CLIP用在分割领域                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b)  |
| [✅](./papers/Open-vocabulary%20Object%20Detection%20via%20Vision%20and%20Language%20Knowledge%20Distillation.md) | 2022 | [ViLD](https://arxiv.org/pdf/2104.13921.pdf) | CLIP在目标检测领域的应用                  | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14) |
|[✅](./papers/Grounded%20Language-Image%20Pre-training.md) | 2022 | [GLIP](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf) | CLIP在目标检测的应用，对标分割的GroupViT            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8)  |
| [✅](./papers/CLIPasso：Semantically-Aware%20Object%20Sketching.md) | 2022 | [CLIPasso](https://arxiv.org/pdf/2202.05822.pdf) | 用简笔画表示物体特征             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52)  |
| [✅](./papers/CLIP4Clip%20An%20Empirical%20Study%20of%20CLIP%20for%20End%20to%20End%20Video%20Clip%20Retrieval.md) | 2021 | [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) | 用简笔画表示物体特征             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4)  |
| [✅](./papers/ActionCLIP%20A%20New%20Paradigm%20for%20Video%20Action%20Recognition.md) | 2021 | [ActionCLIP](https://arxiv.org/pdf/2109.08472.pdf) | CLIP进入动作识别             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349)  |
| [✅](./papers/HOW%20MUCH%20CAN%20CLIP%20BENEFIT%20VISION-ANDLANGUAGE%20TASKS.md) | 2021 | [VisionLanguageTask](https://arxiv.org/pdf/2107.06383.pdf) | CLIP用回到Vision Language            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8f167ec1149921fac63b1ea855443de109bb013a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/How-Much-Can-CLIP-Benefit-Vision-and-Language-Tasks-Shen-Li/8f167ec1149921fac63b1ea855443de109bb013a)  |
| [✅](./papers/AudioCLIP%20Extending%20CLIP%20to%20Image,%20Text%20and%20Audio.md) | 2021 | [AudioCLIP](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747631) | CLIP进入语音领域             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ffda4530df9eec0e3f714dba3459ac50dab17d89c%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Audioclip%3A-Extending-Clip-to-Image%2C-Text-and-Audio-Guzhov-Raue/fda4530df9eec0e3f714dba3459ac50dab17d89c)  |
| [✅](./papers/Can%20Language%20Understand%20Depth.md) | 2022 | [Can Language Understand Depth](https://arxiv.org/pdf/2207.01077.pdf) | 深度估计图             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92)  |


*[跳转至目录](#目录)*