# Transformer Structure

## 目录
- [笔记](#笔记)
- [返回上一层 README](../README.md)


## 笔记

| 笔记 | 年份 | 收录 | 名字                                                         | 简介                 | 引用 |
| ------ | ---- | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
|  | 2020 | ICLR | [Self-Attention vs Convolutional Layer](https://arxiv.org/pdf/1911.03584.pdf) | 自注意力与卷积的关系                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1ce1d287e825c78d381a95c0134e080bf1310611%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/On-the-Relationship-between-Self-Attention-and-Cordonnier-Loukas/1ce1d287e825c78d381a95c0134e080bf1310611)  |
|  | 2020 | ICML | [Transformer vs RNN](https://arxiv.org/pdf/2006.16236.pdf) | Transformer与时序网络的关系               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f68e1bb253925d8431588555d3010419f322e04%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Transformers-are-RNNs%3A-Fast-Autoregressive-with-Katharopoulos-Vyas/6f68e1bb253925d8431588555d3010419f322e04)  |
|  | 2023 | arXiv | [Transformer vs SVM](https://arxiv.org/pdf/2308.16898.pdf) | Transformer与支持向量机的关系                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff9a4ed62ea6da274c1c81748b2bca240655b7c29%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Transformers-as-Support-Vector-Machines-Tarzanagh-Li/f9a4ed62ea6da274c1c81748b2bca240655b7c29)  |
|  | 2023 | arXiv | [White-Box Transformers](https://arxiv.org/pdf/2306.01129.pdf) | 白箱Transformer                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe94804f8df0e5a3eff6f0a278606d60dcb767d56%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/White-Box-Transformers-via-Sparse-Rate-Reduction-Yu-Buchanan/e94804f8df0e5a3eff6f0a278606d60dcb767d56)  |


*[跳转至目录](#目录)*