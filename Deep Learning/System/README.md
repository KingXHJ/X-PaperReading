# System

## 目录
- [笔记](#笔记)
- [返回上一层 README](../README.md)


## 笔记

| 笔记 | 年份 | 收录 | 名字                                                         | 简介                 | 引用 |
| ------ | ---- | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |
| [✅](./papers/Scaling%20Distributed%20Machine%20Learning%20with%20the%20Parameter%20Server.md)  |  2014 | Operating Systems Design and Implementation | [参数服务器](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | 支持千亿参数的传统机器学习模型       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Scaling-Distributed-Machine-Learning-with-the-Li-Andersen/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2)  |
| [✅](./papers/GPipe%20Efficient%20Training%20of%20Giant%20Neural%20Networks%20using%20Pipeline%20Parallelism.md)  | 2018 | NeurIPS | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | 流水线（Pipeline）并行      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc18663fea10c8a303d045fd2c1f33cacf9b73ca3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GPipe%3A-Efficient-Training-of-Giant-Neural-Networks-Huang-Cheng/c18663fea10c8a303d045fd2c1f33cacf9b73ca3)  |
|  | 2019 | arXiv | [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) | 张量（Tensor）并行      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a) |
|  | 2020 | International Conference for High Performance Computing, Networking, Storage and Analysis | [Zero](https://arxiv.org/pdf/1910.02054.pdf) | 参数分片      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ZeRO%3A-Memory-optimizations-Toward-Training-Trillion-Rajbhandari-Rasley/00c957711b12468cb38424caccdf5291bb354033)  |
|  | 2022 | MLSys | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) |  将Jax拓展到上千TPU核上       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Pathways%3A-Asynchronous-Distributed-Dataflow-for-ML-Barham-Chowdhery/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352)  |


*[跳转至目录](#目录)*