# 论文信息
- 时间：2014
- 期刊：NeurIPS
- 网络名称： GAN
- 意义：生成模型的开创工作	
- 作者：
![GAN author](../pictures/GAN%20author.png)
- 实验环境：
- 数据集：
# 一、解决的问题
1. 之前的方法总是想去构造出一个分布函数出来，提供一些可学习的参数，然后最大化似然函数，但是在高维的去情况下，非常难计算
2. VAE工作和NCE的工作
# 二、做出的创新
1. 提出了一个新的模型架构，通过对抗的过程来估计生成模型
2. 同时训练两个模型：
    - 生成模型G：用来抓住数据分布。让辨别模型D犯错。
    - 辨别模型D：估计一个样本到底是从真正的训练数据出来的，还是由G生成出来的
3. 如果G和D都是MLP的话，就可以用BP
4. 模型里不需要使用隐马尔可夫链或者推理展开（比别人的方法简单）
# 三、设计的模型
1. 最简单的应用就是：当生成器和辨别器都是MLP的时候
2. 生成器：
    - 生成器在数据 $x$ 上学习一个 $p_g$ 的分布
    - 定义一个先验的噪音分布变量 $p_{z}(z)$ (高斯噪声，均值为0方差为1)
    - 学习一个MLP，使得 $z$ 可以映射到 $G(z ; \theta _g)$
3. 辨别器：
    - 辨别器也是个MLP，有自己可以学习的 $D(x ; \theta _d)$
    - 输出一个标量，表示真实数据或者生成数据，二分类分类器
4. 训练 $G$ 来最小化 $log(1-D(G(z)))$

5. 总之就是训练D和G：

    $$\underset{G}{min} \underset{D}{max} V(D,G) = \mathbb{E}_ {x \sim p_ {data} (x)} [ log D(x) ] + \mathbb{E}_ {z \sim p_z (z)} [ log(1-D(G(z))) ]$$

    - 在完美的情况下， $D(x)=1, (1-D(G(z)))=1$ ，最后整个式子结果应该是0，即最大化 $logD(x)$ ，最小化 $D(G(z))$
    
6. 证明优化函数没问题：

    $$D ^ {\*} _ {G} (x) = \frac{p_ {data}(x)}{p_ {data}(x)+p_ g(x)}$$
    
    - '\*'代表最优解
    ![GAN identify](../pictures/GAN%20identify.png)

    ![GAN KL](../pictures/GAN%20KL.png)

7. 算法1能优化目标函数
    ![GAN indentify algorithm](../pictures/GAN%20identify%20algorithm.png)

# 四、实验结果
1. GAN在做什么
    ![GAN do](../pictures/GAN%20do.png)
    - $x$ 是黑色，生成器是绿色，辨别器是蓝色
    - 噪音 $z$ 是均匀分布，企图映射成高斯分布的 $x$
    - 生成器逐渐靠近训练数据分布，使得辨别器没有优化的空间了
2. GAN的算法：
    ![GAN algorithm](../pictures/GAN%20algorithm.png)
    - 先采样m个训练样本和m个噪音样本
    - 放进价值函数里面求辨别器梯度，更新辨别器
    - 再采样m个噪音样本
    - 放到函数的第二项里面（通过噪音生成再辨别），算生成器梯度，更新
    - $k step$ 是超参数
## 1、比之前模型的优势

## 2、有优势的原因
1. 本身无监督学习
2. 用了有监督学习的优化函数
## 3、改进空间
1. 早期的时候G特别弱，根据公式的第二项，很容易把D训练的特比好
    - 解决方法，更新G的时候，把函数 $log(1-D(G(z)))$ 改成最大化 $logD(G(z))$

2. 算法难跑，且难以判断收敛
3. 训练非常难
# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论
