# 论文信息
- 时间：2014
- 期刊：Operating Systems Design and Implementation(Conference)
- 网络名称： 参数服务器
- 意义：支持千亿参数的传统机器学习模型
- 作者：Mu Li∗‡, David G. Andersen∗, Jun Woo Park∗, Alexander J. Smola∗†, Amr Ahmed†, Vanja Josifovski†, James Long†, Eugene J. Shekita†, Bor-Yiing Su†; ∗Carnegie Mellon University ‡; Baidu †; Google
- 实验环境：
- 数据集：

# 0、一些介绍
1. 系统会议上的论文内容有:
    - 操作系统内核
    - 数据的抽象
        - 可编程SSD（固态硬盘）
    - 安全
    - 网络
        - GPU上网络编程的抽象
    - 云
    - Debug
    - 分布式系统里面的事务
    - 重放事件
    - 机器学习
        - 图
2. 学术界和工业界对大数据的理解：
    - 学术界：数据可以不用邮件发了，转而用U盘去拷贝，数据量级GB
    - 工业界：数据量级TB

# 一、解决的问题
1. 使用大规模机器学习
2. 提出了一个参数服务器的框架，用来针对于分布式的机器学习任务
    - 数据和任务都分布在一些任务（工作）节点上
    - 有一些服务器节点来维护一个全局共享的参数
    - 这些参数通常会表示成一个稠密的或者稀疏的向量/矩阵
    - 这个框架去管理一些异步的数据通信，并且支持一些灵活的一致性模型，弹性的可扩展性和持续的容灾
3. 分布式的优化和推理现在已经成为了解决大规模机器学习问题的一个前置的条件
4. 大规模数据难以用一台机器完成
5. 做一个性能很好的分布式算法很难（痛点）
    - 计算复杂度很高
    - 数据通讯量很大
    - 有效的处理以上两点是一个高效的分布式系统要考虑的问题
6. TB和PB级别的数据去做一个全局共享的模型
    - 所有计算节点都要去访问这个模型
    - 列举三个问题：
        - 所有计算节点访问这些参数，会导致大量的网络通讯
        - 机器学习算法都是顺序的模型，导致大量的全局同步，影响性能
        - 计算规模大，要用成百上千台机器做计算的时候，容灾就很严重
            - 如果计算组中的机器出现了不能计算的问题，需要保证其他机器能够正常的计算下去
            ![parameter server statistics](../pictures/parameter%20server%20statistics.png)
            - （论文第一页右上角非常重要）
            - 跑大任务的时候，失败的概率很大
            - 机器挂了的原因：
                - 磁盘满了
                - 机器使用权被抢走
                - 基础架构不稳定
                - 过热
7. 工程上的挑战
    1. 如果要处理分布式的一些数据分析的问题，就需要不断地去读和写全局的参数
    2. 参数服务器提供的就是怎样去汇聚和同步这些计算节点，和它们的统计信息
    3. 每一个参数的服务节点，会去维护这些全局共享参数的一部分。因为参数非常的大，一台机器放不下，所以会有很多的机器一起来维护
    4. 每一个计算节点叫work node，他就会每次去拿整个参数里面的一块，再读些数据进行一个计算
    5. 这会带来两个关键的挑战：
        - 一个是通讯，计算节点不断地向服务节点要数据
            - 这个在分布式系统里叫key value
            - 但是市面上的key value的data store（不是一个数据库，就是一个纯数据），它们提供的抽象不是很够的（key可以理解为某一个参数的下标），每次拿出一个key肯定是不高效的，一个一个去发浮点数
            - 机器学习的数据结构通常是一个结构化的数学物体，比如说向量、矩阵或者张量，所以每次更新不是去一个一个去发浮点数，而是去发一段一段的东西，因此要做一个segment
            - 不过要考虑并不是所有的机器学习算法都是这样子的。比如稀疏的Logistic，这就不是将整个结构发出去，而是提取稀疏中的非零元，所以，用segment是更好的选择，可能是向量中的一段，或者是矩阵中的一整行
        - 容灾：一台机器挂了，不需要把整个任务去重启，而是从上一个保存点重启
            - 这里用到的技术是，对每一个服务器节点，去做一个实时的复制，使得一台节点挂了，但是数据在另一个地方还会有，这个在分布式系统用得很多，但是机器学习用的不多
            - 对计算节点来说，因为不用存全局共享参数，所以可以动态调整
                ![parameter server ml performed](../pictures/parameter%20server%20ml%20performed.png)
                - 坐标是对数化的

                ![parameter server data analysis](../pictures/parameter%20server%20data%20analysis.png)
                - 数据结构

# 二、做出的创新
**Related work中的机器学习部分值得一看，讲的全面而且精炼**
1. 第三代开源的参数服务器框架
    - 两个好处：
        - 把之前框架里面共同共享的模块抽象出来之后，实现用户任务的时候就会比较简单
        - 能够适配多种不一样的算法
            - 稀疏的logistic回归
            - LDA
            - 分布式的Sketching
2. 根据真实的任务（重点）
    - 提供了5个关健特征
        1. 有效通讯
            - 异步通讯
            - 针对机器学习算法做了大量的压缩，通讯量降低了一个规模
        2. 提供了一个灵活的、一致性的模型
            - 一致性是说：一个节点对同样一个参数在任何时刻拿到的值是否相同
                - 强一致性适合对优化算法要求较高的机器学习
                - 弱一致性适合对系统的高效性来说更好一点
        3. 弹性的可扩展性
            - 训练的时候可以加入新的节点，而且整个任务不会挂掉
        4. 容灾
            - 机器出现问题的时候，要花多长时间恢复过来
            - 李沐老师做到了，几台机器挂掉的时候，可以在1s之内恢复回来
            - 用到的技术是Vector clock（向量钟）
        5. 用起来简单
            - 2014年工业界很少用python来开发，也没有numpy这种矩阵有好的工具，主要还是C++
            - 全局的parameter可以抽象成一个向量或者矩阵
3. 新颖度
    - 找到了合适的系统技术适配到机器学习算法里
    - 并且改变这些机器学习的算法使得它更加的系统友好
    - 放弃掉了分布式系统里要求特别高的要求
        - 一致性
    - 同时对机器学习的算法做了一些修改，使他能够容忍这些丢到的一致性
    - 这些结果就会得到了**第一个通用的机器学习系统，能够真的做到工业界的大小**
        - 第一个有两个条件：
            - 通用的（当时的工业界都是特殊开发的系统）
            - 工业界大小


# 三、设计的模型
**设计原则：简单、实用**
1. 参数服务器的架构图
    ![parameter server structure](../pictures/parameter%20server%20structure.png)
    - 设计目的
        - 尽量的通用
    - server group
        - 服务管理器
            - 跟资源管理去通讯，请求服务节点的数量以及管理和维护服务节点，即参数在哪一块
            - 管理容灾
    - worker group
        - 任务调度器
            - 调度优化算法（for loop）怎么去执行
            - 向资源管理器请求资源去跑
            - 管理具体干活的计算节点
        - 计算节点
            - 拿到任务之后就去读数据
            - 和服务器节点交互（收发梯度、收发权重）
        - 有多个工作组
            - 用于干并行的任务
            - 进行隔离和通信
    - training data
    - resource manager
2. (Key, Value)Vectors
    - 是参数服务器对数据的一个抽象
    - Key可以认为是参数矩阵（向量或张量）的一个下标
        - 很多场景下，下标不一定是连续的
        - 是哈希出来的值，可以当成一个特别长的稀疏向量
    - Value可以是一个浮点数或者是一个向量甚至是深度神经网络里面一层的一个W
    - 和一般的分布式系统不同，这里的Key和Value不一定是字符串，可能是int64或者int128。这样的好处是在实现算法的时候比较简单，拿到的就是一个很长的向量，就可以用一些数值运算库，直接写矩阵乘法的代码，而不是写成最原始的C++写法
3. 通讯接口Push & Pull
    - 支持两个通讯的方法
        - 把梯度push回server节点
        - 把权重从server节点pull回来
        - （参考了git的命名方式）
    - key是int64或者int128，带着key走很占带宽
        - 零散着发，做一个带区间的push和pull
        - 允许把一段区间内的所有Key和Value发出去
        - 这种方法对绝大多数机器学习算法都可行的
        - 灵活、简单、容易做优化
4. 服务端允许用户定义的函数
    - 服务器的代码理论上不应该去动，涉及安全的问题
    - 但是又需要在服务器端进行编程
    - 服务器在保证安全的情况下，开放部分API供用户自定义
5. 异步的任务和关系
    - 任务就是一个抽象，就是说一段函数
    - 任务在实现里面叫做，远端的一个函数执行（rpc）
        - 一台机器，向另外一台机器发送与一个远端的请求，去完成一些任务（push/pull就是一个rpc）
        - 为了优化性能，核心观点是，所有的任务是可以异步执行的（即发送请求后，可以不用处于等待状态，继续其他工作，这样节省计算资源）
    - 然而有依赖性的任务怎么办
        - 通过execute-after-finished
        - 即等到依赖的任务完成，再进行下一步
            ![parameter server dependency](../pictures/parameter%20server%20dependency.png)
            - 有一个问题是，iter10正在发送的时候，iter11开始计算了，但是iter11用的是旧的梯度
            - iter12是必须等iter11完成之后，才能开始计算
6. 通过加依赖组成灵活的一致性模型
    ![parameter server consistency](../pictures/parameter%20server%20consistency.png)
    - 展示了三个常用的一致性模型
        - 圆圈和数字表示迭代或者小批量
        - 箭头表示依赖
        - 第一个是机器学习最常见的顺序执行
        - 第二个是彼此之间没有特定依赖，可能iter1还用的是老权重，但是iter2用的就是新的权重
        - 第三个是允许有延迟，但是不允许延迟太多
7. 用户定义过滤器
    - 问题是要发送大量数据，但是这些数据不一定总是必要的，可以用过滤器来减少一些信息的发送
    - significantly modified filter
        - 假设一个权重W，在上一轮中，没有改变很多的情况下，就不发送它了。这样如果有大量的权重，没有太更新的话，就会节省很多时间

# 四、模型实现
1. Vector Clock
    - 在分布式系统里是一个非常强大的概念
    - 假设有非常多个计算节点，要访问数据W
    - 它们的时间是不一样的，因为有异步性
    - 想要维护不同时间节点的不同参数，这是一个成本很高的事情，等价说，把权重复制计算节点个数次，这是无法承受的
    - 这里用的技术是，因为每次发送的是一个区间的Key，可以放松到每一段来计算哪一个节点在什么地方
        - 比如深度学习中，语言模型可能有几十亿几百亿的一个权重，但是每一次可能只是对一个层进行发送，那么层数也就十几或者几百的样子，所以在做vector clock计时的时候，只要去算每一个层，每一个节点，用的是哪一个时间算好的值即可，这样存储量就降低了
2. Message
    - 通讯Key和Value
    - 每次通讯一个区间
        - 首先告知通讯的时间，然后是Key和Value，但是我们不喜欢发Key，更重要的是梯度和Value
        - 如果发过的东西没有变化的话，就不需要重复发送了
        - 做法就是把这些Key的值重新哈希一次，发送出去，sever端看到存的哈希值一样，就可以省掉发送了
        - 实现上通过filter
3. Consistent Hashing(一致性的哈希，和区块链很像)
    - 核心思想是server端怎么存权重（做热备份）
        ![parameter server consistent hash](../pictures/parameter%20server%20consistent%20hash.png)
        - 左图表示一个Key的环（一致性哈希环），在环里面分段，可以随机插进去
        - 每一段由一个服务器节点来维护，同时也会去存下面的两段作为备份
        - 这样每一段最多允许两个server挂掉，还可以保持继续执行
        - 加入新的节点，只需要插入进去即可
        - 问题是，增加了带宽（重要问题）和延迟
        - 解决方法是，做汇聚，但是会增加延迟（在机器学习中，对延迟不是那么看重，可以通过异步来隐藏）
        - 这样server就可以容灾了
    - worker 容灾？
        - worker有个schedule，会去看各个worker在自己任务上的完成度（ping，看通不通）
        - 如果ping不通了，就把发给之前worker的任务发给别人去做，或者问系统在请求一个worker
# 五、实验结果
1. PB级别的真实数据
2. 任务里面含有了10亿级别的样本和参数
3. 实验包括：
    - 稀疏的Logistic回归
    - LDA式的一个聚类算法
    - 一个分布式的Sketching
## 1、比之前模型的优势
![parameter server test1](../pictures/parameter%20server%20test1.png)
![parameter server test2](../pictures/parameter%20server%20test2.png)
- System-B和Parameter Server算法基本一样
    - B没用异步，因此收敛时间短，但是等待时间长
    - 参数服务器用了异步，收敛时间长，但是等待时间大大降低
![parameter server test3](../pictures/parameter%20server%20test3.png)
- 省时间的原因
    - server: 用了filter和哈希，而且压缩稀疏矩阵会大大节省空间
    - worker: 发送的是梯度，不是那么稀疏，所以压缩的效果并不如server那么明显
        - KKT是刻画对一个凸函数来说，离最优解有多远
![parameter server test4](../pictures/parameter%20server%20test4.png)
- 放不放松一致性
    - 不放松一致性的话，计算时间较少，但是等待时间就会很长
    - 放松一致性会增加计算量，但是减少等待时间
    - 不过放松程度是有最优解的
## 2、有优势的原因

## 3、改进空间
1. 异步通信对当时的机器学习人太过于高端了，简化后叫pslite，放在了mxnet里面作为分布式的后端
2. 对分布式来讲，机器学习确实是一个简单的任务，模型大小除以计算量是非常小的，说明通讯不是瓶颈，而是计算量，而且数据量也不是那么的大，也不需要成百上千台机器进行训练
3. 站在百度和Google的机器学习肩膀上，做了数据的压缩，灵活的一致性模型和容灾
# 六、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 七、代码
![parameter server algorithm](../pictures/parameter%20server%20algorithm.png)
# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论