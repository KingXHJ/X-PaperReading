# 论文信息
- 时间：2016
- 期刊：ECCV
- 网络/算法名称：SSD
- 意义：⽤单个深度神经⽹络检测图像中对象
- 作者：Wei Liu1, Dragomir Anguelov2, Dumitru Erhan3, Christian Szegedy3, Scott Reed4, Cheng-Yang Fu1, Alexander C. Berg1; 1UNC Chapel Hill, 2Zoox Inc., 3Google Inc., 4University of Michigan, Ann-Arbor
- 实验环境： Nvidia Titan X
- 数据集：
# 一、解决的问题
1. 我们提出了⼀种使⽤单个深度神经⽹络检测图像中对象的⽅法。我们的⽅法名为 SSD，将边界框的输出空间离散化为⼀组默认框，每个特征图位置具有不同的纵横⽐和⽐
例。在预测时，⽹络为每个默认框中的每个对象类别的存在⽣成分数，并对框进⾏调整以更好地匹配对象形状。此外，该⽹络结合了来⾃多个具有不同分辨率的特征图的预测，以⾃然地处理各种⼤⼩的对象。 SSD 相对于需要对象提议的⽅法来说很简单， ***因为它完全消除了提议⽣成和后续像素或特征重采样阶段，并将所有计算封装在单个⽹络中*** 。这使得 SSD 易于训练，并且可以直接集成到需要检测组件的系统中。在 PASCAL VOC、COCO 和 ILSVRC 数据集上的实验结果证实，SSD 与使⽤额外对象建议步骤的⽅法相⽐具有竞争⼒的准确性，并且速度更快，同时为训练和推理提供了统⼀的框架。对于 300 × 300 输⼊，SSD在 Nvidia Titan X 上以 59 FPS 的 VOC2007 测试达到 74.3% mAP1，对于 512 × 512 输⼊，SSD 达到 76.9% mAP，优于可⽐的最先进的 Faster R-CNN模型。与其他单阶段⽅法相⽐，即使输⼊图像尺⼨较⼩，SSD 也具有更好的精度

2. 当前最先进的对象检测系统是以下⽅法的变体：假设边界框，为每个框重新采样像素或特征，并应⽤⾼质量分类器。⾃从选择性搜索⼯作到基于 Faster R-CNN 的 PASCAL VOC、COCO 和 ILSVRC 检测的当前领先结果以来，该管道在检测基准上占主导地位，尽管具有更深层次的特征，例如 ResNet 。虽然准确，但这些⽅法对于嵌⼊式系统来说计算量太⼤，⽽且即使使⽤⾼端硬件，对于实时应⽤程序来说也太慢了。这些⽅法的检测速度通常以每帧秒数 (SPF) 来衡量，即使是最快的⾼精度检测器 Faster R-CNN，也只能以每秒 7 帧 (FPS) 的速度运⾏。已经有很多尝试通过攻击检测管道的每个阶段来构建更快的检测器（参⻅第 4 节中的相关⼯作），但到⽬前为⽌，速度的显着提⾼只是以检测精度显着降低为代价的。

3. 相关工作
    1. 图像中的⽬标检测⽅法有两类，⼀类基于滑动窗⼝，另⼀类基于区域提议分类。在卷积神经⽹络出现之前，这两种⽅法的最新技术[可变形部件模型 (DPM)](http://people.csail.mit.edu/torralba/courses/6.870/papers/FelzenszwalbMcAllesterRamanan.pdf)和[选择性搜索](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)具有相当的性能。然⽽，在结合了选择性搜索区域建议和基于卷积⽹络的后分类的 R-CNN 带来的巨⼤改进之后，区域建议对象检测⽅法变得普遍。

    1. 原始的 R-CNN ⽅法已经以多种⽅式进⾏了改进。第⼀组⽅法提⾼了后分类的质量和速度，因为它需要数千个图像裁剪的分类，这是昂贵且耗时的。SPPnet 显着加快了原始 R-CNN ⽅法的速度。它引⼊了⼀个空间⾦字塔池化层，该层对区域⼤⼩和⽐例更稳健，并允许分类层重⽤通过以多种图像分辨率⽣成的特征图计算的特征。 Fast R-CNN 扩展了 SPPnet，因此它可以通过最⼩化置信度和边界框回归的损失来端到端地微调所有层，这是在 MultiBox 中⾸次引⼊的，⽤于学习⽬标性。

    1. 第⼆组⽅法使⽤深度神经⽹络提⾼提案⽣成的质量。在 MultiBox 等最新作品中，基于低级图像特征的选择性搜索区域提议被直接从单独的深度神经⽹络⽣成的提议所取代。这进⼀步提⾼了检测精度，但导致设置有些复杂，需要训练两个神经⽹络，并且它们之间存在依赖关系。 Faster R-CNN ⽤从区域建议⽹络 (RPN) 中学习的建议替换选择性搜索建议，并引⼊了⼀种通过在微调共享卷积层和预测层之间交替将 RPN 与 Fast R-CNN 集成的⽅法这两个⽹络。通过这种⽅式，region proposals ⽤于汇集中级特征，最终的分类步骤成本更低。我们的 SSD 与 Faster R-CNN 中的区域建议⽹络 (RPN) ⾮常相似，因为我们也使⽤⼀组固定的（默认）框进⾏预测，类似于 RPN 中的锚框。但是，我们不是使⽤这些来合并特征并评估另⼀个分类器，⽽是同时为每个框中的每个对象类别⽣成⼀个分数。因此，我们的⽅法避免了将 RPN 与 Fast R-CNN 合并的复杂性，并且更容易训练、更快、更直接地集成到其他任务中。

    1. 与我们的⽅法直接相关的另⼀组⽅法完全跳过建议步骤并直接预测多个类别的边界框和置信度。 OverFeat 是滑动窗⼝⽅法的深度版本，在了解底层对象类别的置信度后，直接从最顶层特征图的每个位置预测边界框。 YOLO 使⽤整个最顶层的特征图来预测多个类别和边界框（这些类别共享）的置信度。我们的 SSD ⽅法属于此类，因为我们没有建议步骤但使⽤默认框。然⽽，我们的⽅法⽐现有⽅法更灵活，因为我们可以使⽤不同⽅⾯的默认框来⾃不同⽐例的多个特征图的每个特征位置的⽐率。如果我们只在最顶层特征图中的每个位置使⽤⼀个默认框，我们的 SSD 将具有与 OverFeat 相似的架构；如果我们使⽤整个最顶层的特征图并添加⼀个完全连接的层来进⾏预测⽽不是我们的卷积预测器，并且不明确考虑多个纵横⽐，我们可以近似地重现 YOLO 。



# 二、做出的创新
1. 本⽂介绍了第⼀个基于深度⽹络的对象检测器，它不会为边界框假设重新采样像素或特征，并且与重新采样的⽅法⼀样准确。这显着提⾼了⾼精度检测的速度（在 VOC2007 测试中 59 FPS 和 mAP 74.3%，⽽ Faster R-CNN 7 FPS 和 mAP 73.2% 或 YOLO 45 FPS 和 mAP 63.4%）。速度的根本改进来⾃于消除边界框建议和随后的像素或特征重采样阶段。我们不是第⼀个这样做的⼈（参⻅ [Overfeat: Integrated recognition, localization and detection using convolutional networks](https://arxiv.org/pdf/1312.6229.pdf), [You only look once: Unified, real-time object detection](https://arxiv.org/pdf/1506.02640v5)），但通过添加⼀系列改进，我们设法显着提⾼了以前尝试的准确性。我们的改进包括使⽤⼩型卷积过滤器来预测对象类别和边界框位置的偏移量，使⽤单独的预测器（过滤器）进⾏不同的纵横⽐检测，并将这些过滤器应⽤于⽹络后期的多个特征图以执⾏多尺度检测。通过这些修改尤其是使⽤多层进⾏不同尺度的预测，我们可以使⽤相对低分辨率的输⼊实现⾼精度，进⼀步提⾼检测速度。虽然这些贡献单独看来可能很⼩，但我们注意到由此产⽣的系统将 PASCAL VOC 的实时检测精度从 YOLO 的 63.4% mAP 提⾼到我们 SSD 的 74.3% mAP。与最近对残差⽹络进⾏的⾮常引⼈注⽬的⼯作相⽐，这是检测精度的相对改进。此外，显着提⾼⾼质量检测的速度可以扩⼤计算机视觉有⽤的设置范围。

2. 我们将我们的贡献总结如下：
    1. 我们引⼊了 SSD，⼀种⽤于多个类别的单次检测器，它⽐以前最先进的单次检测器 (YOLO) 更快，⽽且准确度明显更⾼，实际上与执⾏显式区域的较慢技术⼀样准确proposals 和 pooling（包括 Faster R-CNN）。

    2. SSD 的核⼼是使⽤应⽤于特征图的⼩型卷积滤波器为⼀组 ***固定的默认边界框*** 预测类别分数和框偏移。

    3. 为了实现⾼检测精度，我们根据不同尺度的特征图⽣成不同尺度的预测，并按纵横⽐明确分离预测。

    4. 这些设计特征导致简单的端到端训练和⾼精度，即使是在低分辨率输⼊图像上，进⼀步改善速度与精度的权衡。

    5. 实验包括对在 PASCAL VOC、COCO 和 ILSVRC 上评估的具有不同输⼊⼤⼩的模型进⾏时序和精度分析，并将其与⼀系列最新的最先进⽅法进⾏⽐较。

# 三、设计的模型
1. Model
    - SSD ⽅法基于前馈卷积⽹络，该⽹络⽣成固定⼤⼩的边界框集合和这些框中对象类实例存在的分数，然后是⾮最⼤抑制步骤以⽣成最终检测。早期的⽹络层基于⽤于⾼质量图像分类的标准架构（在任何分类层之前被截断），我们将其称为基础⽹络(VGG16)。然后我们向⽹络添加辅助结构以⽣成具有以下关键特征的检测：
    
    1. ⽤于检测的多尺度特征图
        - 我们将卷积特征层添加到截断的基础⽹络的末尾。这些层的⼤⼩逐渐减⼩，并允许在多个尺度上预测检测。⽤于预测检测的卷积模型对于每个特征层都是不同的（参⻅ Overfeat 和 YOLO ，它们在单个尺度特征图上运⾏）。

    1. ⽤于检测的卷积预测器
        - 每个添加的特征层（或者可选地来⾃基础⽹络的现有特征层）可以使⽤⼀组卷积过滤器产⽣⼀组固定的检测预测。这些在图 2 中的 SSD ⽹络架构的顶部表⽰。对于⼤⼩为 $m \times n$ 且具有 $p$ 个通道的特征层，⽤于预测潜在检测参数的基本元素是⼀个 $3 \times 3 \times p$ 的⼩内核，它产⽣类别的得分，或相对于默认框坐标的形状偏移。在应⽤内核的 $m \times n$ 个位置中的每⼀个位置，它都会产⽣⼀个输出值。边界框偏移输出值是相对于默认值测量的相对于每个特征图位置的框位置（参⻅ YOLO 的架构，该架构使⽤中间全连接层⽽不是此步骤的卷积过滤器）。
            ![SSD2.png](../pictures/SSD2.png)

    1. 默认框和纵横⽐
        - 我们将⼀组默认边界框与每个特征图单元相关联，⽤于⽹络顶部的多个特征图。默认框以卷积⽅式平铺特征图，因此每个框相对于其对应单元格的位置是固定的。在每个特征图单元格中，我们预测相对于单元格中默认框形状的偏移量，以及每个类的分数，这些分数表⽰每个框中的类实例的存在。具体来说，对于给定位置的 $k$ 个盒⼦中的每个盒⼦，我们计算 $c$ 类分数和相对于原始默认盒⼦形状的 4 个偏移量。这导致在特征图中的每个位置周围应⽤总共 $(c + 4)k$ 个过滤器，为一个 $m \times n$ 特征图产⽣ $(c + 4)kmn$ 个输出。有关默认框的说明，请参阅图 1。我们的默认框类似于 Faster R-CNN 中使⽤的锚框，但是我们将它们应⽤于⼏个不同分辨率的特征图。在⼏个特征图中允许不同的默认框形状让我们有效地离散化可能的输出框形状的空间。
            ![SSD1.png](../pictures/SSD1.png)

1. Training
    1. 训练 SSD 和训练使⽤ region proposals 的典型检测器之间的主要区别在于，真值信息需要分配给固定检测器输出集中的特定输出。 YOLO的训练以及 Faster R-CNN 和 MultiBox 的区域建议阶段也需要⼀些版本。⼀旦确定了这个分配，就会端到端地应⽤损失函数和反向传播。训练还涉及选择⼀组默认框和检测尺度，以及硬负挖掘和数据增强策略。

    1. 匹配策略
        - 在训练期间，我们需要确定哪些默认框对应于真值检测并相应地训练⽹络。对于每个真值框，我们从随位置、纵横⽐和⽐例变化的默认框中进⾏选择。我们⾸先将每个地⾯真值框与具有最佳 jaccard 重叠的默认框匹配（如 MultiBox 中所⽰）。与 MultiBox 不同，我们然后将默认框与任何 jaccard 重叠⾼于阈值 (0.5) 的地⾯实况进⾏匹配。这简化了学习问题，允许⽹络预测多个重叠默认框的⾼分，⽽不是要求它只选择具有最⼤重叠的框。

    1. 训练⽬标
        - SSD训练目标来源于MultiBox目标，但扩展到处理多个对象类别。设 $x^{p}_ {ij} = \lbrace 1,0 \rbrace$ 是用于将第i个默认框与第j个p类地面真实框匹配的指示符。在上面的匹配策略中，我们可以有 $\sum_ {i}x^{p}_ {ij} \ge 1$ 。总体目标损失函数是定位损失（loc）和置信损失（conf）的加权和：
            $$\begin{align}
            L(x,c,l,g) = \frac{1}{N}(L_ {conf}(x,c) + \alpha L_ {loc}(x,l,g))
            \end{align}$$
            其中 N 是匹配的默认框的数量。如果 $N = 0$ ，则将损失设置为 0。定位损失是预测框 ( $l$ ) 和地⾯真值框 ( $g$ ) 参数之间的 $Smooth L_ {1}$ 损失。与 Faster R-CNN 类似，我们回归到默认边界框 ( $d$ ) 的中⼼ ( $cx, cy$ ) 及其宽度 ( $w$ ) 和⾼度 ( $h$ ) 的偏移量。
            $$\begin{align}
            &L_ {loc}(x,l,g) = &\sum^{N}_ {i \in Pos} \sum_ {m \in \lbrace cx,cy,w,h \rbrace} &x^{k}_ {ij} smooth_ {L1}(l^{m}_ {i} - \hat{g}^{m}_ {j}) \notag \\
            \hat{g}^{cx}_ {j} &= (g^{cx}_ {j} - d^{cx}_ {i}) / d^{w}_ {i}&\hat{g}^{cy}_ {j} &= (g^{cy}_ {j} - d^{cy}_ {i}) / d^{h}_ {i} \\
            \hat{g}^{w}_ {j} &= log(g^{w}_ {j} / d^{w}_ {i})&\hat{g}^{h}_ {j} &= log(g^{h}_ {j} / d^{h}_ {i}) \notag
            \end{align}$$
            置信度损失是多个类别置信度 $(c)$ 的 softmax 损失。
            $$\begin{align}
            L_ {conf}(x,c) = - \sum^{N}_ {i \in Pos}x^{p}_ {ij}log(\hat{c}^{p}_ {i}) - \sum_ {i \in Neg}log(\hat{c}^{0}_ {i}) \quad where \quad \hat{c}^{p}_ {i} = \frac{exp(c^{p}_ {i})}{\sum_ {p}exp(c^{p}_ {i})}
            \end{align}$$
            并且通过交叉验证将权重项 $\alpha$ 设置为1。

    1. 为默认框选择⽐例和纵横⽐
        - 为了处理不同的对象⽐例，Overfeat和SPPnet建议以不同的尺⼨处理图像，然后组合结果。然⽽，通过在单个⽹络中利⽤来⾃⼏个不同层的特征图进⾏预测，我们可以模拟相同的效果，同时还可以在所有对象尺度上共享参数。以前的⼯作FCN和[Hypercolumns for object segmentation and fine-grained localization](https://arxiv.org/pdf/1411.5752.pdf)经表明，使⽤来⾃较低层的特征图可以提⾼语义分割质量，因为较低层捕获了输⼊对象的更多细节。同样，[ParseNet: Looking wider to see better](https://arxiv.org/pdf/1506.04579.pdf) 表明添加从特征图中汇集的全局上下⽂可以帮助平滑分割结果。受这些⽅法的启发，我们同时使⽤下部和上部特征图进⾏检测。图 1 显⽰了框架中使⽤的两个⽰例特征图（8×8 和 4×4）。在实践中，我们可以使⽤更多的⼩计算开销。

        - 已知⽹络中不同级别的特征图具有不同的（经验）感受野⼤⼩ [Object Detectors Emerge in Deep Scene CNNs](https://arxiv.org/pdf/1412.6856.pdf)。幸运的是，在 SSD 框架内，默认框不需要对应每⼀层的实际感受野。我们设计默认框的平铺，以便特定的特征图学会响应对象的特定⽐例。假设我们要使⽤ m 个特征图进⾏预测。每个特征图的默认框的⽐例计算如下：
            $$\begin{align}
            s_ {k} = s_ {min} + \frac{s_ {max} - s_ {min}}{m - 1}(k - 1), \quad \quad k \in [1,m]
            \end{align}$$
            其中， $s_ {min}$ 为0.2， $s_ {max}$ 为0.9，这意味着最低层的尺度为0.2，最高层的尺度是0.9，并且其间的所有层都是规则间隔的。我们对默认框施加不同的纵横比，并将它们表示为 $a_ {r} \in \{ 1,2,3,\frac{1}{2},\frac{1}{3} \}$ 。我们可以计算每个默认框的宽度（ $w^{a}_ {k} = s_ {k}\sqrt{a_ {r}}$ ）和高度（ $h^{a}_ {k} = s_ {k}\sqrt{a_ {r}}$ ）。对于纵横比为 $1$ 的情况，我们还添加了一个比例为 $s^{'}_ {k} = \sqrt{s_ {k}s_ {k+1}}$ 的默认框，从而导致每个特征图位置有6个默认框。我们将每个默认框的中心设置为 $(\frac{i+0.5}{|f_ {k}|}, \frac{j+0.5}{|f_ {k}|})$ ，其中 $|f_ {k}|$ 是第 $k$ 个正方形特征图的大小， $i,j \in[0，|f_ {k}|)$ 。在实践中，还可以设计默认框的分布以最佳地适合特定数据集。如何设计最佳平铺也是一个开放的问题。

        - 通过结合来⾃许多特征图的所有位置的具有不同⽐例和纵横⽐的所有默认框的预测，我们有⼀组不同的预测，涵盖各种输⼊对象⼤⼩和形状。例如，在图 1 中，狗与 $4 \times 4$ 特征图中的默认框匹配，但不与 $8 \times 8$ 特征图中的任何默认框匹配。这是因为那些框具有不同的尺度并且与狗框不匹配，因此在训练期间被认为是负⾯的。

    1. Hard negative mining
        - 在匹配步骤之后，⼤多数默认框都是负的，尤其是当可能的默认框数量很⼤时。这在正负训练⽰例之间引⼊了显着的不平衡。我们没有使⽤所有的负样本，⽽是使⽤每个默认框的最⾼置信度损失对它们进⾏排序，并选择顶部的样本，使负样本和正样本之间的⽐率最多为 3:1。我们发现这会带来更快的优化和更稳定的训练。
            
    1. 数据增强：
        - 为了使模型对各种输入对象大小和形状更加鲁棒，通过以下选项之一对每个训练图像进行随机采样
            1. 使⽤整个原始输⼊图像
            1. 对面片进行采样，使与对象的最小jaccard重叠为0.1、0.3、0.5、0.7或0.9。
            1. 随机对patch进行采样
        
        - 每个采样块的大小是原始图像大小的 $[0.1，1]$ ，并且纵横比在 $\frac{1}{2}$ 和 $2$ 之间。如果真值框的中心在采样的面片中，我们保留其重叠部分。在上述采样步骤之后，除了应用类似于[Some Improvements on Deep Convolutional Neural Network Based Image Classification](https://arxiv.org/ftp/arxiv/papers/1312/1312.5402.pdf)中所述的一些光度量失真之外，每个采样块被调整为固定大小，并以 $0.5$ 的概率水平翻转

1. 结论
    - 本⽂介绍了 SSD，⼀种⽤于多个类别的快速单次⽬标检测器。我们模型的⼀个关键特征是使⽤附加到⽹络顶部多个特征图的多尺度卷积边界框输出。这种表⽰使我们能够有效地对可能的盒⼦形状的空间进⾏建模。我们通过实验验证，在给定适当的训练策略的情况下，⼤量精⼼选择的默认边界框会提⾼性能。我们构建的 SSD 模型⾄少⽐现有⽅法多⼀个数量级的框预测采样位置、⽐例和纵横⽐ [YOLO](http://arxiv.org/abs/1506.02640v5) [Scalable object detection using deepneural networks](https://arxiv.org/pdf/1312.2249.pdf)。我们证明，给定相同的 VGG-16 基础架构，SSD 在准确性和速度⽅⾯均优于其最先进的对象检测器。我们的 SSD512 模型在 PASCAL VOC 和 COCO 上的准确性明显优于最先进的 Faster R-CNN ，同时速度提⾼了 3 倍。我们的实时 SSD300 模型以 59 FPS 的速度运⾏，这⽐当前的实时 YOLO 替代⽅案更快，同时产⽣明显更⾼的检测精度。
    
# 四、实验结果

## 1、比之前模型的优势

## 2、有优势的原因
1. Data augmentation is crucial
1. More default box shapes is better
1. Atrous is faster
1. Multiple output layers at different resolutions is better
## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论