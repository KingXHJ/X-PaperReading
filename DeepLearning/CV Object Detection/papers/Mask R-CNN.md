# 论文信息
- 时间：2017
- 期刊：CVPR
- 网络/算法名称：Mask R-CNN
- 意义：能够预测对象掩码
- 作者：Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick; Facebook AI Research (FAIR)
- 实验环境：
- 数据集：
# 一、解决的问题
1. 我们提出了⼀个概念上简单、灵活且通⽤的对象实例分割框架。我们的⽅法可以有效地检测图像中的对象，同时为每个实例⽣成⾼质量的分割掩码。这种称为 Mask R-CNN 的⽅法扩展了 Faster R-CNN，它添加了⼀个⽤于预测对象掩码的分⽀，该分⽀与⽤于边界框识别的现有分⽀并⾏。 Mask R-CNN 易于训练，并且只增加了 Faster R-CNN 的少量开销，以 5 fps 的速度运⾏。此外，Mask R-CNN 很容易推⼴到其他任务，例如，允许我们在同⼀框架中估计⼈体姿势。我们在 COCO 挑战套件的所有三个轨道中都显⽰了最佳结果，包括实例分割、边界框对象检测和⼈物关键点检测。没有花⾥胡哨的东西，Mask R-CNN 在每项任务上都优于所有现有的单⼀模型参赛作品，包括 COCO 2016 挑战赛的获胜者。我们希望我们简单有效的⽅法将作为坚实的基线，并有助于简化实例级识别的未来研究。

2. Introduction
    - 视觉社区在短时间内迅速改进了对象检测和语义分割结果。在很⼤程度上，这些进步是由强⼤的基线系统推动的，例如分别⽤于对象检测和语义分割的Fast/Faster R CNN [12、36 ]和完全卷积⽹络 (FCN) 框架。这些⽅法在概念上是直观的，提供了灵活性和鲁棒性，以及快速的训练和推理时间。我们在这项⼯作中的⽬标是为实例分割开发⼀个类似的⽀持框架。

    - 实例分割具有挑战性，因为它需要正确检测图像中的所有对象，同时还要精确分割每个实例。因此，它结合了对象检测的经典计算机视觉任务中的元素，其⽬标是对单个对象进⾏分类并使⽤边界框和语义对每个对象进⾏定位分割，其⽬标是将每个像素分类到⼀组固定的类别中，⽽不区分物体的姿态。 ***(按照通⽤术语，我们使⽤对象检测来表⽰通过边界框⽽不是掩码进⾏检测，使⽤语义分割来表⽰不区分实例的逐像素分类。然⽽我们注意到实例分割既是语义的⼜是⼀种检测形式。)*** 鉴于此，⼈们可能期望需要⼀种复杂的⽅法才能获得良好的结果。然⽽，我们证明了⼀个⾮常简单、灵活和快速的系统可以超越先前最先进的实例分割结果。

    - 我们的⽅法称为 Mask R-CNN，通过添加⼀个分⽀来预测每个感兴趣区域 (RoI) 上的分割掩码，与现有的分类和边界框回归分⽀并⾏，从⽽扩展了 Faster R-CNN (图1). mask 分⽀是应⽤于每个 RoI 的⼩型 FCN，以像素到像素的⽅式预测分割掩码。考虑到 Faster R-CNN 框架，Mask R-CNN 易于实施和训练，这有助于⼴泛的灵活架构设计。此外，mask 分⽀仅增加了少量计算开销，从⽽实现了快速系统和快速实验。
        ![Mash R-CNN1.png](../pictures/Mask%20R-CNN1.png)

    - 原则上 Mask R-CNN 是 Faster R-CNN 的直观扩展，但正确构建 Mask 分⽀对于获得良好结果⾄关重要。最重要的是，Faster R CNN 并不是为⽹络输⼊和输出之间的像素到像素对⻬⽽设计的。这⼀点在 [RoIPool](http://arxiv.org/abs/1504.08083v2) 中最为明显，RoIPool 是处理实例的事实上的核⼼操作，它执⾏⽤于特征提取的粗略空间量化。为了修复错位，我们提出了⼀个简单的、⽆量化的层，称为 RoIAlign，它忠实地保留了精确的空间位置。尽管是看似微⼩的变化，RoIAlign 却产⽣了巨⼤的影响：它将掩模准确度相对提⾼了 10% 到 50%，在更严格的定位指标下显⽰出更⼤的收益。其次，我们发现将掩码和类别预测分离是很重要的：我们独⽴地为每个类别预测⼀个⼆进制掩码，类别之间没有竞争，并依靠⽹络的 RoI 分类分⽀来预测类别。相⽐之下，FCN 通常执⾏每像素多类分类，将分割和分类结合起来，并且根据我们的实验，实例分割效果不佳。

    - 没有花⾥胡哨的东西，Mask R-CNN 在 COCO 实例分割任务上超越了之前所有最先进的单⼀模型结果，包括来⾃ 2016 年竞赛获胜者的精⼼设计的参赛作品。作为副产品，我们的⽅法在 COCO 对象检测任务上也表现出⾊。在消融实验中，我们评估了多个基本实例，这使我们能够证明其稳健性并分析核⼼因素的影响。

    - 我们的模型可以在 GPU 上以每帧约 200 毫秒的速度运⾏，⽽在⼀台 8 GPU 机器上进⾏ COCO 训练需要⼀到两天。我们相信，快速的训练和测试速度，以及框架的灵活性和准确性，将有利于并简化未来对实例分割的研究。

    - 最后，我们通过 COCO 关键点数据集上的⼈体姿态估计任务展⽰了我们框架的通⽤性。通过将每个关键点视为⼀个单热⼆进制掩码，只需进⾏最少的修改，就可以应⽤ Mask R-CNN 来检测特定于实例的姿势。 Mask R-CNN 超越了 2016 年 COCO 关键点竞赛的获胜者，同时以 5 fps 的速度运⾏。因此，Mask R-CNN 可以更⼴泛地视为实例级识别的灵活框架，并且可以轻松扩展到更复杂的任务。


# 二、做出的创新
1. 相关工作
    1. R-CNN
        - ⽤于边界框对象检测的基于区域的 CNN (R-CNN) ⽅法是关注可管理数量的候选对象区域并评估卷积⽹络独⽴于每个RoI。R-CNN 得到扩展以允许使⽤ RoIPool 关注特征图上的 RoI，从⽽实现更快的速度和更⾼的准确性。 Faster R-CNN 通过使⽤区域提议⽹络 (RPN) 学习注意⼒机制来推进该流。 Faster R-CNN 对许多后续改进⾮常灵活且稳健，并且是当前多个基准测试中的领先框架。

    1. 实例分割：
        - 受 R-CNN 有效性的驱动，许多实例分割⽅法都基于分割提议。早期的⽅法重新分类为⾃下⽽上的⽚段。 DeepMask 和后续作品学习提出候选⽚段，然后由 Fast R-CNN 对其进⾏分类。在这些⽅法中，分割先于识别，这是缓慢且不太准确的。同样，[Instance-aware Semantic Segmentation via Multi-task Network Cascades](https://www.semanticscholar.org/reader/1e9b1f6061ef779e3ad0819c2832a29168eaeb9d)提出了⼀个复杂的多阶段级联，它从边界框提议中预测分段提议，然后进⾏分类。相反，我们的⽅法基于掩码和类标签的并⾏预测，更简单、更灵活。

        - 最近，[Fully Convolutional Instance-Aware Semantic Segmentation](https://www.semanticscholar.org/reader/0366b36006a6b37c673a42aad03ae77e8ef6ecda)将[Instance-Sensitive Fully Convolutional Networks](https://arxiv.org/pdf/1603.08678.pdf)中的分段建议系统和[R-FCN: Object Detection via Region-based Fully Convolutional Networks](https://www.semanticscholar.org/reader/b724c3f7ff395235b62537203ddeb710f0eb27bb)中的对象检测系统结合起来⽤于“完全卷积实例分割”（FCIS）。 上述3篇文献中的共同思想是完全卷积地预测⼀组位置敏感的输出通道。这些通道同时处理对象类、框和掩码，使系统速度更快。但是 FCIS 在重叠实例上表现出系统性错误并产⽣虚假边缘（图6），表明它受到了分割实例的基本困难的挑战。
            ![Mask R-CNN6.png](../pictures/Mask%20R-CNN6.png)

        - 实例分割的另⼀系列解决⽅案是由语义分割的成功驱动的。从每个像素的分类结果（例如，FCN 输出）开始，这些⽅法试图将同⼀类别的像素分成不同的实例。与这些⽅法的分割优先策略相反，Mask R-CNN 基于实例优先策略。我们预计未来将对这两种策略进⾏更深⼊的研究。


# 三、设计的模型
- Mask R-CNN 在概念上很简单：Faster R-CNN 对每个候选对象都有两个输出，⼀个类标签和⼀个边界框偏移量；为此，我们添加了第三个分⽀，⽤于输出对象掩码。因此，Mask R-CNN 是⼀个⾃然⽽直观的想法。但是额外的掩码输出与类和框输出不同，需要提取更精细的对象空间布局。接下来，我们介绍 Mask R-CNN 的关键要素，包括像素到像素对⻬，这是 Fast/Faster R-CNN 主要缺失的部分。

1. Faster R-CNN：我们⾸先简要回顾⼀下 Faster R-CNN 检测器。
    - Faster R-CNN 由两个阶段组成。第⼀阶段称为区域提议⽹络 (RPN)，提出候选对象边界框。第⼆阶段，本质上是 Fast R-CNN ，使⽤ RoIPool 从每个候选框中提取特征，并执⾏分类和边界框回归。可以共享两个阶段使⽤的特征以加快推理速度。我们建议读者参考[Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors](https://www.semanticscholar.org/reader/a312a573ef81793d56401e932ef6c9498791a3d1)，了解 Faster R-CNN 与其他框架之间最新的、全⾯的⽐较。

2. Mask R-CNN： 
    - Mask R-CNN 采⽤相同的两阶段程序，具有相同的第⼀阶段（即 RPN）。在第⼆阶段，在预测类和框偏移的同时，Mask R-CNN 还为每个 RoI 输出⼀个⼆进制掩码。这与⼤多数最近的系统形成对⽐，其中分类取决于掩码预测。我们的⽅法遵循 Fast R-CNN 的精神，即并⾏应⽤边界框分类和回归（事实证明这在很⼤程度上简化了原始 R-CNN 的多阶段管道）。
    
    - 形式上，在训练期间，我们将每个采样 RoI 的多任务损失定义为 $L = L_ {cls} + L_ {box} + L_ {mask}$ 。分类损失 $L_ {cls}$ 和边界框损失 $L_ {box}$ 与 Fast R-CNN 中定义的相同。 mask 分⽀对每个 RoI 都有⼀个 $Km^{2}$ 维输出，它编码分辨率为 $m \times m$ 的 $K$ 个⼆进制掩码，$K$ 个类中的每⼀个。为此，我们应⽤每像素 sigmoid，并将 $L_ {mask}$ 定义为平均⼆元交叉熵损失。对于与真实类别 $k$ 关联的 RoI， $L_ {mask}$ 仅在第 $k$ 个掩码上定义（其他掩码输出不影响损失）。

    - 我们对 $L_ {mask}$ 的定义允许⽹络为每个类别⽣成掩码，⽽⽆需类别之间的竞争；我们依靠专⻔的分类分⽀来预测⽤于选择输出掩码的类标签。这解耦了掩码和类别预测。这与将 FCN 应⽤于语义分割时的常⻅做法不同，后者通常使⽤每像素 softmax 和多项式交叉熵损失。在那种情况下，跨类⾯具竞争；在我们的例⼦中，对于每像素 sigmoid 和⼆元损失，它们没有。我们通过实验表明，这个公式是获得良好实例分割结果的关键。

3. 掩码表⽰：
    - 掩码对输⼊对象的空间布局进⾏编码。因此，与不可避免地通过全连接 (fc) 层折叠成短输出向量的类标签或框偏移不同，提取掩码的空间结构可以通过卷积提供的像素到像素对应⾃然地解决。
    
    - 具体来说，我们使⽤ FCN 从每个 RoI 预测⼀个 $m \times m$ 掩码。这允许 mask 分⽀中的每⼀层都保持明确的 $m \times m$ 对象空间布局，⽽不会将其折叠成缺乏空间维度的⽮量表⽰。与以前重新排序到 fc 层以进⾏掩码预测的⽅法不同，我们的全卷积表⽰需要更少的参数，并且如实验所证明的那样更准确。

    - 这种像素到像素的⾏为要求我们的 RoI 特征（它们本⾝是⼩特征图）很好地对⻬，以忠实地保留显式的每像素空间对应关系。这促使我们开发以下 RoIAlign 层，该层在掩模预测中起着关键作⽤。

4. RoIAlign：
    - RoIPool [12]是⼀种标准操作，⽤于从每个 RoI 中提取⼀个⼩特征图（例如，7×7）。 RoIPool ⾸先将⼀个浮点数的 RoI 量化为特征图的离散粒度，然后将这个量化的 RoI 细分为本⾝量化的空间 bin，最后聚合每个 bin 覆盖的特征值（通常通过最⼤池化）。例如，通过计算 $[x / 16]$ 在连续坐标 $x$ 上执⾏量化，其中 16 是特征图步⻓， $[ \cdot ]$ 是舍⼊；同样，在划分为 bin（例如 7×7）时执⾏量化。这些量化在 RoI 和提取的特征之间引⼊了错位。虽然这可能不会影响对⼩翻译具有鲁棒性的分类，但它对预测像素精确掩码有很⼤的负⾯影响。

    - 为了解决这个问题，我们提出了⼀个 RoIAlign 层，它消除了 RoIPool 的严苛量化，将提取的特征与输⼊正确对⻬。我们提议的更改很简单：我们避免对 RoI 边界进⾏任何量化或 bins（即，我们使⽤ $x/16$ ⽽不是 $[x/16]$ ）。我们使⽤双线性插值[Spatial Transformer Networks](https://www.semanticscholar.org/reader/fe87ea16d5eb1c7509da9a0314bbf4c7b0676506)来计算每个 RoI bin 中四个定期采样位置的输⼊特征的精确值，并聚合结果（使⽤最⼤值或平均值），详⻅图3 。我们注意到，只要不执⾏量化，结果对确切的采样位置或采样的点数不敏感。
        ![Mask R-CNN3.png](../pictures/Mask%20R-CNN3.png)

    - 正如我们在 §4.2 中展⽰的那样，RoIAlign 带来了巨⼤的改进。我们还与 [Instance-Aware Semantic Segmentation via Multi-task Network Cascades](https://www.semanticscholar.org/reader/1e9b1f6061ef779e3ad0819c2832a29168eaeb9d) 中提出的 RoIWarp 操作进⾏了⽐较。与 RoIAlign 不同，RoIWarp 忽略了对⻬问题，并在[Instance-Aware Semantic Segmentation via Multi-task Network Cascades](https://www.semanticscholar.org/reader/1e9b1f6061ef779e3ad0819c2832a29168eaeb9d)中实现为量化 RoI，就像 RoIPool ⼀样。因此，尽管 RoIWarp 也采⽤了由[Spatial Transformer Networks](https://www.semanticscholar.org/reader/fe87ea16d5eb1c7509da9a0314bbf4c7b0676506)驱动的双线性重采样，但它的性能与 RoIPool 相当，如实验所⽰（表2c 中有更多详细信息），证明了对⻬的关键作⽤。

5. ⽹络架构：
    - 为了展⽰我们⽅法的通⽤性，我们实例化了具有多种架构的 Mask R-CNN。为清楚起⻅，我们区分了：(i) ⽤于对整个图像进⾏特征提取的卷积主⼲架构，以及 (ii) ⽤于边界框识别（分类和回归）和分别应⽤于每个图像的掩模预测的⽹络头投资回报率。

    - 我们使⽤名称⽹络深度特征来表⽰⻣⼲架构。我们评估深度为 50 或 101 层的 ResNet 和 [ResNeXt](https://www.semanticscholar.org/reader/f6e0856b4a9199fa968ac00da612a9407b5cb85c) ⽹络。 Faster R-CNN 与 ResNets 的原始实现从第 4 阶段的最终卷积层中提取特征，我们称之为 C4。例如，这个带有 ResNet-50 的⻣⼲由 ResNet-50-C4 表⽰。

    - 我们还探索了 Lin 等⼈最近提出的另⼀个更有效的主⼲。 [Feature Pyramid Networks for Object Detection](https://www.semanticscholar.org/reader/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36)，称为 Feature Pyra mid Network (FPN)。 FPN 使⽤具有横向连接的⾃顶向下架构，从单尺度输⼊构建⽹络内特征⾦字塔。 Faster R-CNN with an FPN back bone 根据其规模从特征⾦字塔的不同层级提取 RoI 特征，但除此之外，该⽅法的其余部分类似于 vanilla ResNet。使⽤ ResNet-FPN 主⼲与 Mask R CNN 进⾏特征提取可在准确性和速度⽅⾯获得出⾊的收益。有关 FPN 的更多详细信息，我们建议读者参阅[Feature Pyramid Networks for Object Detection](https://www.semanticscholar.org/reader/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36)。

    - 对于⽹络头，我们密切遵循之前⼯作中提出的架构，我们在其中添加了完全卷积掩码预测分⽀。具体来说，我们从 ResNet 和 FPN 论⽂中扩展了 Faster R-CNN box heads 。详细信息如图 4 所⽰。ResNet -C4 ⻣⼲⽹的头部包括 ResNet 的第 5 级（即 9 层“res5” ），这是计算密集型的。对于 FPN，主⼲已经包含 res5，因此允许使⽤更少过滤器的更⾼效的头部。
        ![Mask R-CNN4.png](../pictures/Mask%20R-CNN4.png)

    - 我们注意到我们的掩码分⽀具有简单的结构。更复杂的设计有可能提⾼性能，但不是这项⼯作的重点。

6. 实施细节
    - 我们根据现有的 Fast/Faster R-CNN ⼯作设置超参数。尽管这些决定是在原始论⽂中针对对象检测做出的，但我们发现我们的实例分割系统对它们来说是稳健的。

    1. 训练：
        - 与 Fast R-CNN ⼀样，如果 RoI 具有⾄少为 0.5 的真实框的 IoU，则被认为是正的，否则被认为是负的。掩码损失Lmask仅在正 RoI 上定义。 mask ⽬标是 RoI 与其相关联的 ground-truth mask 之间的交集。

        - 我们采⽤以图像为中⼼的训练。调整图像⼤⼩，使其⽐例（较短的边缘）为 800 像素。每个 mini-batch 每个 GPU 有 2 个图像，每个图像有 N 个采样 RoI，正负⽐例为 1:3 。 C4 主⼲的 N 为 64，FPN 的 N 为 512。我们在 8 个 GPU 上训练（因此有效的⼩批量⼤⼩为 16）进⾏ 160k 次迭代，学习率为 0.02，在 120k 次迭代时减少 10。我们使⽤ 0.0001 的权重衰减和 0.9 的动量。使⽤ [ResNeXt](https://www.semanticscholar.org/reader/f6e0856b4a9199fa968ac00da612a9407b5cb85c)，我们使⽤每个 GPU 1 个图像和相同的迭代次数进⾏训练，起始学习率为 0.01。

        - RPN 锚点跨越 5 个尺度和 3 个纵横⽐。为了⽅便消融，RPN 是单独训练的，除⾮另有说明，否则不与 Mask R-CNN 共享特征。对于本⽂中的每个条⽬，RPN 和 Mask R-CNN 具有相同的主⼲，因此它们是可共享的。

    2. 推论：
        - 在测试时，C4 主⼲的提议数为 300，FPN 的提议数为 1000。我们在这些建议上运⾏框预测分⽀，然后进⾏⾮最⼤抑制。然后将掩码分⽀应⽤于得分最⾼的 100 个检测框。虽然这与训练中使⽤的并⾏计算不同，但它加快了推理速度并提⾼了准确性（由于使⽤了更少、更准确的 RoI）。Mask分⽀可以为每个 RoI 预测 $K$ 个掩码，但我们只使⽤第 $k$ 个掩码，其中 $k$ 是分类分⽀预测的类别。然后将 $m \times m$ 浮点数掩码输出调整为 RoI ⼤⼩，并以 0.5 的阈值进⾏⼆值化。

        - 请注意，由于我们只计算前 100 个检测框上的掩码，因此 Mask R-CNN 为其对应的 Faster R-CNN 添加了少量开销（例如，在典型模型上约为 20%）。

# 四、实验结果

## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论