# 论文信息
- 时间：2020
- 期刊：CVPR
- 网络名称：CVP-MVSNet
- 意义：从粗糙到精细的代价金字塔，精度高，速度慢
- 作者：Jiayu Yang1, Wei Mao1, Jose M. Alvarez2, Miaomiao Liu1,3; 1Australian National University, 2NVIDIA, 3Australian Centre for Robotic Vision
- 实验环境：NVIDIA TITAN RTX
- 数据集：DTU；Tanks and Temples
# 一、解决的问题
1. 尽管深度学习时代之前的传统方法在使用 ***朗伯曲面*** 重建场景方面取得了巨大的成就，但它们仍然会受到照明变化、低纹理区域和反射的影响，从而导致不可靠的匹配对应关系，无法进一步重建
2. MVSNet的代价体矩阵为了支持更大分辨率的图像，通过时间换空间
3. 为了节约计算资源，Point-MVSNet通过3D点云，对每个点云在K近邻的邻居上，利用边缘卷积，迭代地沿着视线预测深度残差，但是计算量仍然受到迭代级别的影响
4. 本文的方法：
    - 首先为每个输入图像构建一个图像金字塔
    - 然后，对于参考图像的最粗分辨率，通过在场景的整个深度范围内采样深度来构建紧凑的成本体
    - 之后，在下一个金字塔级别，从当前深度估计的邻居执行剩余深度搜索，以使用多尺度3D CNN进行正则化来构建部分成本体
    - 当我们在每个级别以短的搜索范围迭代地构建这些成本体时，它会产生一个小而紧凑的网络。因此，本文的网络在基准数据集上的性能比当前最先进的网络快6倍。
# 二、做出的创新
1. 提出的模型：
    - 优势：一种基于成本体的神经网络，用于多视图图像的深度推断。可以证明，以粗略到精细的方式构建成本体积金字塔，而不是以固定分辨率构建成本体积，可以产生 ***紧凑、轻量级的网络，并允许我们推断高分辨率深度图以获得更好的重建结果***
    - 未来展望：希望探索将我们的方法集成到 ***基于学习(learning-based)的动态结构框架(structure-from-motion)*** 结构中，以进一步降低不同应用程序的内存需求。
    - 未来展望：希望探索将我们的方法集成到基于学习()的动态结构框架(structure-from-motion)结构中，以进一步降低不同应用程序的内存需求。
2. 模型思路：
    - 首先基于以图像的最粗分辨率在整个深度范围内对前平行平面进行均匀采样来构建成本体积
    - 然后，给定当前的深度估计，在像素深度残差上迭代构造新的成本体，以执行深度图细化。虽然与Point-MVSNet分享了类似的见解，即迭代地预测和细化深度，但本文表明，与Point-MVSNet在3D点上的工作相比，在成本体金字塔上的工作可以产生更紧凑、更高效的网络结构
    - 进一步详细分析了（剩余）深度采样和图像分辨率之间的关系，这是构建紧凑成本体积金字塔的一个原则。在基准数据集上的实验结果表明，本文的模型可以更快地执行6倍，并且与最先进的方法具有相似的性能
3. 与Point-MVSNet的不同:
    - 与Point-MVSNet有着相似的见解，即以粗略到精细的方式预测和细化深度图，但我们的工作与他们的工作在以下四个主要方面有所不同
    - 首先，Point-MVSNet中的方法在3D点云上执行卷积。相反，我们 ***在图像坐标上定义的规则网格*** 上构建成本体积，已经证明这在运行时会更快
    - 其次，我们提供了 ***基于深度采样和图像分辨率之间的相关性*** 构建紧凑成本体积金字塔的规则
    - 作为第三个主要区别，我们使用 ***多尺度3D-CNN正则化*** 来覆盖大的感受野，并鼓励残余深度估计的局部平滑，这导致了更高的精度。最后，与Point-MVSNet和其他相关工作相比，我们的方法可以用小分辨率图像输出小分辨率的深度
4. 主要贡献：
    - 我们提出了一种基于成本体的、紧凑的、计算效率高的深度推理网络
    - 在详细分析深度残差搜索范围和图像分辨率之间关系的基础上，我们以粗到细的方式构建了成本体金字塔
    - 我们的框架可以以更少的内存需求处理高分辨率图像，比当前最先进的框架（即Point MVSNet）快6倍，并且在基准数据集上实现了更好的准确性

# 三、设计的模型

![CVP-MVSNet struct](../pictures/CVP-MVSNet%20strut.png)
- 参考帧 $\mathbf{I} _{0} \in \mathbb{R}^{H \times W}$
- N个邻域帧 $\lbrace \mathbf{I} _{i} \rbrace^{N} _{i=0}$
- 所有视角的相机的内参、旋转矩阵和平移向量 $\lbrace \mathbf{K} _{i} \mathbf{R} _{i} \mathbf{t} _{i} \rbrace^{N} _{i=0}$
- 目的：为 $\mathbf{I} _{0}$ 从 $\lbrace \mathbf{I} _{i} \rbrace^{N} _{i=0}$ 推断深度图 $\mathbf{D}$
- 创新：是在成本体金字塔上使用一个 ***前馈深度网络*** ，该网络以粗到细的方式构建

1. 特征金字塔
    - 采取 ***可学习的特征*** ，解决原图像对应像素随光照的变化。这已经被证明是提取稠密相关特征的关键一步
    - 现有工作中的一般是利用高分辨率图像来提取多尺度图像特征，即使对于低分辨率深度图的输出也是如此。相比之下，我们表明低分辨率图像包含足够的信息，可用于估计低分辨率深度图
    - 特征提取分为两步：
        1. 首先，为每个输入图像 $i \in \lbrace 0,1,...,N \rbrace$ 构建(L+1)层图像金字塔 $\lbrace \mathbf{I}^{j} _{i} \rbrace^{L} _{j=0}$ ，最低一层级别对应原图 $\mathbf{I}^{0} _{i} = \mathbf{I} _{i}$
        2. 其次，使用CNN（即特征提取网络）获得 $l$ 级的特征表示。具体来说，它由9个卷积层组成，每个卷积层后面都有一个泄漏整流线性单元(Leaky-ReLU)。用相同的CNN对所有图片做特征提取。提取到的 $l$ 级特征图 $\lbrace \mathbf{f}^{l} _{i} \rbrace^{N} _{i=0} , \mathbf{f}^{l} _{i} \in \mathbb{R}^{H / 2^l \times W / 2^l \times F}$ ，其中F是通道数，实验中选择16
2. 代价体金字塔
    - 目的：给定提取的特征，下一步是在 ***参考视图*** 中构造用于深度推断的成本体
    - 问题：常见的方法通常以固定分辨率构建单个成本体，这会导致大量内存需求，从而限制了高分辨率图像的使用
    - 方案：构建成本金字塔，这是一个迭代估计和细化深度图以实现高分辨率深度推断的过程
        1. 首先基于图像金字塔中最粗分辨率的图像和场景中前平行平面的均匀采样来构建用于粗略深度图估计的成本体积
        2. 然后，基于粗估计和深度残差假设迭代构造部分成本体积，以获得更高分辨率和精度的深度图

    1. 粗略深度图推断的成本体
        - 开始构建成第 $L$ 级成本体对应于最低的图像分辨率 $(H / 2^L , W / 2^L)$
        - 通过在整个深度范围内均匀采样M个正面平行平面来构建参考视图的成本体
            - 最低深度 $d_{min}$
            - 最大深度 $d_{max}$
            - 采样深度 $d = d_{min} + m(d_{max} - d_{min}) / M , m \in \lbrace 0,1,2,...,M-1 \rbrace$ 代表一个平面，其法线 $n_0$ 是参考相机的主轴
        - 单应性变换 $\mathbf{H} _{i}(d)$ 在深度d处的第i个源视图和参考视图之间的关系(好像直接抄的MVSNet的公式，但是那个应该是错的吧) $$\mathbf{H} _{i}(d) = \mathbf{K}^{L} _{i}\mathbf{R} _{i}(\mathbf{I} - \frac{(\mathbf{t} _{0} - \mathbf{t} _{i})\mathbf{n}^{T} _{0}}{d})\mathbf{R}^{-1} _{0}(\mathbf{K}^{L} _{0})^{-1}$$ 其中 $\mathbf{I}$ 是单位矩阵， $\mathbf{K}^{L} _{i}$ 和 $\mathbf{K}^{L} _{0}$ 是 $\mathbf{K} _{i}$ 和 $\mathbf{K} _{i}$ 在第L级放缩后的内参矩阵
            - 每个单应性变化指明了一组从参考帧 $x$ 到邻域帧 $\tilde{x}_ {i}$ 的映射 $\lambda_{i} \tilde{x}_ {i} = \mathbf{H}_ {i}(d) x$ 其中 $\lambda_{i}$ 代表了像素 $\tilde{x}$ 在邻域帧中的深度
        - 有了 $\tilde{x}$ 和 $\lbrace \mathbf{f}^{L} _{i} \rbrace^{N} _{i=1}$ 通过双线性插值，重塑从邻域帧到参考帧的扭曲变换 $\lbrace \tilde{\mathbf{f}}^{L} _{i,d} \rbrace^{N} _{i=1}$ 。深度d处的所有像素的成本被定义为其来自N+1个视图的特征的方差 $\mathbf{C}^{L} _{d} = \frac{1}{(N+1)} \sum^{N} _{i=0}(\tilde{\mathbf{f}}^{L} _{i,d} - \bar{\mathbf{f}}^{L} _{d})^{2}$ 其中 $\tilde{\mathbf{f}}^{L} _{d} = \mathbf{f}^{L} _{0}$ 是参考矩阵的特征图， $\bar{\mathbf{f}}^{L} _{d}$ 是所有视图( $\lbrace \tilde{\mathbf{f}}^{L} _{i,d} \rbrace^{N} _{i=1} \cup \mathbf{f}^{L} _{0}$ )的特征体的平均值。该度量鼓励每个像素的正确深度具有最小的特征方差，这与光度一致性约束相对应。计算每个深度假设的成本图，并将这些成本图连接到单个成本体 $\mathbf{C}^{L} \in \mathbb{R}^{W / 2^l \times H / 2^l \times M \times F}$
        - 获得良好深度估计精度的关键参数是深度采样分辨率M
    
    2. 多尺度深度残差推断的成本体
        - 最终的目的是获得参考帧 $T_{0}$ 的深度图 $\mathbf{D} =  \mathbf{D}^{0}$
        - 流程：从第（l+1）级的给定深度估计 $\mathbf{D}^{l+1}$ 开始迭代，以获得下一级 $\mathbf{D}^{l}$ 的精细深度图，直到到达底层
            - 首先上采样 $\mathbf{D}^{l+1}$ 到下一个层级 $\mathbf{D}^{l+1}_ {\uparrow}$ 通过 ***双三次插值*** 
            - 然后构建部分成本体，以回归定义为 $\Delta D^{l}$ 以获得精细深度图 $D^{l} =  D^{l+1}_ {\uparrow} + \Delta D^{l}$ 第l级的 $D^{l}$
        - 改进：虽然与Point-MVSNet分享了类似的见解，以迭代预测深度残差，但我们认为，不是在点云上执行卷积，而是通过多尺度3D卷积，在深度残差上构建常规3D成本体，可以实现更紧凑、更快和更高精度的深度推断
        - 原因：是相邻像素的深度位移是相关的，这表明常规的的多尺度3D卷积将为深度残差估计提供有用的上下文信息。因此，将深度位移假设安排在规则的3D空间中，并 ***如下计算成本体积***
            - 给定相机参数 $\lbrace \mathbf{K}_ {i} \mathbf{R}_ {i} \mathbf{t}_ {i} \rbrace^{N}_ {i=0}$ 和上采样深度估计 $\mathbf{D}^{l+1}_ {\uparrow}$ 。当前每个像素 $\mathbf{p}=(u,v)$ 被定义为 $d_{\mathbf{p}} = \mathbf{D}^{l+1}_ {uparrow}(u,v)$ ，设每个深度残差假设区间为 $\Delta d_{\mathbf{p}} = s_{\mathbf{p}} / M$ ，其中 $s_{\mathbf{p}}$ 表示深度搜索范围在p的时候，M代表采样的深度残差序号

            ![CVP-MVSNet reprojection](../pictures/CVP-MVSNet%20reprojection.png)
            - 考虑对应的假设3D点与深度的投影 $(D^{l+1}_ {\uparrow}(u,v) + m \Delta d_{\mathbf{p}})$ 在邻域帧中是 $$\lambda_i x^{\prime}_ {i} = \mathbf{K}^{l}_ {i} (\mathbf{R}_ {i} \mathbf{R}^{-1}_ {0} ((\mathbf{K}^{i}_ {0})^{-1} (u,v,1)^{T} (d_{\mathbf{p}} + m \Delta d_{\mathbf{p}}) - \mathbf{t}_ {0}) + \mathbf{t}_ {i})$$ 其中 $\lambda_i$ 代表在邻域帧i中的相关深度， $m \in \lbrace -M / 2,...,M/2 - 1 \rbrace$ 
            - 然后，该像素在每个深度残差假设下的代价类似地基于公式 $\mathbf{C}^{L}_ {d}$ ，最终获得部分代价体  $\mathbf{C}^{L} \in \mathbb{R}^{W / 2^l \times H / 2^l \times M \times F}$
            - 在下一节中，我们将介绍我们的解决方案，以确定所有像素的深度搜索间隔和范围 $s_{\mathbf{p}}$ ，这对于获得准确的深度估计至关重要
    
    3. 深度图推断
        - 在本节中，我们首先提供以最粗的图像分辨率执行深度采样，以及以更高的图像分辨率，对局部深度搜索范围进行离散化，来构建成本体的细节。然后，在成本体上引入深度图估计器来实现深度图推断
        1. 成本体金字塔的深度采样
            - 观察到虚拟深度平面的深度采样与图像分辨率有关
            
            ![CVP-MVSNet interpolation](../pictures/CVP-MVSNet%20interpolation.png)
            - 不需要密集地对深度平面进行采样，因为图像中那些采样的3D点的投影太近。在实验中，为了确定虚拟平面的数量，计算图像中对应0.5像素距离的平均深度采样间隔
            - 为了确定每个像素当前深度估计周围的深度残差的局部搜索范围
                - 首先将其3D点投影到邻域视图中，沿着两个方向的 ***对极线*** 找到距离投影两个像素的点（见图CVP-MVSNet reprojection “ 2像素长度 ”），然后将这两个点反向投影到3D射线中。这两条光线与参考帧中的视觉光线的相交决定了当前级别上深度细化的搜索范围
        2. 深度图估计器
            - 同MVSNet一样，采用3D卷积建立代价体金字塔 $\lbrace \mathbf{C}^{l} \rbrace^{L}_ {l=0}$ ，聚合上下文信息并输出置信度体 $\lbrace \mathbf{P}^{l} \rbrace^{L}_ {l=0}$ ，其中  $\mathbf{P}^{l} \in  \mathbb{R}^{H / 2^l \times W / 2^l \times M}$ 
            - ***详细的3D卷积网络设计见" Supp. Mat"***
            - 注意： $\mathbf{}^L$ 和 $\lbrace \mathbf{P}^{l} \rbrace^{L - 1}_ {l=0}$ 分别在绝对深度和残差深度上生成
                - 首先将soft-argmax应用于 $\mathbf{P}^L$ 以获得粗略的深度图
                - 然后，通过将软argmax应用于 $\lbrace \mathbf{P}^{l} \rbrace^{L - 1}_ {l=1}$ 来迭代细化所获得的深度图，以获得更高分辨率的深度残差。
            - 由于采样深度 $d = d_{min} + m(d_{max} - d_{min}) / M , m \in \lbrace 0,1,2,...,M-1 \rbrace$ 在第L层。因此，为每个像素的深度估计计算公式为： $$D^{L}(\mathbf{p}) = \sum^{M-1}_ {m=0}d \mathbf{P}^{L}_ {\mathbf{p}}(d)$$
            - 为了进一步细化当前估计，即粗深度图或第（l+1）级的细化深度，我们估计残差深度。假设 $r_{\mathbf{p}} = m \cdot \Delta d^{l}_ {\mathbf{p}}$ 表示深度残差假设。计算下一层的更新深度为： $$\mathbf{D}^{l}(\mathbf{p}) = \mathbf{D}^{l + 1}_ {\uparrow}(\mathbf{p}) + \sum^{(M - 2) / 2}_ {m = -M/2} r_{\mathbf{p}} \mathbf{P}^{l}_ {\mathbf{p}}(r_{\mathbf{p}})$$ 其中， $l \in \lbrace L-1,L-2,...,0 \rbrace$ 在本文的实验中，在进一步需要金字塔深度估计以获得良好结果之后，并没有观察到深度图细化
    4. 损失函数
        - 采用监督学习策略，并构建地面真实深度 $\lbrace \mathbf{D}^{l}_ {GT} \rbrace^{L}_ {l=0}$ 的金字塔作为监督信号。与现有的MVSNet框架类似，我们使用 $l_1$ 范数测量真值和估计深度之间的绝对差异。对于每个培训样本，损失是： $$Loss = \sum^{L}_ {l=0} \sum_ {\mathbf{p} \in \omega}||\mathbf{D}^{l}_ {GT}(\mathbf{p}) - \mathbf{D}^{l}(\mathbf{p})||_ {1}$$ 其中， $\omega$ 是具有真值测量值的有效像素集
        
# 四、实验结果
1. 我们的方法在准确性、完整性和总分方面优于当前所有基于学习的方法。与基于几何的方法相比，只有Galliani等人提出的方法[12]在平均精度方面提供了稍好的结果
2. DTU数据集的结果
    - 现在，我们将我们的结果与相关的基于学习的方法在不同输入分辨率的GPU内存使用和运行时间方面进行了比较
    - 我们的网络能够以更低的运行时间生成更好的点云。此外，与相同大小的深度图输出的Point-MVSNet相比，我们的方法速度快六倍，并且消耗的内存少六倍，精度相似。与Point-MVSNet相比，我们可以输出精度更高、内存使用更少、运行时间更短的高分辨率深度图
    - 我们的方法能够重建比Point-MVSNet更多的细节。与R-MVSNet和Point-MVSNet相比，正如我们在法线贴图中看到的那样，我们的结果在曲面上更平滑，同时在边缘区域捕获更多高频细节。
3. Tanks and Temples的结果
    - 在DTU上训练，不经过任何下游微调，在新数据集上进行点云重建
    - 为了公平比较，我们使用了MVSNet的相同相机参数、深度范围和视图。为了进行比较，我们考虑了四个基线，并评估了f-score。我们的方法得出的平均f-score比Point-MVSNet高5%，这是DTU数据集的最佳基线，仅比P-MVSNet低1%。请注意，P-MVSNet对点云融合应用了比我们更多的深度过滤过程，这只是遵循MVSNet提供的简单融合过程
4. 消融实验
    1. 训练金字塔级别
        - 首先分析了金字塔层数对重建质量的影响。为此，我们对图像进行下采样，以形成具有四个不同级别的金字塔。建议的2级金字塔是最好的。随着金字塔级别的增加，最粗糙级别的图像分辨率降低。对于超过2个级别的图像，该分辨率太小，无法生成要细化的良好初始深度图
    2. 评估像素间隔设置
        - 我们现在分析改变像素间隔设置对深度细化的影响。深度采样由源视图中相应的像素偏移确定，因此，设置合适的像素间隔很重要。当间隔太小（0.25像素）或太大（2像素）时，性能会下降。
## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、代码

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论
