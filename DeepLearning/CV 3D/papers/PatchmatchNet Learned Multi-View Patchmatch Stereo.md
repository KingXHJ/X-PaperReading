# 论文信息
- 时间：2020
- 期刊：CVPR
- 网络名称：PatchMatchNet
- 意义：借鉴传统PatchMatch算法，采用金字塔思想，精度高，速度快	
- 作者：Fangjinhua Wang1 Silvano Galliani2 Christoph Vogel2 Pablo Speciale2 Marc Pollefeys1,2; 1Department of Computer Science, ETH Zurich; 2Microsoft Mixed Reality & AI Zurich Lab
- 实验环境：2 Nvidia GTX 1080Ti GPUs
- 数据集：DTU、Tanks&Temples和ETH3D

# 专有名词
1. Lambertian表面：
    - 是指在一个固定的照明分布下从所有的视场方向上观测都具有相同亮度的表面，Lambertian表面不吸收任何入射光．Lambertian反射也叫散光反射，不管照明分布如何，Lambertian表面在所有的表面方向上接收并发散所有的入射照明，结果是每一个方向上都能看到相同数量的能量
2.  传统MVS分为四个大类：
    - 基于体素的
    - 基于曲面演化的
    - 基于斑块(patch)的
    - 基于深度图的：更加简洁和灵活
# 一、解决的问题
1. 传统算法存在的问题：
    - 遮挡
    - 光线变化
    - 无纹理/弱纹理区域
    - 非朗伯曲面
2. MVS基准在3D视觉领域表现非常好。很多基于学习的方法改善了传统算法面临的难题，表现得比一些传统的方法要好，但是他们很少关注内存和运行时间
3. 由于3D CNN通常耗费时间和内存，一些方法在特征提取期间对输入进行采样，并以低分辨率计算成本体积和深度图
4. 由于内存限制，很多方法无法很好地扩展到几百万像素的真实图像大小，因此无法利用全分辨率。显然，低内存和时间消耗，是在内存和计算受限设备（如手机或混合现实耳机）以及延迟要求很高应用中实现处理的关键
5. 一些传统的MVS方法完全放弃了保持结构化成本体的想法，而是基于开创性的Patchmatch算法。Patchmatch采用随机迭代算法进行近似最近邻域计算。特别是，利用深度图固有的空间一致性，可以快速找到一个好的解决方案，而无需查看所有可能性。对基于MVS的深度学习方法设置中，patch的低内存要求与视差范围无关，以及隐式平滑效果，使得这个方法变得非常有吸引力


# 二、做出的创新
![PatchMatchNet performance](../pictures/PatchMatchNet/PatchMatchNet%20performance.png)

1. 提出了PatchmatchNet，一种新颖的可学习的Patchmatch ***级联公式*** ，用于高分辨率多视图立体，并基于深度特征的学习自适应传播和评估模块进行了扩展
2. 三大创新:
    - PatchmatchNet继承自其命名祖先，自然具有低内存需求，与视差范围无关。PatchmatchNet具有高计算速度和低内存需求，可以处理更高分辨率的图像，并且比采用3D成本体正则化的竞争对手更适合在资源有限的设备上运行
    - 首次在端到端可训练体系结构中引入迭代多尺度Patchmatch，并在每次迭代中使用新颖的学习自适应传播和评估方案来改进Patchmatchcore算法
    - 广泛的实验表明，我们的方法在DTU、Tanks&Temples和ETH3D上的性能和通用性非常有竞争力，但效率明显高于所有现有顶级性能模型：比最先进的方法快至少2.5倍，内存使用量少两倍，并且具有良好的泛化性能和竞争性
3. 意义：PatchmatchNet使基于学习的MVS更高效，更适用于内存受限的设备或时间关键型应用程序
4. 做出的贡献：
    - 实现了端到端训练的PatchMatch方法，提出了由粗到细的框架，来加速计算
    - 使用可学习的自适应模块来扩展Patchmatch的传统传播和成本评估步骤，这些模块提高了准确性，并将两个步骤都建立在深度特征的基础上。在源视图的成本聚合期间估计可见性信息。此外，提出了一种鲁棒训练策略，将随机性引入训练中，以提高可见性估计和泛化的鲁棒性
    - 在多个数据集上的结果表明，与大多数基于学习的方法相比，PatchMatchNet实现了具有竞争力的性能，同时减少了内存消耗和运行时间

# 三、设计的模型

![PatchMatchNet struct](../pictures/PatchMatchNet/PatchMatchNet%20struct.png)
- 由多尺度特征提取、基于学习的PatchMatch（包含迭代地从粗糙到精细的框架）和一个空间优化模块，三个部分组成
1. 多尺度特征提取
    1. 表示：
        - 输入N张图片，维度是 $W \times H$
        - $I_0$ 表示参考图像， $\lbrace I_i \rbrace^{N-1}_ {i=1}$ 表示邻域帧
    2. 在应用于基于学习的Patchmatch算法之前，提取了像素级的特征，这与特征金字塔网络(FPN)相似。特征在多分辨率图像上分层提取，可以加速深度图从粗糙到精细的计算
2. 基于学习的PatchMatch

    ![PatchMatchNet pipeline](../pictures/PatchMatchNet/PatchMatchNet%20pipeline.png)
    1. 步骤：
        1. 初始化：生成随机的假设
        2. 传播：传播假设到邻域
        3. 评估：为所有的假设计算匹配代价，并选择最佳的结果
        - 初始化后，该方法在传播和评估之间迭代，直到收敛。利用深度学习，我们提出了传播和评估模块的自适应版本，并调整了初始化。简而言之，传播模块根据提取的深度特征自适应采样点进行传播。自适应评估自学去估计用于成本计算的可见性信息，并自适应地对空间邻域进行采样，以基于深度特征再次汇聚代价。与此前方法不同的是，我们避免将每像素假设参数化为倾斜平面，因为存储消耗很大。相反，依靠学习的自适应评估来组织窗口内的空间模式，在该窗口上计算匹配代价
    2. 初始化和局部扰动
        1. 在Patchmatch的第一次迭代中，以随机方式执行初始化以促进多样化
            - 基于预定义的深度范围 $[d_{min}，d_{max}]$ ，我们在逆深度范围内对每像素 $D_f$ 深度假设进行采样，对应于图像空间中的均匀采样。这有助于我们的模型适用于复杂和大规模场景
            - 为了确保我们均匀地覆盖深度范围，我们将（逆）范围划分为 $D_f$ 区间，并确保每个区间由一个假设覆盖
            - 由于 $disparity = \frac{baseline \cdot f}{depth}$ ，所以深度和视差为反比例关系，因此深度取倒数就是在视差，即像素层面上操作
        2. 对于阶段k上的后续迭代，我们通过在归一化逆深度范围 $R_k$ 中均匀地生成每像素 $N_k$ 个假设来执行局部扰动，并针对更精细的阶段逐渐减小 $R_k$
            - 为了定义 $R_k$ 的中心，我们利用先前迭代的估计，可能是从较粗的阶段上采样。这提供了一套比仅仅使用传播更为多样的假设
            - 围绕先前的估计进行采样可以局部细化结果并纠正错误的估计（见 ***补充材料*** ）。
    3. 自适应传播
        - ![PatchMatchNet adaptive propagation](../pictures/PatchMatchNet/PatchMatchNet%20adaptive%20propagation.png)

        1. 深度值的空间一致性通常只存在于同一物理表面的像素。因此，我们希望以自适应方式进行传播，从同一表面收集假设，而不是像Gipuma和DeepPruner那样，从静态的邻域集合中传播深度假设。这有助于PatchMatch更快地收敛，并提供更准确的深度图。
            - 我们的自适应方案倾向于从同一表面的像素收集假设——对于有纹理的对象和无纹理的区域——使我们能够有效地收集比仅使用静态模式更有前景的深度假设
        2. 我们将自适应传播的实现基于可变形卷积网络。由于每个解析阶段的方法相同，我们省略了表示该阶段的子索引
            - 为了聚集参考帧中像素 $\mathbf{p}$ 的深度假设 $K_p$ ，我们的模型学习了额外的2D偏移量 $\lbrace \Delta \mathbf{o}_ {i}(\mathbf{p}) \rbrace^{K_{p}}_ {i=1}$ 其应用于固定2D偏移 $\lbrace \mathbf{o}_ {i} \rbrace^{K_{p}}_ {i=1}$ 的顶部，并组织为网格
            - 我们在参考特征图 $\mathbf{F}_ {0}$ 上应用2D CNN以学习每个像素 $\mathbf{p}$ 的附加2D偏移，并通过双线性插值获得深度假设 $\mathbf{D}_ {p}(\mathbf{p})$ ，如下所示： $$\mathbf{D}_ {p}(\mathbf{p}) = \lbrace \mathbf{D}(\mathbf{p} + \mathbf{o}_ {i} + \Delta \mathbf{o}_ {i}(\mathbf{p})) \rbrace^{K_p}_ {i=1}$$ 其中 $\mathbf{D}$ 是上一次迭代的深度图，可能是从更粗糙的阶段上采样的

    4. 自适应评估
        - 自适应评估模块执行以下步骤：可微分扭曲、匹配成本计算、自适应空间代价聚合和深度回归。由于该方法在每个解析阶段都是相同的，因此我们省略了子索引以便于记法
        1. 可微分扭曲
            - 在平面扫描立体之后，大多数基于学习的MVS方法在采样深度假设下建立前向平行平面，并将源图像的特征图扭曲成它们。配备参考帧0和邻域帧i的固有矩阵 $\lbrace K_i \rbrace^{K}_ {i=0}$ 和相对变换 $\lbrace [\mathbf{R}_ {0,i} | \mathbf{t}_ {0,i}] \rbrace^{K}_ {i=1}$ ，我们计算参考中像素 $\mathbf{p}$ 在邻域中,以齐次坐标给出的对应像素 $\mathbf{p}_ {i,j} := \mathbf{p}_ {i}(d_{j})$ ，以及深度假设 $d_{j} := d_{j}(\mathbf{p})$ 如下： $$\mathbf{p}_ {i,j} = \mathbf{K}_ {i} \cdot (\mathbf{R}_ {0,i} \cdot (\mathbf{K}^{-1}_ {0} \cdot \mathbf{p} \cdot d_{j}) + \mathbf{t}_ {0,i})$$ 我们通过可微双线性插值获得视图i的扭曲邻域特征图和第j组（每像素不同）深度假设 $\mathbf{F}_ {i}(\mathbf{p}_ {i,j})$
        2. 匹配代价计算
            - 对于多视图立体，这一步骤必须将来自任意数量的邻域视图的信息集成到单个像素代价 $\mathbf{p}$ 和深度假设 $d_j$ 中。为此，我们通过逐组相关性计算每个假设的代价，并使用逐像素视图权重对视图进行聚合。通过这种方式，我们可以在代价聚合期间使用可见性信息并获得鲁棒性。最后，通过一个小网络，将每个组的代价预测为单个数字，每个参考像素和假设
            - 分别让 $\mathbf{F}_ {0}(\mathbf{p}) , \mathbf{F}_ {i}(\mathbf{p}_ {i,j}) \in \mathbb{R}^{C}$ 作为参考帧和邻域帧的特征。在把它们的特征通道分成G组后， $\mathbf{F}_ {0}(\mathbf{p})^{g}$ 和 $\mathbf{F}_ {i}(\mathbf{p}_ {i,j})^{g}$ ，以及第g个组的相似度 $\mathbf{S}_ {i}(\mathbf{p} , j)^{g} \in \mathbb{R}$ 的计算： $$\mathbf{S}_ {i}(\mathbf{p} , j)^{g} = \frac{G}{C} \langle \mathbf{F}_ {0}(\mathbf{p})^{g} , \mathbf{F}_ {i}(\mathbf{p}_ {i,j})^{g} \rangle$$ 其中， $\langle \cdot , \cdot \rangle$ 是内积。我们使用 $\mathbf{S}_ {i}(\mathbf{p} , j) \in \mathbb{R}^{G}$ 表示相应的组相似性向量。假设和像素的聚合提供了张量 $\mathbf{S}_ {i} \in \mathbb{R}^{W \times H \times D \times G}$
            - 要查找逐像素视图权重， $\lbrace \mathbf{w}_ {i}(\mathbf{p}) \rbrace^{N-1}_ {i=1}$ ，我们在第3阶段的第一次迭代中利用了初始深度假设集的多样性。我们打算用 $\mathbf{w}_ {i}(\mathbf{p})$ 表示源图像 $\mathbf{I}_ {i}$ 中像素 $\mathbf{p}$ 的可见性信息。权重计算一次，并保持固定，并为更精细的阶段进行上采样
            - 一个简单的逐像素视图权重网络，由具有1×1×1核和Sigmoid形非线性的3D卷积层组成，采用初始相似度集 $\mathbf{S}_ {i}$ 来输出每个像素0到1之间的数字，并采用深度假设来生成 $\mathbf{P}_ {i} \in \mathbb{R}^{W \times H \times D}$ 。像素 $\mathbf{p}$ 和邻域图像 $\mathbf{I}_ {i}$ 的视图权重由下式给出： $$\mathbf{w}_ {i}(\mathbf{p}) = max \lbrace \mathbf{P}_ {i}(\mathbf{p} , j) | j = 0,1,...,D-1 \rbrace$$ 其中， $\mathbf{P}_ {i}(\mathbf{p} , j)$ 直观地表示 $\mathbf{p}$ 处第j深度假设所覆盖范围的可见性置信度
            - 像素 $\mathbf{p}$ 和第j个假设的最终每组相似性 $\bar{\mathbf{S}}(\mathbf{p} , j)$ 是权重之和 $\mathbf{S}_ {i}(\mathbf{p} , j)$ 和视图权重 $\mathbf{w}_ {i}(\mathbf{p})$ ： $$\bar{\mathbf{S}}(\mathbf{p} , j) = \frac{\sum^{N-1}_ {i=1} \mathbf{w}_ {i}(\mathbf{p}) \cdot \mathbf{S}_ {i}(\mathbf{p} , j)}{\sum^{N-1}_ {i=1} \mathbf{w}_ {i}(\mathbf{p})}$$
            - 最后，我们将所有像素和假设的 $\bar{\mathbf{S}}(\mathbf{p} , j)$ 合成为 $\bar{\mathbf{S}} \in \mathbb{R}^{W \times H \times D \times G}$ ，并应用具有3D卷积和1×1×1核的小网络，以获得单个成本 $\mathbf{C} \in \mathbb{R}^{W \times H \times D}$ ，每像素和深度假设
        3. 自适应空间代价聚合
            - 传统的MVS匹配算法通常在空间窗口（即，在我们的情况下为一个前向平行平面）上聚集代价，以提高匹配鲁棒性和隐式平滑效果。可以说，我们的多尺度特征提取器已经从空间域中的一个大的感受野中聚集了邻域信息。然而，我们建议研究空间代价聚合。为了防止跨表面边界聚集的问题，我们提出了一种基于Patchmatch立体和AANet的自适应空间聚集策略。对于 $K_e$ 像素 $\lbrace \mathbf{p}_ {k} \rbrace^{K_e}_ {k=1}$ 的空间窗口，将其组织为网格，我们学习每个像素的额外偏移 $\lbrace \Delta \mathbf{p}_ {k} \rbrace^{K_e}_ {k=1}$ 。聚集空间成本 $\tilde{\mathbf{C}}(\mathbf{p},j)$ 定义为： $$\tilde{C}(\mathbf{p},j) = \frac{1}{\sum^{K_e}_ {k=1}w_{k}d_{k}} \sum^{K_e}_ {k=1}w_{k}d_{k} \mathbf{C}(\mathbf{p} + \mathbf{p}_ {k} + \Delta \mathbf{p}_ {k},j)$$ 其中， $w_{k}$ 和 $d_{k}$ 基于特征和深度相似度对成本C进行加权（细节在 ***补充材料*** 中）。与自适应传播类似，每像素位移集 $\lbrace \Delta \mathbf{p}_ {k} \rbrace^{K_e}_ {k=1}$ 通过在参考特征图 $\mathbf{F}_ {0}$ 上应用2D CNN找到。

            ![PatchMatchNet sample](../pictures/PatchMatchNet/PatchMatchNet%20sample.png)

            举例说明了学习的自适应聚集窗口。采样位置保持在对象边界内，而对于无纹理区域，采样点在更大的空间背景上聚集，这可能会减少估计的模糊性
        4. 深度回归
            - 使用softmax，我们将（负）代价 $\tilde{\mathbf{C}}$ 转换为概率 $\mathbf{P}$ ，用于亚像素深度回归和测量估计置信度。像素 $\mathbf{p}$ 处的回归深度值 $\mathbf{D}(\mathbf{p})$ 被发现为假设的预期w.r.t. $\mathbf{P}$ ： $$\mathbf{D}(\mathbf{p}) = \sum^{D-1}_ {j=0} d_j \cdot \mathbf{P}(\mathbf{p},j)$$
5. 深度贴图细化
    - 我们发现，与其在最佳分辨率级别（阶段0）上使用Patchmatch，不如直接向上采样（从分辨率 $\frac{W}{2} \times \frac{H}{2}$ 到 $W \times H$ ），并用RGB图像细化我们的估计
    - 基于MSG-Net，我们设计了深度残差网络。为了避免对某个深度标度产生偏差，我们将输入深度图预缩放到范围[0，1]，并在细化后将其转换回。我们的细化网络学习输出残差，该残差被添加到Patchmatch， $\mathbf{D}$ 的（上采样）估计中，以获得细化的深度图 $\mathbf{D}_ {ref}$ 
    - 该网络独立地从 $\mathbf{D}$ 和 $\mathbf{I}_ {0}$ 提取特征图 $\mathbf{F}_ {D}$ 和 $\mathbf{F}_ {I}$ ，并对 $\mathbf{F}_ {D}$ 应用反卷积，以将特征图上采样到图像大小。多个2D卷积层应用于两个特征图（深度图和图像）的级联(concatenation)之上，以提供深度残差

6. 损失函数
    - 损失函数 $L_ {total}$ 将所有深度估计和具有相同分辨率的真值情况之间的损失视为总和： $$L_ {total} = \sum^{3}_ {k=1} \sum^{n_k}_ {i=1} L^{k}_ {i} + L^{0}_ {ref}$$ 其中 $L^{k}_ {i}$ 为阶段 $k(k=1,2,3)$ 上Patchmatch的第i次迭代的损失和 $L^{0}_ {ref}$ ，以及最终细化深度图采用了平滑L1损失

# 四、实验结果
1. 稳健的训练策略
    - 许多基于学习的方法基于视图选择分数，选择两个最佳邻域视图，以训练DTU上的模型。然而，所选邻域视图与参考视图具有很强的可见性相关性，这可能会影响逐像素视图权重网络的训练。相反，我们提出了一种基于PVSNet的稳健训练策略。对于每个参考视图，我们从十个最佳邻域视图中随机选择四个进行训练。该策略增加了训练时的多样性，并实时增加了数据集，从而提高了泛化性能。此外，对那些具有弱可见性相关性的随机邻域视图的训练为我们的可见性估计提高了鲁棒性
2. 基准模型表现
    1. 在DTU数据集上的表现
        - Gipuma在准确性方面表现最佳，我们的方法在完整性方面优于其他方法，并在总体质量方面取得了竞争性表现
        - 我们的解决方案以更精细的细节重建更稠密的点云，这反映了更高的完整性。此外，我们在边界和薄结构处的重建似乎优于CasMVSNet。我们的自适应传播可以通过使用边界内邻域的信息，从边界处的误差中恢复在较粗的分辨率下引起的误差。如果仅依赖先前估计的采样，如CasMVSNet，可能会失败
    2. 内存和运行时比较
        - 我们将内存消耗和运行时间与几种最先进的基于学习的方法进行了比较，这些方法在低内存消耗和低运行时间的情况下实现了竞争性能：CasMVSNet、UCS-Net和CVP-MVSNet。这些方法提出了与输入图像相同分辨率的3D成本体和输出深度图的级联公式。
        
        ![PatchMatchNet memory comp](../pictures/PatchMatchNet/PatchMatchNet%20memory%20comp.png)

        - 当深度假设的数量固定时，所有方法的内存和运行时间几乎随分辨率线性增加（特别是，这将导致使用原始成本体方法的方法的放大图像空间采样不足）。请注意，在更高的分辨率下，其他方法无法装入用于评估的GPU的内存。我们进一步观察到，PatchmatchNet的内存消耗和运行时间增长比其他方法慢得多。例如，在分辨率为1152×864（51.8%）的情况下，与CasMVSNet相比，内存消耗和运行时间分别减少了67.1%和66.9%，与UCS Net相比分别减少了55.8%和63.9%，与CVP MVSNet比较分别减少了68.5%和83.4%。我们得出结论，我们的方法在内存消耗和运行时间方面比大多数最先进的基于学习的方法更有效，而且性能非常有竞争力。
    3. 在Tanks & Temples数据集上的表现
        - 我们使用在DTU上训练的模型，没有任何微调。为了进行评估，我们将输入图像大小设置为1920×1056，视图数N设置为7。使用OpenMVG恢复相机参数和稀疏点云。在评估期间，每个深度图的GPU内存和运行时间分别为2887 MB和0.505 s。我们的方法在中间数据集上的性能与得分最高的CasMVSNet相当。 ***对于更复杂的高级数据集，我们的方法在所有方法中表现最好。*** 总的来说，由于其简单、可扩展的结构，与通常使用3D成本体积正则化的最先进的基于学习的方法相比，我们的PatchmatchNet展示了具有竞争力的泛化性能、低内存消耗和低运行时间
    4. 在ETH3D上评估
        - 我们使用在DTU上训练的模型，没有任何微调。为了评估，我们将输入图像大小设置为2688×1792。由于ETH3D中的视点变化很大，我们还使用N=7个视图来利用更多的多视图信息。使用COLMAP恢复相机参数和稀疏点云。在评估期间，用于估计每个深度图的GPU内存消耗和运行时间分别为5529 MB和1.250 s。在训练数据集上，我们的方法的性能与COLMAP和PVSNet相当。在特别具有挑战性的测试数据集上，我们的方法在所有方法中表现最佳。此外，我们的方法是迄今为止（2020年11月16日）在ETH3D基准上评估最快的方法。注意到PVSNet是一种最先进的基于学习的方法，定量结果证明了我们方法的有效性、效率和泛化能力
        - 通过另一组实验我们得出结论，我们的像素视图权重确实能够确定参考视图和源视图之间的共可见区域
3. 消融实验
    - 我们进行消融研究以分析部件。除非另有规定，以下所有研究都是在DTU的评估集上进行的
    1. 自适应传播（AP）和自适应评估（AE）
        - 我们将我们的基础模型与使用固定2D偏移（类似于Gipuma）进行传播（w/o AP）的版本进行比较，并将固定2D补偿用于在评估步骤（w/o AE）中对邻域进行空间成本聚集采样。我们的自适应传播和自适应评估模块都提高了结果的准确性和完整性。
    2. 迭代次数
        - 回想一下，在训练期间，我们不包括第1阶段Patchmatch的自适应传播。因此，我们也将第1阶段的迭代次数保持为1。Patchmatch的更多迭代通常会提高性能，但是，在“2,2,1”迭代之后，改进会停滞
        
        ![PatchMatchNet error distribution](../pictures/PatchMatchNet/PatchMatchNet%20error%20distribution.png)
        
        我们进一步可视化了图中设置“2,2,1,1”的反向深度范围内的归一化绝对误差分布。我们观察到，在所有阶段的Patchmatch5次迭代之后，误差会收敛。与使用大量邻居进行传播的Gipuma相比，我们将Patchmatch嵌入了一个从粗到细的框架中，以加快远程交互。除此之外，我们学习的自适应传播、多样的初始化和局部扰动都有助于更快的收敛
    3. 像素视图权重（Pixel-wise View Weight VW）和稳健训练策略（Robust Training Strategy RT）
        - 在这个实验中，我们放弃了逐像素视图加权（w/o VW），并没有遵循我们的策略，而是选择四个最佳源视图进行训练（w/oRT）。为了研究泛化性能，我们进一步测试了Tanks&Temples和ETH3D。我们观察到在没有像素视图权重或稳健训练策略的其他数据集上的性能下降。这证明了这两个模块提高了鲁棒性和更好的泛化性能
    4. 视图数
        - 除了我们对DTU评估集的N=5个视图的标准设置外，我们还评估了N=2、3、6时的性能。使用更多视图可以提高性能，例如通过缓解遮挡问题。使用更多输入视图，重建质量在准确性和完整性方面都会有所提高
## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论

## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、补充材料
1. 为什么不使用3D成本体正则化
    1. 我们基于学习的Patchmatch的自适应评估利用具有1×1×1核的3D卷积层进行匹配成本计算以及逐像素视图权重估计。这与之前常见的工作形成对比，其中3D U-Net规范了成本体。类似地，PVSNet认为成本体的分布本身不够有区别，也应用了3D U-Net来预测每个源视图的可见性
    2. 这种正则化框架的问题在于它要求代价体中有规则的空间结构。尽管我们将每个像素的匹配成本和深度假设与其他作品一样连接(concatenate)成体的形状，但我们不具备这样的规则结构：
        - 每个像素及其空间邻居的深度假设不同，这使得难以在空间域中聚合成本信息
        - 每个像素的深度假设在与CIDER相反的深度范围内不均匀分布，这使得难以沿深度维度聚集成本信息
    3. 回想一下，在Patchmatch的初始迭代中计算逐像素视图权重期间，深度假设在逆深度范围内随机分布，即每个像素的假设在空间上不同。在每个后续迭代（阶段k）中，我们通过在归一化逆深度范围 $R_k$ 中均匀地生成每像素 $N_k$ 个深度假设来执行局部扰动，该范围以先前的估计为中心。因此，空间邻域的假设可能存在显著差异，尤其是在深度不连续和薄结构处。此外，包括通过自适应传播获得的假设，即在逆深度范围内不均匀的假设，将进一步增加这些影响
    4. 然而，最终，我们避免3D成本体正则化的方法的主要原因是效率。在粗略到精细的框架中，在每个阶段的Patchmatch的多次迭代中运行这样的正则化框架将显著增加内存消耗和运行时间，并减轻我们构建高性能但特别轻量级的框架的主要贡献，该框架可以以高计算速度运行

2. 如何在Patchmatch的局部扰动步骤中设置归一化反深度范围 $R_k$ ？
    1. 在初始迭代之后，我们的假设集通过自适应传播和先前估计的局部扰动获得。回想一下，我们的局部扰动过程通过在归一化逆深度范围 $R_k$ 内均匀生成每像素 $N_k$ 个深度假设来丰富假设集。目标是双重的。特别是在开始时，在低分辨率下，这有助于进一步探索搜索空间。更重要的是，我们的自适应传播隐含地假设了前向平行表面，因为我们没有明确地包括切向表面信息（由于隐含的大量内存消耗）。在先前估计的局部附近采样将局部细化解决方案，并减轻未明确建模切向表面信息的潜在缺点。我们发现，在早期阶段就应用这些扰动有助于为假设传播注入积极影响，并注意到，仅在最佳水平上进行后验改进无法恢复相同的质量。在实践中，我们再次以粗略到精细的方式进行操作，并根据层次结构级别相应地设置 $R_k$ 

    ![PatchMatchNet Cumulative](../pictures/PatchMatchNet/PatchMatchNet%20Cumulative.png)

    2. 上图显示了DTU评估集上反向深度范围内归一化绝对误差的累积分布函数。在第3阶段Patchmatch的第一次迭代之后，估计误差显著降低：90.0%的情况下，归一化误差已经小于0.1。显而易见，每次迭代后，性能都会不断提高。为了校正估计误差并细化阶段k的结果，我们将 $R_k$ 设置为补偿大部分估计误差。例如，在第一次迭代之后，我们在第3阶段设置Patchmatch的 $R_3 = 0.38$ ，以便我们可以覆盖假设范围内的大部分地面真实深度，然后细化结果。此外，当 $R_k$ 中的采样在细化中失败时，自适应传播将进一步使用来自邻居的深度假设来校正那些错误的估计
3. 为什么不包括阶段1上Patchmatch的最后一次迭代的传播？
    - 与MVSNet类似，点云重构主要包括光度一致性滤波、几何一致性滤波和深度融合。光度一致性过滤用于过滤掉那些具有低置信度的深度假设。基于MVSNet，我们将置信度定义为在估计附近的小范围内的深度假设的概率和。我们使用阶段1上Patchmatch最后一次迭代的概率P $\mathbf{D}(\mathbf{p}) = \sum^{D-1}_ {j=0} d_j \cdot \mathbf{P}(\mathbf{p},j)$ 进行滤波。在这个迭代中，我们只执行局部扰动，而没有自适应传播。在第1阶段，以四分之一的图像分辨率运行，并且算法几乎收敛，通过从空间邻居传播获得的假设通常与当前解决方案非常相似。概率空间的这种不规则采样导致回归中的偏差，并且估计在大多数传播样本所在的当前解处变得过于自信。相反，通过仅执行局部扰动，深度假设在逆深度范围内均匀分布。与之前的迭代相反，我们通过使用基于soft-argmin运算的逆深度回归计算像素 $\mathbf{p}$ 处的估计深度 $\mathbf{D}(\mathbf{p})$ ： $$\mathbf{D}(\mathbf{p}) = (\sum^{D-1}_ {j=0} \frac{1}{d_j} \cdot \mathbf{P}(\mathbf{p},j))^{-1}$$ 其中， $\mathbf{P}(\mathbf{p},j)$ 是像素 $\mathbf{p}$ 在第j层深度的概率。然后我们计算最接近估计的四个深度假设的概率和，以测量置信度
4. 自适应空间成本聚集中的加权
    1. 回想一下，在本文的等式 $\tilde{C}(\mathbf{p},j) = \frac{1}{\sum^{K_e}_ {k=1} w_{k} d_{k}} \sum^{K_e}_ {k=1}w_{k}d_{k} \mathbf{C}(\mathbf{p} + \mathbf{p}_ {k} + \Delta \mathbf{p}_ {k},j)$ 中，我们使用两个权重来聚合我们的空间成本，基于空间特征相似性的 $\lbrace w_ {k} \rbrace^{K_e}_ {k=1}$ 和基于深度假设的相似性的 $\lbrace d_ {k} \rbrace^{K_e}_ {k=1}$ 。像素 $\mathbf{p}$ 处的特征权重 $\lbrace w_ {k} \rbrace^{K_e}_ {k=1}$ 基于在参考特征图 $\mathbf{F}_ {0}$ 中测量的 $\mathbf{p}$ 周围的采样位置处的特征相似性 $\lbrace \mathbf{p} + \mathbf{p}_ {k} + \Delta \mathbf{p}_ {k} \rbrace^{K_e}_ {k=1}$ ，我们通过双线性插值从 $\mathbf{F}_ {0}$ 中提取相应的特征。然后，我们在每个采样位置的特征和 $\mathbf{p}$ 之间应用逐组相关。结果被连接到一个体积中，我们在该体积上应用具有1×1×1核和Sigmoid非线性的3D卷积层，以输出描述每个采样点和 $\mathbf{p}$ 之间相似性的归一化权重
    2. 如第1节所述，在整个估计过程中，相邻像素将被分配不同的深度值。对于像素 $\mathbf{p}$ 和第j个深度假设，我们的深度权重 $\lbrace d_ {k} \rbrace^{K_e}_ {k=1}$ 考虑了这一点，并降低了具有较大深度差的样本的影响，特别是当位于深度不连续性上时。为此，我们使用第j个假设收集每个采样点和像素 $\mathbf{p}$ 之间的反向深度绝对差，并通过对反向差应用Sigmoid函数进行归一化来获得权重

5. 多级深度估计的评价
    - 我们使用多个阶段以粗略到精细的方式估计深度图。在这里，我们分析了多阶段框架的有效性。我们在第3、2和1阶段对估计的深度图进行上采样，以达到与输入相同的分辨率，然后重建点云。重建质量从较粗的阶段逐渐提高到较细的阶段。这表明我们的多阶段框架可以以更高的精度和完整性重建场景几何结构

6. 自适应传播的可视化
    - 我们在两种典型情况下可视化采样位置，即对象边界和无纹理区域。
    
    ![PatchMatchNet figure11](../pictures/PatchMatchNet/PatchMatchNet%20figure11.png)

    对于物体边界处的像素 $\mathbf{p}$ ，所有采样点往往位于与 $\mathbf{p}$ 相同的表面上。相反，对于无纹理区域中的像素 $\mathbf{q}$ ，采样点分布在更大的区域上。通过从大区域采样，可以将更多样的深度假设集传播到 $\mathbf{q}$ ，并减少无纹理区域中深度估计的局部模糊性。可视化显示了自适应传播如何成功地使采样适应不同的挑战性情况的两个示例

7. 自适应评估的可视化
    - 这里，我们再次可视化了两种情况下的采样位置，即对象边界和无纹理区域。
    
    ![PatchMatchNet figure12](../pictures/PatchMatchNet/PatchMatchNet%20figure12.png)
    
    对于物体边界处的像素 $\mathbf{p}$ ，采样点往往停留在物体边界内，从而它们聚焦于相似的深度区域。对于无纹理区域中的像素 $\mathbf{q}$ ，点被稀疏分布以从大背景中采样，这有助于获得可靠的匹配并减少模糊度。同样，可视化展示了我们的自适应评估如何将空间成本聚集的采样适应不同的情况



# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论
