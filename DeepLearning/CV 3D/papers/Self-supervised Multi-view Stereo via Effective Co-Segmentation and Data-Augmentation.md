# 论文信息
- 时间：2021
- 期刊：AAAI
- 网络名称：JDACS-MS (Joint Data Augmentation and Co-Segmentation self-supervised MVS framework)
- 意义：无监督网络解决不同视角下颜色不一致问题
- 作者：Hongbin Xu1,3*, Zhipeng Zhou1*, Yu Qiao1,2†, Wenxiong Kang3, Qiuxia Wu3; 1ShenZhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; 2Shanghai AI Lab, Shanghai, China; 3South China University of Technology, Guangzhou, China
- 实验环境： 4 NVIDIA RTX 2080Ti GPUs
- 数据集：DTU; Tanks&Temples

# 一、解决的问题
1. 颜色恒定模糊(color constancy ambiguity)
    - 现状：现有的方法依赖于不同视图之间的对应点共享相同颜色的假设，这在实践中可能并不总是正确的。这可能导致不可靠的自监督信号，并损害最终重建性能
    - 原因：颜色相同的假设在实际场景中往往会因为光照条件，反射，噪声等因素而不成立
2. 问题：3D Deep Learning在监督和非监督结果之间仍然存在明显的差距
3. 问题：自监督学习的损失往往是根据真实视图的重建深度图和另一个视角的重建深度图之间的误差
4. 一方面，语义注释相对昂贵。另一方面，场景的巨大变化使得语义类别对于需要指定类的分割来说是不固定的
    - 解决：无监督协同分割采用NMF(non-negative matrix factorization)来动态挖掘不同views间共同的语义簇
5. 问题：数据增强中的自然颜色波动将导致自我监督中的颜色恒定性模糊
    - 解决：为了保护自监督的可靠性，添加了一个使用不同增强方式的数据增强分支。常规的训练分支的输出作为伪真值来监督数据增强分支的输出

# 二、做出的创新
1. 提出了一个更可靠的集成了语义协同分割(semantic co-segmentation)和数据增强(data augmentation)的监督框架
    - 从多视图图像中挖掘相互语义，以指导语义一致性
    - 设计了有效的数据增强机制，通过将常规样本的预测视为 ***伪真值*** 来规范化增强样本的预测，从而确保变换的鲁棒性
2. 解决颜色恒定模糊(color constancy ambiguity)
    - 先验的语义一致性可以提供抽象的匹配线索来指导监督
    - 数据增强一致性的先验可以增强对颜色波动的鲁棒性
3. 所有的贡献：
    - 提出了统一无监督MVS管道机制。提取先验的语义一致性和数据增强一致性，提供了可靠的引导来克服颜色恒定不确定性
    - 提出了新的自监督方法：语义一致性可以挖掘，在任意场景下，不同视图间，相同的语义信息
    - 提出了新颖的方式，向无监督MVS中合并大量数据增强，提供了颜色波动的规范化
    - 实验证明，JDACS-MS模型比此前的无监督网络效果都要好上一大截，而且可以和部分有监督网络竞争

# 三、设计的模型
![JDACS-MS struct](../pictures/JDACS-MS%20struct.png)

1. 一共三个分支，两次反向传播
    - 深度估计(Depth Estimation)：与协同分割的loss一块做反向传播，共有四个loss：分割、光度一致性、SSIM和Smooth
    - 协同分割(Co-Segmentation)：与深度估计的loss一块更新CVP-MVSNet为主干网络的参数
    - 数据增强(Data-Augmentation)：单独一个分支，做了图像伽马变换（就是改变颜色）和随机掩码。用深度估计的结果做GroudTruth，进行反向传播
2. 深度估计(Depth Estimation)
    1. 可以使用任意一个此前的MVS网络的结构，本文采用MVSNet作为主干网络(backbone)
    2. 光度一致性(Photometric Consistency)
        - 目的是最小化原图于合成图在相同视角下的差异
        - 算法：取参考帧的像素，投影到邻域帧上，获得对应信息；从邻域帧到参考帧的扭曲变化图像可以由邻域帧可微双线性抽样得到 $$I^{\prime}_ {i} (p_ {j})=I_ {i} (p^{\prime}_ {j})$$ 随着变形，同时生成二进制有效性掩码 $M_ {i}$ ，指示新视图中的有效像素，因为一些像素可能会投影到图像的外部区域。 在MVS系统中，我们可以将所有 $N - 1$ 个源视图扭曲到参考视图以计算损失 $$\mathbb{L}_ {PC} = \sum^{N}_ {i=2} \frac{||(I^{\prime}_ {i} - I_ {1}) \odot M_ {i}||_ {2} + (\nabla I^{\prime}_ {i} - \nabla I_ {1}) \odot M_ {i}||_ {2}}{||M_ {i}||_ {1}}$$ 其中， $\nabla$ 代表梯度， $\odot$ 是向量点乘
3. 协同分割(Co-Segmentation)
    1. 原因：由于场景的巨大变化和MVS中手动注释的昂贵成本，手工标注的效率太低，选择通过无监督的共同分割从多视图图像中挖掘隐含的公共片段
    2. 目标：协同分割旨在定位给定图像集合的公共对象的前景像素。且NMF具有固有的聚类特性，应用在训练的CNN层的激活函数中，可以用来寻找图像间的语义关系
    3.  NMF(non-negative matrix factorization)
        - 非负矩阵分解是一组多元分析于线性代数中的算法，例如一个矩阵A分解成P和Q矩阵，并且这三个矩阵都没有负的元素
        - 非负矩阵分析具有自动对矩阵的列进行聚类的特点
        - 如果要求Q矩阵是正交矩阵的话 $Q Q^T = I$ , 则满足近似关系 $A \simeq P Q$ 要求最小化下面的函数，等价于优化K-means聚类,其中 $F$ 是弗罗贝尼乌斯规范： $$|| A - P Q ||_F , P \ge 0 , Q \ge 0$$
    4. 基于CNN激活的聚类
        - ReLU由于它的理想梯度特性，被广泛用于CNN中
        - 在协同分割结构中，引入了预训练的VGG网络用于特征提取。N个视角中提取到的特征图，每一个的维度都是(H,W,C)
        - 之后多视角特征图被连接(concatenate)在一起，并且重塑成矩阵A，维度是(NHW,C)
        - 根据乘法更新规则(multiplicative update rule in (Ding, He, and Simon 2005))求解NMF，矩阵A被分解成维度为(NHW,K)的矩阵P和维度为(K,C)的矩阵Q。其中，K是NMF代表语义聚类数量的因子
    ![JDACS-MS NMF](../pictures/JDACS-MS%20NMF.png)

    5. Q矩阵
        - 由于正交矩阵的约束，矩阵Q的每行可以看作是C维的一个聚类中心，该中心对应视图间的一个相关对象
    6. P矩阵
        - 矩阵分解增强了P的每行和Q的每列之间的乘积，以最佳地近似A中每个像素的C维特征。
        - P矩阵的行对应N个图像的每个像素的空间位置
        - P包含了每个像素与每个聚类的相似性，可以进一步重塑成N个热度图送入softmax层，构建分割图
    7. 语义一致性损失
        - 目的：由于协同分割图S是从矩阵P中提取的，基于语义一致性，构建自监督约束
        - 思想：借鉴光度一致性的思想并通过多视角扩展到分割图上
            - 首先计算参考帧每个像素p与邻域帧上的匹配点坐标p'
            - 再通过双线性采样从邻域帧映射到参考帧，得到扭曲变换的分割图 $S^{\prime}_i$ ，再每个像素计算交叉熵 
            
            $$S^{\prime}_i(p_j)=S_i(p^{\prime}_j)$$

            $$\mathbb{L}_ {SC} = - \sum^{N}_ {i=2} [\frac{1}{||M_i||_ {1}} \sum^{HW}_ {j=1}f(S_{1,j}) log(S^{\prime}_ {i,j})M_ {i,j}]$$
            - 这里 $f(S_{1,j}) = onehot(argmax(S_{i,j}))$ 并且 $M_i$ 是指示从第i个邻域帧到参考帧的有效像素的二进制掩码。
4. 数据增强(Data-Augmentation)
    1. 使用原因：
        - 方案：一个随机的向量 $\theta$ 被用于在图像I上，参数化一个任意的数据增强 $\tau_{\theta} : I \to \hat{I}_ {\tau_{\theta}}$
        - 思想：对比学习中的一些最新研究证明了数据增强在自监督学习中的益处。数据增强带来了具有挑战性的样本，这增强了无监督损失的可靠性，从而提供了对变化的鲁棒性
        - 问题：数据增强很少使用在自监督学习中，因为自然颜色波动在数据增强图像中，会干扰到自监督的颜色一致性约束
        - 措施：本文通过对比原始数据和增强数据的输出作为正则化项，而非优化原始数据
    2. 数据增强一致性损失
        - 原始数据I的深度估计图D；数据增强图 $\hat{I}_ {\tau_ {\theta}}$ 的深度估计图 $\hat{D}_ {\tau_ {\theta}}$
        - 在对比学习方法中，是要最小化两者的差异：

        $$\mathbb{L}_ {DA} = \frac{1}{||M_ {\tau_ {\theta}}||_ {1}} \sum||(D - \hat{D}_ {\tau_ {\theta}}) \odot M_ {\tau_ {\theta}}||_ {2}$$
        - 这里 $M_{\tau_{\theta}}$ 代表变换下的遮挡信息 $\tau_{\theta}$
        - 由于不同视图之间的对极约束，我们框架中的集成增强方法不应改变像素的空间位置。***下面将展示我们的方法中使用的一些增强方法，如下所示：***
    3. 交叉视图掩码：
        - 为了模拟多视图场景中的遮挡幻觉，我们随机生成一个二元裁剪遮罩 $1 - M_{\tau_{\theta_1}}$ ，以遮挡参考视图上的一些区域
        - 然后将遮挡掩码投影到其他视图，以遮蔽图像中的相应区域
        - 假设剩余区域 $M_{\tau_{\theta_1}}$ 应该对变换免疫，我们可以对比原始样本和增强样本的结果之间的有效区域
    4. 伽马校正
        - 作用原理：伽马校正是一种非线性操作，用于调整图像的照度
        - 方案：为了模拟各种照明，我们集成了由 $\theta_2$ 参数化的随机伽马校正 $\tau_ {\theta_2}$ ，以挑战无监督损失

    5. 颜色抖动和模糊(Color Jitter and Blur)
        - 许多变换可以将颜色波动附加到图像，例如随机颜色抖动、随机模糊、随机噪声
        - 颜色波动使得MVS中的无监督损失不可靠，因为光度损失需要视图之间的颜色恒定性。相反，这些表示为 $\tau_{\theta_3}$ 的变换可以创建具有挑战性的场景，并在自我监督中调整对颜色波动的鲁棒性。
    - 整体变换 $\tau_ {\theta}$ 可以表示为上述增强的组合： $$\tau_ {\theta} =  \tau_ {\theta_3} \circ \tau_ {\theta_2} \circ \tau_ {\theta_1}$$ 这里 $\circ$ 代表复合函数，eg $(g \circ f)(x) = g(f(x))$
5. 总体架构和损失
    1. 总体框架有三个部分：
        - 深度估计分支
        - 共分割分支
        - 数据增强分支
    2. 目标：处理自监督MVS中的颜色恒定模糊问题
    3. 除了基于光度一致性（颜色） $\mathbb{L}_ {PC}$ 的基本自我监督信号之外，我们将语义一致性（分类） $\mathbb{L}_ {SC}$ 和数据增强一致性 $\mathbb{L}_ {DA}$ 这两个额外的自我监督信号添加到框架中。除上述损失外，还应用了（Mahjourian，Wicke，and Angelova 2018；Khot et al.2019）提出的用于深度估计的一些常见正则化术语，如结构化相似性（评价两个图象的图像框相似性） $\mathbb{L}_ {SSIM}$ 和深度平滑度（用深度梯度代表RGB梯度） $\mathbb{L}_ {Smooth}$ 
    4. 最终目标可构建如下： $$\mathbb{L} = \lambda_1 \mathbb{L}_ {PC} + \lambda_2 \mathbb{L}_ {SC} + \lambda_3 \mathbb{L}_ {DA} + \lambda_4 \mathbb{L}_ {SSIM} + \lambda_5 \mathbb{L}_ {Smooth}$$ 这里权重根据经验设置为： $\lambda_1 = 0.8, \lambda_2 = 0.1, \lambda_3 = 0.1, \lambda_4 = 0.2, \lambda_5 = 0.0067$


# 四、实验结果
![JDACS-MS performance](../pictures/JDACS-MS%20performance.png)

1. 实施细节
    1. 主干网络：
        - 默认选择MVSNet
        - ***如果选择多级MVSNet（如CVP MVSNet）（Yang等人，2020）作为主干，我们将该框架表示为JDACS-MS***
    2. 训练和测试：
        - 训练和测试阶段的超参数遵循Unsup MVS的相同设置（Khot等人2019）
        - 对于JDACS，批次大小设置为每个GPU 1个，对于JDACS-MS，批次大小为每个GPU4个；这样消耗的显存每个GOU不到10G
        - Adam优化器，学习率0.001，每两个轮回减少一半
        - JDACS作为MVSNet训练了10个轮回（Yao等人，2018），JDACS-MS作为CVP MVSNet培训了27个轮回（杨等人，2020）
    3. 误差度量
        - Acc：是指从结果到真值的距离，反映了重建的质量
        - Completeness：是以从真值到结果的距离来衡量的，它反应了多少表面被捕获
        - Over-all：是准确性和完整性的平均值，作为综合误差度量

2. DTU基准结果
    1. 与SOTA的比较
        - 提出的方法在所有官方指标上都优于以前的无监督方法。此外，提出的方法在总体度量上可以比传统方法和一些监督方法更好地重构点云。有监督的方法在准确性度量方面往往具有更好的性能，而无监督方法通常在完整性度量方面取得更好的性能
    2. 有监督与自监督
        - SOTA监督方法和以前的无监督方法之间仍然存在明显的性能差距
        - 为了在没有额外组件的情况下提供公平的比较，将提出的自监督框架与相同网络设置中的监督方法进行了比较。唯一的区别是，我们的模型是在没有任何真值深度图的情况下训练的
        - 监督基线借用了以前的数据（MVSNet来自（Chen et al.2019），CVP MVSNet自（Yang et al.2020））
        - 结果表明，我们提出的框架可以在相同的网络环境中与有监督的对手打个平手

3. 消融实验
    1. 不同先验成分的影响
        - 将这些额外的先验赋予自监督训练可以提高MVS的表现
    2. 语义聚类数的影响
        - 语义聚类的数量K是确定不同视图中常见语义概念类别的一个重要超参数
        - 当语义聚类数大于4时，语义部分的定位比聚类数小于4的语义部分的准确度要低。因此，在我们提出的方法中，我们选择K＝4个簇作为默认设置

4. 泛化
    - 遵循与MVS2相同的超参数设置（Dai等人，2019）
    - 我们提出的JDACS比先前的无监督方法具有更好的性能，平均得分为8个场景，这是截至2020年9月9日的最佳无监督MVS方法

## 1、比之前模型的优势

## 2、有优势的原因

## 3、改进空间

# 五、结论
- 一方面，提出的方法可以将跨视图数据增强一致性强制为具有挑战性变化的自我监督
- 另一方面，挖掘不同视图之间的隐含公共语义簇，并加强跨视图语义一致性，以提供语义级对应度量
## 1、模型是否解决了目标问题

## 2、模型是否遗留了问题

## 3、模型是否引入了新的问题

# 六、补充材料
1. 实施补充细节
    1. NMF的实施
        - NMF在JDACS框架的共分割分支中扮演着重要角色。我们使用乘法更新规则迭代计算NMF的解
        - 算法逻辑，见代码部分
    
    2. JDACS-MS的实现
    
        ![JDACS-MS illustration](../pictures/JDACS-MS%20illustration.png)

        - 损失函数：
            $$\mathbb{L}_ {JDACS-MS} = \sum^{5}_ {s=1} (\lambda_1 \mathbb{L}^{s}_ {PC} + \lambda_2 \mathbb{L}^{s}_ {SC} + \lambda_3 \mathbb{L}^{s}_ {DA} + \lambda_4 \mathbb{L}_ {SSIM} + \lambda_5 \mathbb{L}_ {Smooth})$$ 
            这里，s表示多级MVSNet的每个阶段，默认情况下，它分为5个阶段；权重根据经验设置为： $\lambda_1 = 0.8, \lambda_2 = 0.1, \lambda_3 = 0.1, \lambda_4 = 0.2, \lambda_5 = 0.0067$
    3. 数据增强一致性
        - 采用各种变换来生成具有挑战性的样本，如遮挡掩模、高斯噪声、模糊、亮度、颜色和对比度的随机抖动。在具有单级MVSNet主干的JDACS中，输入是原始多视图图像，并对每个视图应用不同的随机化变换。在具有多级MVSNet主干的JDACS-MS中，输入是多尺度的图像金字塔，并且在每个视图的图像金字塔的不同级别上添加不同的变换
    4. 如何避免GPU内存溢出？
        - 所提出的框架具有三个并行分支，这可能导致训练期间GPU内存溢出。在具有11G内存的GPU（如GTX 1080 Ti或RTX2080 Ti）上训练模型可能是不切实际的。因此，我们使用一个简单的技巧，通过用时间交换GPU内存来避免内存溢出。在训练过程中，由这三个并行分支和损失函数组成的每个步骤被分成两个顺序训练步骤。例如，我们可以在第一步传播深度估计分支和共分割分支，并将估计的深度图保存为伪深度标签。然后计算光度学一致性损失和共分割一致性损失，并在反向传播过程中计算梯度。更新渐变后，缓存内存将被清除。在第二步中，在数据增强分支中进行正向传播，并且在反向传播期间更新权重
    5. 如何调整自监督损失的权重？
        - 在实践中，收敛性对自监督损失中每个属性的权重敏感。如果应用了不适当的权重，很可能导致自监督中无效的解决方案。因此，平衡权重很重要
        - 对于光度一致性损失项，权重是在开放实现之后分配的
        - 对于共分割一致性损失，权重是根据损失规模设置的，可以从0.01到0.1进行选择
        - 对于数据增强一致性损失项目，数据增强一致性实际上是对自监督框架的一种强正则化
        - 在训练过程的开始阶段，它可能会破坏自监督的收敛。因此，我们将数据增强一致性损失的权重设置为0.01作为初始值，并在每2个周期后将其增加2倍，作为预热(warming up)过程
    6. 可视化
        - 除了主论文图6所示的3D重建的定性比较外，我们还在图10中提供了更多的比较。此外，我们展示了DTU数据集（图11）和Tanks&Temples数据集（见图12）上的重建结果可视化。有关数据集的定量结果，请参考主论文中的表1和表6。

2. 限制和讨论
    1. 粗粒度语义特征的限制
        - 如主论文中的表5和图8所示，共分割结果只能提供不超过4个语义聚类的粗粒度语义特征。原因是语义质心是从专门用于分类任务的预训练VGG的特征空间中聚类的，其中只有粗粒度语义才足以构建可区分的线索。然而，在直觉上，细粒度语义可以为自监督提供更有效的通信先验。未来，需要更准确、更精细的语义特征来进一步提高自我监督的性能
    2. 无纹理区域的限制
        - 尽管我们提出的方法可以处理颜色变化巨大的具有挑战性的情况，但它仍然无法推广到无纹理区域。所有自我监督重建损失的收敛仅在彩色区域有效。因为无纹理区域中的任何像素共享相同的颜色强度，导致自监督损失固定为0，变得毫无意义。然而，无纹理区域通常出现在现实场景中，自监督可能会被混淆，无法生成。探索处理无纹理区域可能是未来的一个潜在方向

# 七、代码
**Algorithm 1** Multiplicative Update Rule Based NMF

$\qquad$ Set the number of segments as K;

$\qquad$ Set the number of maximum iterations as $ite_{max}$ and the tolerance constant as tol;

$\qquad$ Initialize non-negative matrices P and Q such that $P \ge 0$ , $Q \ge 0$;

$\qquad$ **for** each iterative step $\mathcal{v}$ , $1 \le \mathcal{v} \le ite_{max}$ **do**

$\qquad \qquad$ $Q^{\mathcal{v} + 1}_ {[i,j]} \gets Q^{\mathcal{v}}_ {[i,j]} \frac{((P^{\mathcal{v}})^{\mathcal{t}} A)_ {[i,j]}}{((P^{\mathcal{v}})^{\mathcal{t}} P^{\mathcal{v}} Q^{\mathcal{v}})_ {[i,j]}}$

$\qquad \qquad$ $P^{\mathcal{v} + 1}_ {[i,j]} \gets P^{\mathcal{v}}_ {[i,j]} \frac{(A (Q^{\mathcal{v} + 1})^{\mathcal{t}})_ {[i,j]}}{(P^{\mathcal{v}} Q^{\mathcal{v} + 1} (Q^{\mathcal{v} + 1})^{\mathcal{t}})_ {[i,j]}}$

$\qquad \qquad$ **if** $||A - P^{\mathcal{v} + 1} Q^{\mathcal{v} + 1}||_ {F} \le tol$ **then**

$\qquad \qquad \qquad$ $P = P^{\mathcal{v} + 1},Q = Q^{\mathcal{v} + 1}$ , stop the iterative process

$\qquad \qquad$ **end if**

$\qquad$ **end for**

# 读者角度（挖掘文章中没有提到的）：
1. 总结文章发现问题的思路
2. 总结文章改进的思想
3. 总结文章还存在或者可以改进的问题
4. 提出对模型参数和细节的一些思考和讨论
