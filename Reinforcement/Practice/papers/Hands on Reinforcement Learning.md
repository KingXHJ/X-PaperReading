# 动手学强化学习


## 目录
- [课程资料](https://hrl.boyuai.com/chapter/intro)
- [强化学习基础篇](#强化学习基础篇)
    - [第 1 章 初探强化学习](#第-1-章-初探强化学习)
    - [第 2 章 多臂老虎机](#第-2-章-多臂老虎机)
    - [第 3 章 马尔可夫决策过程](#第-3-章-马尔可夫决策过程)
    - []()
    - []()
    - []()
    - []()
    - []()
- [强化学习进阶篇](#强化学习进阶篇)
- [强化学习前沿篇](#强化学习前沿篇)
- [返回上一层 README](../README.md)




*[跳转至目录](#目录)*



## 强化学习基础篇




### 第 1 章 初探强化学习

1. 强化学习和面向预测任务的区别
    - 面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。
    1. 首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。
    1. 其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。

1. 强化学习的环境
    - 与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：
        1. 一是智能体决策的动作的随机性
        1. 二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。
    - 通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。

1. 强化学习的目标
    - 在强化学习中，我们关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。
    - 价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。
    - 整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）

1. 强化学习中的数据
    - 有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。】
    - 强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）
        > **归一化的占用度量** 用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的 **状态动作对（state-action pair） 的概率分布**。
    - 强化学习本质的思维方式
        1. 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
        1. 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

1. 强化学习的独特性
    - 一般有监督学习和强化学习的范式之间的区别为
        1. 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小； $$最优模型=\underset{模型}{argmin}\mathbb{E}_ {(特征, 标签)~数据分布}[损失函数(标签, 模型(特征))]$$
        1. 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。 $$最优策略=\underset{策略}{argmin}\mathbb{E}_ {(状态, 动作)~策略的占用度量}[奖励函数(状态, 动作)]$$

*[跳转至目录](#目录)*


### 第 2 章 多臂老虎机
- [CH2 code](../code/Hands%20on%20Reinforcement%20Learning/第2章-多臂老虎机问题.ipynb)

强化学习的本质：强化学习关注智能体和环境交互过程中的学习，这是一种试错型学习（trial-and-error learning）范式

强化学习的经典问题：探索与利用（exploration vs. exploitation）
1. 探索（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。
1. 利用（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。

多臂老虎机（multi-armed bandit，MAB）的特点
1. 多臂老虎机不存在状态信息，只有动作和奖励
1. 回报没有延迟

经典算法(不可学习的策略)
1. $\epsilon$-贪心算法
    - $\epsilon$-贪婪算法在完全贪婪算法的基础上添加了噪声，每次以概率 $\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $1-\epsilon$ 随机选择一根拉杆（探索）
1. 上置信界（upper confidence bound，UCB）算法
    - 它的思想用到了一个非常著名的数学原理：霍夫丁不等式（Hoeffding's inequality）
1. 汤普森采样（Thompson sampling）算法
    - 汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法

*[跳转至目录](#目录)*


## 第 3 章 马尔可夫决策过程

与多臂老虎机问题的区别：马尔可夫决策过程（Markov decision process，MDP）包含状态信息以及状态之间的转移机制

重要概念：
1. 随机过程（stochastic process）

1. 马尔可夫性质（Markov property）
    - 需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是时刻的状态其实包含了 $t-1$ 时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

1. 马尔可夫过程（Markov process）[马尔可夫链（Markov chain）]
    - 采样（sampling）：给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode）

1. 马尔可夫奖励过程（Markov reward process）[感觉就是普通的深度学习]
    - 在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$

    - 一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成
        1. $\mathcal{S}$ 是有限状态的集合
        
        1. $\mathcal{P}$ 是有限状态的集合
        
        1. $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望。
        
        1. $\gamma$ 是折扣因子（discount factor）， $\gamma$ 的取值范围为 $[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励

    - 价值函数
        - 分类
            1. 状态价值函数：定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报 $$V^{\pi}(s)=\mathbb{E}_ {\pi}[G_ {t}|s_ {t}=s]$$
            
            1. 动作价值函数：在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报 $$Q^{\pi}(s, a)=\mathbb{E}_ {\pi}[G_ {t}|S_ {t}=s, A_ {t}=a]$$
            
            - 两种函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果 $$V^{\pi}(s)=\sum_ {a\in A}\pi(a|s)Q^{\pi}(s,a)$$
            
            - 使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积： $$Q^{\pi}(s,a)=r(s,a)+\gamma \sum_ {s^{\prime}\in S}P(s^{\prime}|s,a)V^{\pi}(s^{\prime})$$

        - 在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）

        - 贝尔曼方程（Bellman equation）
            1. 普通形式： $V(s)=r(s)+\gamma\sum_ {s{\prime}\in S}p(s^{\prime}|s)V(s^{\prime})$

            1. 矩阵形式： $$\begin{aligned}\mathcal{V} &= \mathcal{R}+\gamma\mathcal{P}\mathcal{V} \\ (\mathcal{I}-\gamma\mathcal{P})\mathcal{V} &= \mathcal{R} \\ \mathcal{V} &= (\mathcal{I}- \gamma\mathcal{P})^{-1}\mathcal{R} \\ \end{aligned}$$

        - 以上解析解的计算复杂度是 $O(n^{3})$，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）

1. 马尔可夫决策过程（Markov decision process，MDP）
    - 马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，MDP）。我们将这个来自外界的刺激称为智能体（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）

    -  一个马尔可夫决策过程由 $<\mathcal{S}, \mathcal{A}, P, r, \gamma>$ 构成
        1. $\mathcal{S}$ 是状态的集合

        1. $\mathcal{A}$ 是动作的集合

        1. $\gamma$ 是折扣因子

        1. $r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$，在奖励函数只取决于状态 $s$ 时，则退化为 $r(s)$

        1. $P(s^{\prime}|s, a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{\prime}$ 的概率

    - 策略：智能体根据当前状态从动作的集合中选择一个动作的函数，被称为策略。
        - 智能体的策略（Policy）通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_ {t}=a|S_ {t}=s)$ 是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率

1. 贝尔曼期望方程
    1. 状态价值函数： $$\begin{aligned} V^{\pi}(s) &= \mathbb{E}_ {\pi}[R_ {t} + \gamma V^{\pi}(S_ {t+1})|S_ {t}=s] \\ &= \sum_ {a \in A} \pi(a|s)(r(s,a) + \gamma \sum_ {s^{\prime} \in S}p(s^{\prime}|s,a)V^{\pi}(s^{\prime})) \\ \end{aligned}$$

    1. 动作价值函数： $$\begin{aligned} Q^{\pi}(s,a) &= \mathbb{E}_ {\pi}[R_ {t} + \gamma Q^{\pi}(S_ {t+1},A_ {t+1})|S_ {t}=s,A_ {t}=a] \\ &= r(s,a) + \gamma \sum_ {s^{\prime} \in S}p(s^{\prime}|s,a) \sum_ {a^{\prime} \in A} \pi(a^{\prime}|s^{\prime})Q^{\pi}(a^{\prime},s^{\prime}) \\ \end{aligned}$$

1. 贝尔曼最优方程
    - 最优策略都有相同的状态价值函数，我们称之为最优状态价值函数，表示为： $$V^{*}(s)=\underset{\pi}{\mathbf{max}}V^{\pi}(s), \quad \forall s \in \mathcal{S}$$

    - 同理，我们定义最优动作价值函数: $$Q^{*}(s,a)=\underset{\pi}{\mathbf{max}}Q^{\pi}(s,a), \quad \forall s \in \mathcal{S},a \in \mathcal{A}$$

    - 为了使 $Q^{pi}(s,a)$ 最大，我们需要在当前的状态动作对 $(s,a)$ 之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系： $$Q^{*}(s,a)=r(s,a) + \gamma \sum_ {s^{\prime} \in S}P(s^{\prime}|s,a)V^{*}(s^{\prime})$$

    - 这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： $$V^{*}(s)=\underset{a \in \mathcal{A}}{\mathbf{max}}Q^{*}(s,a)$$

    - 根据 $V^{*}(s)$ 和 $Q^{*}(s,a)$ 的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）： $$\begin{aligned} V^{*}(s) &= \underset{a \in \mathcal{A}}{\mathbf{max}}\lbrace r(s,a) + \gamma \sum_ {s^{\prime} \in \mathcal{S}}p(s^{\prime}|s,a)V^{*}(s^{\prime}) \rbrace \\ Q^{*}(s,a) &= r(s,a) + \gamma \sum_ {s^{\prime} \in \mathcal{S}}p(s^{\prime}|s,a)\underset{a \in \mathcal{A}}{\mathbf{max}}Q^{*}(s,a) \end{aligned}$$



算法：
1. MDP->MRP
    - 问题：这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用

1. 蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法

1. 最优策略：在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（optimal policy）
    - 强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报


占用度量:

- 不同策略的价值函数是不一样的。这是因为对于同一个 MDP，不同策略会访问到的状态的概率分布是不同的

- 一个策略的状态访问分布（state visitation distribution）： $$\mathcal{v}^{\pi}(s)=(1-\gamma)\sum^{\infty}_ {t=0}\gamma^{t}P^{\pi}_ {t}(s)$$

    1. MDP 的初始状态分布为 $\mathcal{v}_ {0}(s)$

    1. 采取策略使得智能体在 $t$ 时刻状态为 $s$ 的概率： $P^{\pi}_ {t}(s)$, 有 $P^{\pi}_ {t}(s)=\mathcal{v}_ {0}(s)$

    1. $1-\gamma$ 是用来使得概率加和为 1 的归一化因子

- 状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质： $$\mathcal{v}^{\pi}(s^{\prime})=(1-\gamma)\mathcal{v}_ {0}(s^{\prime}) + \gamma \int P(s^{\prime}|s,a)\pi (a|s)\mathcal{v}^{\pi}(s)dsda$$

- 此外，我们还可以定义策略的占用度量（occupancy measure）： $$\rho^{\pi}(s,a)=(1-\gamma)\sum^{\infty}_ {t=0}\gamma^{t}P^{\pi}_ {t}(s)\pi (a|s)$$

- 它表示动作状态对 $(s,a)$ 被访问到的概率。二者之间存在如下关系： $$\rho^{\pi}(s,a)=\mathcal{v}^{\pi}(s)\pi (a|s)$$

- 进一步得出如下两个定理
    1. 定理 1：智能体分别以策略 $\pi_ {1}$ 和 $\pi_ {2}$ 和同一个 MDP 交互得到的占用度量 $\rho^{\pi_ {1}}$ 和 $\rho^{\pi_ {2}}$ 满足 $$\rho^{\pi_ {1}}=\rho^{\pi_ {2}} \Longleftrightarrow \pi_ {1}=\pi_ {2}$$

    1. 定理 2：给定一合法占用度量 $\rho$ ，可生成该占用度量的唯一策略是 $$\pi_ {\rho}=\frac{\rho(s,a)}{\sum_ {a^{\prime}}\rho(s,a^{\prime})}$$

- 注意：以上提到的“合法”占用度量是指存在一个策略使智能体与 MDP 交互产生的状态动作对被访问到的概率。
 

## 强化学习进阶篇





## 强化学习前沿篇