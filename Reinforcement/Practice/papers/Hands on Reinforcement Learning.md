# 动手学强化学习


## 目录
- [课程资料](https://hrl.boyuai.com/chapter/intro)
- [强化学习基础篇](#强化学习基础篇)
    - [第 1 章 初探强化学习](#第-1-章-初探强化学习)
- [强化学习进阶篇](#强化学习进阶篇)
- [强化学习前沿篇](#强化学习前沿篇)
- [返回上一层 README](../README.md)




*[跳转至目录](#目录)*



## 强化学习基础篇

### 第 1 章 初探强化学习

1. 强化学习和面向预测任务的区别
    - 面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。
    1. 首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。
    1. 其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。

1. 强化学习的环境
    - 与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：
        1. 一是智能体决策的动作的随机性
        1. 二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。
    - 通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。

1. 强化学习的目标
    - 在强化学习中，我们关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。
    - 价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。
    - 整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）

1. 强化学习中的数据
    - 有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。】
    - 强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）
        > 归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的 **状态动作对（state-action pair）** 的概率分布。
    - 强化学习本质的思维方式
        1. 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
        1. 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

1. 强化学习的独特性
    - 一般有监督学习和强化学习的范式之间的区别为
        1. 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；$$最优模型=\underset{模型}{argmin}\mathbb{E}_ {(特征, 标签)~数据分布}[损失函数(标签, 模型(特征))]$$
        1. 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。$$最优策略=\underset{策略}{argmin}\mathbb{E}_ {(状态, 动作)~策略的占用度量}[奖励函数(状态, 动作)]$$

*[跳转至目录](#目录)*


### 第 2 章 多臂老虎机




## 强化学习进阶篇





## 强化学习前沿篇