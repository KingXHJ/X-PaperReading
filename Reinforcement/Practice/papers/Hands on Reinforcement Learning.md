# 动手学强化学习


## 目录
- [课程资料](https://hrl.boyuai.com/chapter/intro)
- [强化学习基础篇](#强化学习基础篇)
    - [第 1 章 初探强化学习](#第-1-章-初探强化学习)
    - [第 2 章 多臂老虎机](#第-2-章-多臂老虎机)
    - [第 3 章 马尔可夫决策过程](#第-3-章-马尔可夫决策过程)
    - []()
    - []()
    - []()
    - []()
    - []()
- [强化学习进阶篇](#强化学习进阶篇)
- [强化学习前沿篇](#强化学习前沿篇)
- [返回上一层 README](../README.md)




*[跳转至目录](#目录)*



## 强化学习基础篇




### 第 1 章 初探强化学习

1. 强化学习和面向预测任务的区别
    - 面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。
    1. 首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。
    1. 其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。

1. 强化学习的环境
    - 与面向决策任务的智能体进行交互的环境是一个动态的随机过程，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：
        1. 一是智能体决策的动作的随机性
        1. 二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。
    - 通过对环境的动态随机过程的刻画，我们能清楚地感受到，在动态随机过程中学习和在一个固定的数据分布下学习是非常不同的。

1. 强化学习的目标
    - 在强化学习中，我们关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。
    - 价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望。
    - 整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return）

1. 强化学习中的数据
    - 有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。】
    - 强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）
        > **归一化的占用度量** 用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的 **状态动作对（state-action pair） 的概率分布**。
    - 强化学习本质的思维方式
        1. 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
        1. 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

1. 强化学习的独特性
    - 一般有监督学习和强化学习的范式之间的区别为
        1. 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；$$最优模型=\underset{模型}{argmin}\mathbb{E}_ {(特征, 标签)~数据分布}[损失函数(标签, 模型(特征))]$$
        1. 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。$$最优策略=\underset{策略}{argmin}\mathbb{E}_ {(状态, 动作)~策略的占用度量}[奖励函数(状态, 动作)]$$

*[跳转至目录](#目录)*


### 第 2 章 多臂老虎机
- [CH2 code](../code/Hands%20on%20Reinforcement%20Learning/第2章-多臂老虎机问题.ipynb)

强化学习的本质：强化学习关注智能体和环境交互过程中的学习，这是一种试错型学习（trial-and-error learning）范式

强化学习的经典问题：探索与利用（exploration vs. exploitation）
1. 探索（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。
1. 利用（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。

多臂老虎机（multi-armed bandit，MAB）的特点
1. 多臂老虎机不存在状态信息，只有动作和奖励
1. 回报没有延迟

经典算法(不可学习的策略)
1. $\epsilon$-贪心算法
    - $\epsilon$-贪婪算法在完全贪婪算法的基础上添加了噪声，每次以概率 $\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $1-\epsilon$ 随机选择一根拉杆（探索）
1. 上置信界（upper confidence bound，UCB）算法
    - 它的思想用到了一个非常著名的数学原理：霍夫丁不等式（Hoeffding's inequality）
1. 汤普森采样（Thompson sampling）算法
    - 汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法

*[跳转至目录](#目录)*


## 第 3 章 马尔可夫决策过程

与多臂老虎机问题的区别：马尔可夫决策过程（Markov decision process，MDP）包含状态信息以及状态之间的转移机制

重要概念：
1. 随机过程（stochastic process）

1. 马尔可夫性质（Markov property）
    - 需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然 $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是时刻的状态其实包含了 $t-1$ 时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

1. 马尔可夫过程（Markov process）[马尔可夫链（Markov chain）]
    - 采样（sampling）：给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode）

1. 马尔可夫奖励过程（Markov reward process）[感觉就是普通的深度学习]
    - 在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$
    - 一个马尔可夫奖励过程由 $<\mathcal{S}, \mathcal{P}, r, \gamma>$ 构成
        1. $\mathcal{S}$ 是有限状态的集合
        1. $\mathcal{P}$ 是有限状态的集合
        1. $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望。
        1. $\gamma$ 是折扣因子（discount factor），$\gamma$ 的取值范围为 $[0,1)$。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励
    - 价值函数
        - 在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）
        - 贝尔曼方程（Bellman equation）：
            1. 普通形式：$V(s)=r(s)+\gamma\sum_ {s{'}\in S}p(s^{'}|s)V(s^{'})$
            1. 矩阵形式：$$\begin{aligned}\mathcal{V}&=\mathcal{R}+\gamma\mathcal{P}\mathcal{V} \\ (\mathcal{I}-\gamma\mathcal{P})\mathcal{V} &= \mathcal{R} \\ \mathcal{V} &= (\mathcal{I} - \gamma\mathcal{P})^{-1}\mathcal{R} \end{aligned}$$
        - 以上解析解的计算复杂度是 $O(n^{3})$，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划（dynamic programming）算法、蒙特卡洛方法（Monte-Carlo method）和时序差分（temporal difference）

1. 马尔可夫决策过程（Markov decision process，MDP）
    - 马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（Markov decision process，MDP）。我们将这个来自外界的刺激称为智能体（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）
    -  一个马尔可夫决策过程由 $<\mathcal{S}, \mathcal{A}, P, r, \gamma>$ 构成
        1. $\mathcal{S}$ 是状态的集合
        1. $\mathcal{A}$ 是动作的集合
        1. $\gamma$ 是折扣因子
        1. $r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$，在奖励函数只取决于状态 $s$ 时，则退化为$r(s)$
        1. $P(s^{'}|s, a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{'}$ 的概率




## 强化学习进阶篇





## 强化学习前沿篇